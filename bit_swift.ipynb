{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "bit_swift.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "swift",
      "display_name": "Swift"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/acoadmarmon/big-transfer-swift/blob/main/bit_swift.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kZRlD4utdPuX",
        "outputId": "2b217804-1daa-4284-be54-5ab785d93a7e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "%install '.package(url: \"https://github.com/tensorflow/swift-models\", .branch(\"tensorflow-0.11\"))' Datasets ImageClassificationModels\n",
        "print(\"\\u{001B}[2J\")"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\r\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wTl6d5efpeb_"
      },
      "source": [
        "import Datasets\n",
        "import ImageClassificationModels\n",
        "import TensorFlow\n",
        "import Python\n",
        "import Foundation\n",
        "import Differentiable\n"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JDjdyWTlaDBc",
        "outputId": "dfa7cdac-af79-4c8e-8786-2c5c239c6b64",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "%include \"EnableIPythonDisplay.swift\"\n",
        "IPythonDisplay.shell.enable_matplotlib(\"inline\")\n",
        "\n",
        "let plt = Python.import(\"matplotlib.pyplot\")\n",
        "let np  = Python.import(\"numpy\")\n",
        "let subprocess = Python.import(\"subprocess\")\n",
        "let glob = Python.import(\"glob\")\n",
        "let pil = Python.import(\"PIL\")\n",
        "let tf = Python.import(\"tensorflow\")\n",
        "//let tfp = Python.import(\"tensorflow_probability\")\n",
        "let h5py = Python.import(\"h5py\")\n",
        "let path = Python.import(\"os.path\")"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2020-11-17 18:24:49.333916: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\r\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h3ilNx78jjEA"
      },
      "source": [
        "//subprocess.call(\"curl https://sdk.cloud.google.com | bash; exec -l $SHELL; gsutil ls gs://uspto-pair/applications/0800401*\", shell: true)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E2SFbiPWpinG"
      },
      "source": [
        "let epochCount = 12\n",
        "let batchSize = 32"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jAEB0ZyHrvGS"
      },
      "source": [
        "// Set hyperrule parameters from bit_hyperrule.py\n",
        "enum ValueError: Error {\n",
        "    case invalidInput(String)\n",
        "}\n",
        "\n",
        "func get_resolution(original_resolution: (Int, Int)) -> (Int, Int) {\n",
        "  let area = original_resolution.0 * original_resolution.1\n",
        "  return area < 96*96 ? (160, 128) : (512, 480)\n",
        "}\n",
        "\n",
        "\n",
        "let known_dataset_sizes:[String: (Int, Int)] = [\n",
        "  \"cifar10\": (32, 32),\n",
        "  \"cifar100\": (32, 32),\n",
        "  \"oxford_iiit_pet\": (224, 224),\n",
        "  \"oxford_flowers102\": (224, 224),\n",
        "  \"imagenet2012\": (224, 224),\n",
        "]\n",
        "\n",
        "func get_resolution_from_dataset(dataset: String) throws -> (Int, Int) {\n",
        "  if let resolution = known_dataset_sizes[dataset] {\n",
        "    return get_resolution(original_resolution: resolution)\n",
        "  }\n",
        "  print(\"Unsupported dataset \" + dataset + \". Add your own here :)\")\n",
        "  throw ValueError.invalidInput(dataset)\n",
        "\n",
        "}\n",
        "\n",
        "func get_mixup(dataset_size: Int) -> Double {\n",
        "  return dataset_size < 20_000 ? 0.0 : 0.1\n",
        "}\n",
        "\n",
        "\n",
        "func get_schedule(dataset_size: Int) -> Array<Int> {\n",
        "  if dataset_size < 20_000{\n",
        "    return [100, 200, 300, 400, 500]\n",
        "  }\n",
        "  else if dataset_size < 500_000 {\n",
        "    return [500, 3000, 6000, 9000, 10_000]\n",
        "  }\n",
        "  else {\n",
        "    return [500, 6000, 12_000, 18_000, 20_000]\n",
        "  }\n",
        "}\n",
        "\n",
        "\n",
        "func get_lr(step: Int, dataset_size: Int, base_lr: Float = 0.003) -> Float? {\n",
        "  /* Returns learning-rate for `step` or nil at the end. */\n",
        "  let supports = get_schedule(dataset_size: dataset_size)\n",
        "  // Linear warmup\n",
        "  if step < supports[0] {\n",
        "    return base_lr * Float(step) / Float(supports[0])\n",
        "  }\n",
        "  // End of training\n",
        "  else if step >= supports.last! {\n",
        "    return nil\n",
        "  }\n",
        "  // Staircase decays by factor of 10\n",
        "  else {\n",
        "    var base_lr = base_lr\n",
        "    for s in supports[1...] {\n",
        "      if s < step {\n",
        "        base_lr = base_lr / 10.0\n",
        "      }\n",
        "    }\n",
        "    return base_lr\n",
        "  }\n",
        "}"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Er7kXeINnwuW",
        "outputId": "eb5dc816-0139-4dd5-e631-3f6e03a33000",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "get_lr(step: 7000, dataset_size: 50000, base_lr: 0.01)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "▿ Optional<Float>\n",
              "  - some : 9.999999e-05\n"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RFmZWY2q-xzd",
        "outputId": "70bbef30-d7e4-412a-93b3-4e9c505f9a48",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "// Build model with weights\n",
        "import FileManager\n",
        "import PythonKit\n",
        "let tf = Python.import(\"tensorflow\")\n",
        "_ = Python.builtins\n",
        "\n",
        "var known_models = [String: String]()\n",
        "// var resnetv2 = ResNetV2(classCount: 1000, depth: .resNet50)\n",
        "\n",
        "let model_name = \"BiT-M-R50x1\"\n",
        "\n",
        "struct Weights {\n",
        "    let name: String\n",
        "    let layer: Tensor<Float>\n",
        "}\n",
        "\n",
        "func get_pretrained_weights_dict(model_name: String) -> Array<Weights> {\n",
        "  let valid_types = [\"BiT-S\", \"BiT-M\"]\n",
        "  let valid_sizes = [(50, 1), (50, 3), (101, 1), (101, 3), (152, 4)]\n",
        "  let bit_url = \"gs://bit_models/\"\n",
        "\n",
        "  for types in valid_types {\n",
        "    for sizes in valid_sizes {\n",
        "      let model_string = types + \"-R\" + String(sizes.0) + \"x\" + String(sizes.1)\n",
        "      known_models[model_string] = bit_url + model_string + \".npz\"\n",
        "    }\n",
        "  }\n",
        "\n",
        "  var f = Python.None\n",
        "  \n",
        "  if let model_path = known_models[model_name] {\n",
        "    subprocess.call(\"gsutil cp \" + model_path + \" .\", shell: true)\n",
        "  }\n",
        "\n",
        "  let weights = np.load(\"./\" + model_name + \".npz\")\n",
        "\n",
        "  var weights_array = Array<Weights>()\n",
        "  for param in weights {\n",
        "      weights_array.append(Weights(name: String(param)!, layer: Tensor<Float>(numpy: weights[param])!))\n",
        "  }\n",
        "  return weights_array\n",
        "}\n",
        "var weights_array = get_pretrained_weights_dict(model_name: model_name)\n",
        "// Get convolution weights"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Copying gs://bit_models/BiT-M-R50x1.npz...\n",
            "/ [1 files][260.4 MiB/260.4 MiB]                                                \n",
            "Operation completed over 1 objects/260.4 MiB.                                    \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hwWf0PEYMA_N"
      },
      "source": [
        "import TensorFlow\n",
        "func getPaddingDimensionsFromKernelSize(kernelSize: Int) -> (Int, Int) {\n",
        "  let padTotal = kernelSize - 1\n",
        "  let padBeginning = Int(padTotal / 2)\n",
        "  let padEnd = padTotal - padBeginning\n",
        "  return (kernelSize + padBeginning, kernelSize + padEnd)\n",
        "}\n",
        "\n",
        "func paddingFromKernelSize(kernelSize: Int) -> [(before: Int, after: Int)] {\n",
        "  let padTotal = kernelSize - 1\n",
        "  let padBeginning = Int(padTotal / 2)\n",
        "  let padEnd = padTotal - padBeginning\n",
        "  let padding = [\n",
        "        (before: 0, after: 0),\n",
        "        (before: padBeginning, after: padEnd),\n",
        "        (before: padBeginning, after: padEnd),\n",
        "        (before: 0, after: 0)]\n",
        "  return padding\n",
        "}\n",
        "\n",
        "public struct StandardizedConv2D: Layer {\n",
        "  public var conv: Conv2D<Float>\n",
        "\n",
        "  public init(\n",
        "    filterShape: (Int, Int, Int, Int),\n",
        "    strides: (Int, Int) = (1, 1),\n",
        "    padding: Padding = .valid,\n",
        "    useBias: Bool = true\n",
        "  )\n",
        "  {\n",
        "  self.conv = Conv2D(\n",
        "      filterShape: filterShape, \n",
        "      strides: strides, \n",
        "      padding: padding,\n",
        "      useBias: useBias)\n",
        "  }\n",
        "\n",
        "  @differentiable\n",
        "  public func callAsFunction(_ input: Tensor<Float>) -> Tensor<Float> {\n",
        "      let axes: Array<Int> = [0, 1, 2]\n",
        "      var standardizedConv = conv\n",
        "      standardizedConv.filter = (standardizedConv.filter - standardizedConv.filter.mean(squeezingAxes: axes)) / sqrt((standardizedConv.filter.variance(squeezingAxes: axes) + 1e-16))\n",
        "      return standardizedConv(input)\n",
        "  }\n",
        "\n",
        "}\n",
        "\n",
        "\n",
        "public struct ConvGNV2: Layer {\n",
        "    public var conv: StandardizedConv2D\n",
        "    public var norm: GroupNorm<Float>\n",
        "    @noDerivative public var isSecond: Bool\n",
        "\n",
        "    public init(\n",
        "        inFilters: Int,\n",
        "        outFilters: Int,\n",
        "        kernelSize: Int = 1,\n",
        "        stride: Int = 1,\n",
        "        padding: Padding = .valid,\n",
        "        isSecond: Bool = false\n",
        "    ) {\n",
        "        self.conv = StandardizedConv2D(\n",
        "            filterShape: (kernelSize, kernelSize, inFilters, outFilters), \n",
        "            strides: (stride, stride), \n",
        "            padding: padding,\n",
        "            useBias: false)\n",
        "        self.norm = GroupNorm<Float>(\n",
        "              offset: Tensor(zeros: [inFilters]),\n",
        "              scale: Tensor(zeros: [inFilters]),\n",
        "              groupCount: 2,\n",
        "              axis: -1,\n",
        "              epsilon: 0.001)\n",
        "        self.isSecond = isSecond\n",
        "    }\n",
        "\n",
        "    @differentiable\n",
        "    public func callAsFunction(_ input: Tensor<Float>) -> Tensor<Float> {\n",
        "        var normResult = norm(input)\n",
        "        if self.isSecond {\n",
        "            normResult = normResult.padded(forSizes: paddingFromKernelSize(kernelSize: 3))\n",
        "        }\n",
        "        let reluResult = relu(normResult)\n",
        "        let convResult = conv(reluResult)\n",
        "        return convResult\n",
        "    }\n",
        "}\n",
        "public struct ShortcutBiT: Layer {\n",
        "    public var projection: StandardizedConv2D\n",
        "    public var norm: GroupNorm<Float>\n",
        "    @noDerivative public let needsProjection: Bool\n",
        "    \n",
        "    public init(inFilters: Int, outFilters: Int, stride: Int) {\n",
        "      needsProjection = (stride > 1 || inFilters != outFilters)\n",
        "      norm = GroupNorm<Float>(\n",
        "          offset: Tensor(zeros: [needsProjection ? inFilters  : 1]),\n",
        "          scale: Tensor(zeros: [needsProjection ? inFilters  : 1]),\n",
        "          groupCount: needsProjection ? 2  : 1,\n",
        "          axis: -1,\n",
        "          epsilon: 0.001)\n",
        "        \n",
        "        projection =  StandardizedConv2D(\n",
        "            filterShape: (1, 1, needsProjection ? inFilters  : 1, needsProjection ? outFilters : 1), \n",
        "            strides: (stride, stride), \n",
        "            padding: .valid,\n",
        "            useBias: false)\n",
        "    }\n",
        "    \n",
        "    @differentiable\n",
        "    public func callAsFunction(_ input: Tensor<Float>) -> Tensor<Float> {\n",
        "        var res = input\n",
        "        if needsProjection { \n",
        "          res = norm(res)\n",
        "          res = relu(res)\n",
        "          res = projection(res)\n",
        "        }\n",
        "        return res\n",
        "    }\n",
        "}\n",
        "\n",
        "\n",
        "\n",
        "public struct ResidualBlockBiT: Layer {\n",
        "    public var shortcut: ShortcutBiT\n",
        "    public var convs: [ConvGNV2]\n",
        "\n",
        "    public init(inFilters: Int, outFilters: Int, stride: Int, expansion: Int){\n",
        "        if expansion == 1 {\n",
        "            convs = [\n",
        "                ConvGNV2(inFilters: inFilters,  outFilters: outFilters, kernelSize: 3, stride: stride),\n",
        "                ConvGNV2(inFilters: outFilters, outFilters: outFilters, kernelSize: 3, isSecond: true)\n",
        "            ]\n",
        "        } else {\n",
        "            convs = [\n",
        "                ConvGNV2(inFilters: inFilters,    outFilters: outFilters/4),\n",
        "                ConvGNV2(inFilters: outFilters/4, outFilters: outFilters/4, kernelSize: 3, stride: stride, isSecond: true),\n",
        "                ConvGNV2(inFilters: outFilters/4, outFilters: outFilters)\n",
        "            ]\n",
        "        }\n",
        "        shortcut = ShortcutBiT(inFilters: inFilters, outFilters: outFilters, stride: stride)\n",
        "    }\n",
        "\n",
        "    @differentiable\n",
        "    public func callAsFunction(_ input: Tensor<Float>) -> Tensor<Float> {\n",
        "        let convResult = convs.differentiableReduce(input) { $1($0) }\n",
        "        return convResult + shortcut(input)\n",
        "    }\n",
        "}\n",
        "\n",
        "public struct BigTransfer: Layer {\n",
        "  public var inputStem: StandardizedConv2D\n",
        "  public var maxPool: MaxPool2D<Float>\n",
        "  public var residualBlocks: [ResidualBlockBiT] = []\n",
        "  public var groupNorm : GroupNorm<Float>\n",
        "  public var flatten = Flatten<Float>()\n",
        "  public var classifier: Dense<Float>\n",
        "  public var avgPool = GlobalAvgPool2D<Float>()\n",
        "  @noDerivative public var finalOutFilter : Int = 0\n",
        "\n",
        "  public init(\n",
        "        classCount: Int, \n",
        "        depth: Depth, \n",
        "        inputChannels: Int = 3\n",
        "    ) {\n",
        "\n",
        "        self.inputStem = StandardizedConv2D(filterShape: (7, 7, 3, 64), strides: (2, 2), padding: .valid, useBias: false)\n",
        "        self.maxPool = MaxPool2D(poolSize: (3, 3), strides: (2, 2), padding: .valid)\n",
        "        let sizes = [64 / depth.expansion, 64, 128, 256, 512]\n",
        "        for (iBlock, nBlocks) in depth.layerBlockSizes.enumerated() {\n",
        "            let (nIn, nOut) = (sizes[iBlock] * depth.expansion, sizes[iBlock+1] * depth.expansion)\n",
        "            for j in 0..<nBlocks {\n",
        "\n",
        "                self.residualBlocks.append(ResidualBlockBiT(\n",
        "                    inFilters: j==0 ? nIn : nOut,  \n",
        "                    outFilters: nOut, \n",
        "                    stride: (iBlock != 0) && (j == 0) ? 2 : 1, \n",
        "                    expansion: depth.expansion\n",
        "                ))\n",
        "                self.finalOutFilter = nOut\n",
        "            }\n",
        "        }\n",
        "        self.groupNorm = GroupNorm<Float>(\n",
        "              offset: Tensor(zeros: [self.finalOutFilter]),\n",
        "              scale: Tensor(zeros: [self.finalOutFilter]),\n",
        "              groupCount: 2,\n",
        "              axis: -1,\n",
        "              epsilon: 0.001)\n",
        "        self.classifier = Dense(inputSize: 512 * depth.expansion, outputSize: classCount)\n",
        "    }\n",
        "\n",
        "  @differentiable\n",
        "  public func callAsFunction(_ input: Tensor<Float>) -> Tensor<Float> {\n",
        "      var paddedInput = input.padded(forSizes: paddingFromKernelSize(kernelSize: 7))\n",
        "      paddedInput = inputStem(paddedInput).padded(forSizes: paddingFromKernelSize(kernelSize: 3))\n",
        "      let inputLayer = maxPool(paddedInput)\n",
        "      let blocksReduced = residualBlocks.differentiableReduce(inputLayer) { $1($0) }\n",
        "      let normalized = relu(groupNorm(blocksReduced))\n",
        "      return normalized.sequenced(through: avgPool, flatten, classifier)\n",
        "  }\n",
        "}\n",
        "\n",
        "extension BigTransfer {\n",
        "    public enum Depth {\n",
        "        case resNet18\n",
        "        case resNet34\n",
        "        case resNet50\n",
        "        case resNet101\n",
        "        case resNet152\n",
        "\n",
        "        var expansion: Int {\n",
        "            switch self {\n",
        "            case .resNet18, .resNet34: return 1\n",
        "            default: return 4\n",
        "            }\n",
        "        }\n",
        "\n",
        "        var layerBlockSizes: [Int] {\n",
        "            switch self {\n",
        "            case .resNet18:  return [2, 2, 2,  2]\n",
        "            case .resNet34:  return [3, 4, 6,  3]\n",
        "            case .resNet50:  return [3, 4, 6,  3]\n",
        "            case .resNet101: return [3, 4, 23, 3]\n",
        "            case .resNet152: return [3, 8, 36, 3]\n",
        "            }\n",
        "        }\n",
        "    }\n",
        "}"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TpdLHcEOpGeU"
      },
      "source": [
        "func get_model_units(model_name: String) -> BigTransfer.Depth {\n",
        "  if model_name.contains(\"R50\") {\n",
        "    return .resNet50\n",
        "  }\n",
        "  else if model_name.contains(\"R101\") {\n",
        "    return .resNet101\n",
        "  }\n",
        "  else {\n",
        "    return .resNet152\n",
        "  }\n",
        "}\n",
        "\n",
        "let bit_type = get_model_units(model_name: model_name)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o-MYC9LUr4HM",
        "outputId": "7435257d-6e50-4bad-f540-de1c600392c2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "var resnetv2 = BigTransfer(classCount: 10, depth: .resNet50)\n",
        "\n",
        "let convs = weights_array.filter {key in return key.name.contains(\"/block\") && key.name.contains(\"standardized_conv2d/kernel\") && !(key.name.contains(\"proj\"))}\n",
        "\n",
        "var k = 0\n",
        "for (idx, i) in resnetv2.residualBlocks.enumerated() {\n",
        "  for (jdx, _) in i.convs.enumerated() {\n",
        "    assert(resnetv2.residualBlocks[idx].convs[jdx].conv.conv.filter.shape == convs[k].layer.shape)\n",
        "    resnetv2.residualBlocks[idx].convs[jdx].conv.conv.filter = convs[k].layer\n",
        "    k = k + 1\n",
        "  }\n",
        "}\n",
        "print(convs.count, k)\n",
        "// https://github.com/zaidalyafeai/Swift4TF/blob/master/Swift4TF_TransferLearning.ipynb "
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "48 48\r\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PtrRmtXMHLU6",
        "outputId": "6c3532b2-efd5-4213-9c61-804ef611eb79",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "// Set weights for all projective convolutions\n",
        "let projective_convs = weights_array.filter {key in return key.name.contains(\"/block\") && key.name.contains(\"standardized_conv2d/kernel\") && (key.name.contains(\"proj\"))}\n",
        "let norm_scale = weights_array.filter {key in return key.name.contains(\"unit01/a/group_norm/gamma\")}\n",
        "let norm_offset = weights_array.filter {key in return key.name.contains(\"unit01/a/group_norm/beta\")}\n",
        "\n",
        "var k = 0\n",
        "for (idx, i) in resnetv2.residualBlocks.enumerated() {\n",
        "    if (i.shortcut.projection.conv.filter.shape != [1, 1, 1, 1])\n",
        "    {\n",
        "      assert(resnetv2.residualBlocks[idx].shortcut.projection.conv.filter.shape == projective_convs[k].layer.shape)\n",
        "      resnetv2.residualBlocks[idx].shortcut.projection.conv.filter = projective_convs[k].layer\n",
        "\n",
        "      assert(resnetv2.residualBlocks[idx].shortcut.norm.scale.shape == norm_scale[k].layer.shape)\n",
        "      resnetv2.residualBlocks[idx].shortcut.norm.scale = norm_scale[k].layer\n",
        "\n",
        "      assert(resnetv2.residualBlocks[idx].shortcut.norm.offset.shape == norm_offset[k].layer.shape)\n",
        "      resnetv2.residualBlocks[idx].shortcut.norm.offset = norm_offset[k].layer\n",
        "      k = k + 1\n",
        "    }\n",
        "}\n",
        "print(projective_convs.count, k)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "4 4\r\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hu7mrFlh7j0P"
      },
      "source": [
        "// Set weights for all standard norms\n",
        "let norm_scale = weights_array.filter {key in return key.name.contains(\"gamma\")}\n",
        "\n",
        "var k = 0\n",
        "for (idx, i) in resnetv2.residualBlocks.enumerated() {\n",
        "  for (jdx, _) in i.convs.enumerated() {\n",
        "    assert(norm_scale[k].layer.shape == resnetv2.residualBlocks[idx].convs[jdx].norm.scale.shape)\n",
        "    resnetv2.residualBlocks[idx].convs[jdx].norm.scale = norm_scale[k].layer\n",
        "    k = k + 1\n",
        "  }\n",
        "}\n",
        "\n",
        "let norm_offset = weights_array.filter {key in return key.name.contains(\"beta\")}\n",
        "\n",
        "var l = 0\n",
        "for (idx, i) in resnetv2.residualBlocks.enumerated() {\n",
        "  for (jdx, _) in i.convs.enumerated() {\n",
        "    assert(norm_offset[l].layer.shape == resnetv2.residualBlocks[idx].convs[jdx].norm.offset.shape)\n",
        "    resnetv2.residualBlocks[idx].convs[jdx].norm.offset = norm_offset[l].layer\n",
        "    l = l + 1\n",
        "  }\n",
        "}\n",
        "\n",
        "assert(resnetv2.groupNorm.scale.shape == norm_scale[k].layer.shape)\n",
        "resnetv2.groupNorm.scale = norm_scale[k].layer\n",
        "assert(resnetv2.groupNorm.offset.shape == norm_offset[l].layer.shape)\n",
        "resnetv2.groupNorm.offset = norm_offset[l].layer"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XSGpiMF-Fcxj"
      },
      "source": [
        "\n"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a62-npIMD24Y"
      },
      "source": [
        "// Set weight for input block\n",
        "let root_convs = weights_array.filter {key in return key.name.contains(\"root_block\")}\n",
        "assert(resnetv2.inputStem.conv.filter.shape == root_convs[0].layer.shape)\n",
        "resnetv2.inputStem.conv.filter = root_convs[0].layer"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HZWyUKAxEHqt",
        "outputId": "efec4e85-bd13-4a25-fbcd-2d8ba749f488",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "let norm_scale = weights_array.filter {key in return key.name.contains(\"unit01/a/group_norm/gamma\")}\n",
        "print(norm_scale.count, projective_convs.count)\n",
        "for i in projective_convs {\n",
        "  print(i.name)\n",
        "}\n",
        "\n"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "4 4\r\n",
            "resnet/block1/unit01/a/proj/standardized_conv2d/kernel\r\n",
            "resnet/block2/unit01/a/proj/standardized_conv2d/kernel\r\n",
            "resnet/block3/unit01/a/proj/standardized_conv2d/kernel\r\n",
            "resnet/block4/unit01/a/proj/standardized_conv2d/kernel\r\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TYKUEjZTQx18",
        "outputId": "023f171e-6d2a-47de-b57a-1093cff1df98",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "let norm_scale = weights_array.filter {key in return key.name.contains(\"dense\")}\n",
        "norm_scale"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0 elements\n"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "51749rK4sSFS",
        "outputId": "6d6c3541-eed4-4469-dbf0-a7d4a039bf79",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "let _ = _ExecutionContext.global\n",
        "\n",
        "let device = Device.defaultXLA\n",
        "device"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "▿ Device(kind: .GPU, ordinal: 0, backend: .XLA)\n",
              "  - kind : TensorFlow.Device.Kind.GPU\n",
              "  - ordinal : 0\n",
              "  - backend : TensorFlow.Device.Backend.XLA\n"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "17K06oETPq_P",
        "outputId": "94a66f22-2d02-4b25-d89d-38a3f9684e16",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "resnetv2.move(to: device)\n",
        "var optimizer = SGD(for: resnetv2, learningRate: 0.001, momentum: 0.9)\n",
        "optimizer = SGD(copying: optimizer, to: device)\n",
        "\n",
        "let cifar10_training_size = 50000\n",
        "\n",
        "Context.local.learningPhase = .training\n",
        "let batchSize = 128\n",
        "let lrSupports = get_schedule(dataset_size: cifar10_training_size)\n",
        "let scheduleLength = lrSupports.last!\n",
        "let stepsPerEpoch = cifar10_training_size / batchSize\n",
        "\n",
        "\n",
        "var epochCount = scheduleLength / stepsPerEpoch\n",
        "let dataset = CIFAR10(batchSize: batchSize, on: Device.default)\n",
        "\n",
        "print(stepsPerEpoch)\n",
        "print(epochCount)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loading resource: cifar-10-binary\r\n",
            "File does not exist locally at expected path: /root/.cache/swift-models/datasets/CIFAR10/cifar-10-binary and must be fetched\r\n",
            "Fetching URL: https://storage.googleapis.com/s4tf-hosted-binaries/datasets/CIFAR10/cifar-10-binary.tar.gz...\n",
            "Archive saved to: /root/.cache/swift-models/datasets/CIFAR10\n",
            "390\n",
            "25\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KHxauxK0OFCN"
      },
      "source": [
        "struct Statistics {\n",
        "    var correctGuessCount = Tensor<Int32>(0, on: Device.default)\n",
        "    var totalGuessCount = Tensor<Int32>(0, on: Device.default)\n",
        "    var totalLoss = Tensor<Float>(0, on: Device.default)\n",
        "    var batches: Int = 0\n",
        "    var accuracy: Float { \n",
        "        Float(correctGuessCount.scalarized()) / Float(totalGuessCount.scalarized()) * 100 \n",
        "    } \n",
        "    var averageLoss: Float { totalLoss.scalarized() / Float(batches) }\n",
        "\n",
        "    init(on device: Device = Device.default) {\n",
        "        correctGuessCount = Tensor<Int32>(0, on: device)\n",
        "        totalGuessCount = Tensor<Int32>(0, on: device)\n",
        "        totalLoss = Tensor<Float>(0, on: device)\n",
        "    }\n",
        "\n",
        "    mutating func update(logits: Tensor<Float>, labels: Tensor<Float>, loss: Tensor<Float>) {\n",
        "        let correct = logits.argmax(squeezingAxis: 1) .== labels.argmax(squeezingAxis: 1)\n",
        "        correctGuessCount += Tensor<Int32>(correct).sum()\n",
        "        totalGuessCount += Int32(labels.shape[0])\n",
        "        totalLoss += loss\n",
        "        batches += 1\n",
        "    }\n",
        "}\n",
        "\n",
        "\n"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jMKT1a_pa8ut"
      },
      "source": [
        ""
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wzYP191ERHI8",
        "outputId": "15dc7f0a-a9fe-4579-b14c-77a45ce9fe4d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "print(\"Beginning training...\")\n",
        "var curr_step: Int = 1\n",
        "let resize_size = get_resolution(original_resolution: (32, 32))\n",
        "let mixup_alpha = get_mixup(dataset_size: cifar10_training_size)\n",
        "let beta = np.random.beta(mixup_alpha, mixup_alpha)\n",
        "\n",
        "epochCount = 1\n",
        "for (epoch, batches) in dataset.training.prefix(epochCount).enumerated() {\n",
        "    let start = Date()\n",
        "    var trainStats = Statistics(on: device)\n",
        "    var testStats = Statistics(on: device)\n",
        "    \n",
        "    Context.local.learningPhase = .training\n",
        "    for batch in batches {\n",
        "        if let new_lr = get_lr(step: curr_step, dataset_size: cifar10_training_size, base_lr: 0.003) {\n",
        "          optimizer.learningRate = new_lr\n",
        "          curr_step = curr_step + 1\n",
        "        }\n",
        "        else {\n",
        "          continue\n",
        "        }\n",
        "\n",
        "        var (eagerImages, eagerLabels) = (batch.data, batch.label)\n",
        "        var npImages = eagerImages.makeNumpyArray()\n",
        "        var resized = tf.image.resize(npImages, [resize_size.0, resize_size.0])\n",
        "        var cropped = tf.image.random_crop(resized, [batchSize, resize_size.1, resize_size.1, 3])\n",
        "        var flipped = tf.image.random_flip_left_right(cropped)\n",
        "        var mixed_up = flipped\n",
        "        var newLabels = Tensor<Float>(Tensor<Int32>(oneHotAtIndices: eagerLabels, depth: 10))\n",
        "        if mixup_alpha > 0.0 {\n",
        "          var npLabels = newLabels.makeNumpyArray()\n",
        "          mixed_up = beta * mixed_up + (1 - beta) * tf.reverse(mixed_up, axis: [0])\n",
        "          npLabels = beta * npLabels + (1 - beta) * tf.reverse(npLabels, axis: [0])\n",
        "          newLabels = Tensor<Float>(numpy: npLabels.numpy())!\n",
        "        }\n",
        "        eagerImages = Tensor<Float>(numpy: mixed_up.numpy())!\n",
        "        let images = Tensor(copying: eagerImages, to: device)\n",
        "        let labels = Tensor(copying: newLabels, to: device)\n",
        "\n",
        "        let 𝛁model = TensorFlow.gradient(at: resnetv2) { resnetv2 -> Tensor<Float> in\n",
        "            let ŷ = resnetv2(images)\n",
        "            let loss = softmaxCrossEntropy(logits: ŷ, probabilities: labels)\n",
        "            trainStats.update(logits: ŷ, labels: labels, loss: loss)\n",
        "            return loss\n",
        "        }\n",
        "\n",
        "        \n",
        "        optimizer.update(&resnetv2, along: 𝛁model)\n",
        "        LazyTensorBarrier()\n",
        "    }\n",
        "\n",
        "    Context.local.learningPhase = .inference\n",
        "    for batch in dataset.validation {\n",
        "        var (eagerImages, eagerLabels) = (batch.data, batch.label)\n",
        "        var npImages = eagerImages.makeNumpyArray()\n",
        "        var resized = tf.image.resize(npImages, [resize_size.0, resize_size.0])\n",
        "        eagerImages = Tensor<Float>(numpy: resized.numpy())!\n",
        "        var newLabels = Tensor<Float>(Tensor<Int32>(oneHotAtIndices: eagerLabels, depth: 10))\n",
        "        let images = Tensor(copying: eagerImages, to: device)\n",
        "        let labels = Tensor(copying: newLabels, to: device)\n",
        "        let ŷ = resnetv2(images)\n",
        "        let loss = softmaxCrossEntropy(logits: ŷ, probabilities: labels)\n",
        "        LazyTensorBarrier()\n",
        "        testStats.update(logits: ŷ, labels: labels, loss: loss)\n",
        "    }\n",
        "\n",
        "    print(\n",
        "        \"\"\"\n",
        "        [Epoch \\(epoch)] \\\n",
        "        Training Loss: \\(String(format: \"%.3f\", trainStats.averageLoss)), \\\n",
        "        Training Accuracy: \\(trainStats.correctGuessCount)/\\(trainStats.totalGuessCount) \\\n",
        "        (\\(String(format: \"%.1f\", trainStats.accuracy))%), \\\n",
        "        Test Loss: \\(String(format: \"%.3f\", testStats.averageLoss)), \\\n",
        "        Test Accuracy: \\(testStats.correctGuessCount)/\\(testStats.totalGuessCount) \\\n",
        "        (\\(String(format: \"%.1f\", testStats.accuracy))%) \\\n",
        "        seconds per epoch: \\(String(format: \"%.1f\", Date().timeIntervalSince(start)))\n",
        "        \"\"\")\n",
        "}\n"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Beginning training...\n",
            "2020-11-17 18:32:55.204674: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcuda.so.1\n",
            "2020-11-17 18:32:55.205162: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-11-17 18:32:55.205814: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1716] Found device 0 with properties: \n",
            "pciBusID: 0000:00:04.0 name: Tesla P100-PCIE-16GB computeCapability: 6.0\n",
            "coreClock: 1.3285GHz coreCount: 56 deviceMemorySize: 15.90GiB deviceMemoryBandwidth: 681.88GiB/s\n",
            "2020-11-17 18:32:55.205887: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\n",
            "2020-11-17 18:32:55.205937: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcublas.so.10\n",
            "2020-11-17 18:32:55.205968: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcufft.so.10\n",
            "2020-11-17 18:32:55.205993: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcurand.so.10\n",
            "2020-11-17 18:32:55.206027: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusolver.so.10\n",
            "2020-11-17 18:32:55.206053: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusparse.so.10\n",
            "2020-11-17 18:32:55.206080: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudnn.so.7\n",
            "2020-11-17 18:32:55.206214: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-11-17 18:32:55.206833: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-11-17 18:32:55.207412: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1858] Adding visible gpu devices: 0\n",
            "2020-11-17 18:32:55.269287: I tensorflow/core/platform/profile_utils/cpu_utils.cc:104] CPU Frequency: 2200000000 Hz\n",
            "2020-11-17 18:32:55.270319: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x229cfc80 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
            "2020-11-17 18:32:55.270374: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
            "2020-11-17 18:32:55.279345: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-11-17 18:32:55.280058: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x2d9fefc0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
            "2020-11-17 18:32:55.280096: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Tesla P100-PCIE-16GB, Compute Capability 6.0\n",
            "2020-11-17 18:32:55.280334: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-11-17 18:32:55.280904: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1716] Found device 0 with properties: \n",
            "pciBusID: 0000:00:04.0 name: Tesla P100-PCIE-16GB computeCapability: 6.0\n",
            "coreClock: 1.3285GHz coreCount: 56 deviceMemorySize: 15.90GiB deviceMemoryBandwidth: 681.88GiB/s\n",
            "2020-11-17 18:32:55.280989: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\n",
            "2020-11-17 18:32:55.281042: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcublas.so.10\n",
            "2020-11-17 18:32:55.281074: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcufft.so.10\n",
            "2020-11-17 18:32:55.281104: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcurand.so.10\n",
            "2020-11-17 18:32:55.281132: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusolver.so.10\n",
            "2020-11-17 18:32:55.281159: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusparse.so.10\n",
            "2020-11-17 18:32:55.281186: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudnn.so.7\n",
            "2020-11-17 18:32:55.281282: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-11-17 18:32:55.281961: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-11-17 18:32:55.282930: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1858] Adding visible gpu devices: 0\n",
            "2020-11-17 18:32:55.287423: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\n",
            "2020-11-17 18:32:59.304510: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1257] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
            "2020-11-17 18:32:59.304565: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1263]      0 \n",
            "2020-11-17 18:32:59.304642: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1276] 0:   N \n",
            "2020-11-17 18:32:59.308725: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-11-17 18:32:59.309446: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-11-17 18:32:59.310008: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
            "2020-11-17 18:32:59.310054: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1402] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 14748 MB memory) -> physical GPU (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0)\n",
            "[Epoch 0] Training Loss: 0.865, Training Accuracy: 36920/49920 (74.0%), Test Loss: 0.269, Test Accuracy: 9082/10000 (90.8%) seconds per epoch: 317.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mOxWs5HlYm9X",
        "outputId": "d0f17ec4-c18d-482b-b7b4-ce5f8ea0a0b5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "\"\"\"\n",
        "On ImageNet we experimented with ResNet-50, ResNet-152[He et al., 2016] and DenseNet-161[Huang et al.,2017].  \n",
        "For these architectures we used pretrained mod-els  fromPyTorch.torchvision.   For  each  of  themodels we ran \n",
        "SWA for10epochs with a cyclical learn-ing rate schedule with the same parameters for all models\n",
        "(the details can be found in the Appendix), \n",
        "and report themean and standard deviation of test error averaged over3runs. The results are shown in Table 2.\n",
        "\"\"\"\n",
        "// Run 10 epochs of cyclical LR at the end here with SWA\n",
        "// https://arxiv.org/pdf/1803.05407.pdf\n",
        "// https://pytorch.org/blog/stochastic-weight-averaging-in-pytorch/\n",
        "// https://github.com/izmailovpavel/contrib_swa_examples/blob/master/train.py"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"On ImageNet we experimented with ResNet-50, ResNet-152[He et al., 2016] and DenseNet-161[Huang et al.,2017].  \\nFor these architectures we used pretrained mod-els  fromPyTorch.torchvision.   For  each  of  themodels we ran \\nSWA for10epochs with a cyclical learn-ing rate schedule with the same parameters for all models\\n(the details can be found in the Appendix), \\nand report themean and standard deviation of test error averaged over3runs. The results are shown in Table 2.\"\n"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mIpxzm0swMvv",
        "outputId": "588cfacf-8d07-48c2-a314-1f19362f6af4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "func get_cosine_annealing_lr(step: Int, steps_per_epoch: Int, base_lr: Float = 0.003, max_lr: Float = 0.1) -> Float? {\n",
        "    return base_lr + 0.5*(max_lr - base_lr)*(1 + cos(Float.pi * Float(step)/Float(steps_per_epoch)))\n",
        "}\n",
        "get_cosine_annealing_lr(step: 1, steps_per_epoch: 100)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "▿ Optional<Float>\n",
              "  - some : 0.09997606\n"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F18FmtFIsxWP",
        "outputId": "6314e8ca-05d1-41e4-f447-b2c6d5ea40a4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "var swaModelWeights: BigTransfer.TangentVector = BigTransfer.TangentVector.zero\n",
        "swaModelWeights = BigTransfer.TangentVector(copying: swaModelWeights, to: device)\n",
        "\n",
        "var numCycles = 5\n",
        "for (epoch, batches) in dataset.training.prefix(numCycles).enumerated() {\n",
        "    let start = Date()\n",
        "    var trainStats = Statistics(on: device)\n",
        "    var testStats = Statistics(on: device)\n",
        "    \n",
        "    var currStep: Int = 0\n",
        "    Context.local.learningPhase = .training\n",
        "    for batch in batches {\n",
        "        if let new_lr = get_cosine_annealing_lr(step: currStep, steps_per_epoch: stepsPerEpoch, base_lr: 0.00001, max_lr: 0.001) {\n",
        "          optimizer.learningRate = new_lr\n",
        "          print(new_lr)\n",
        "          curr_step = curr_step + 1\n",
        "        }\n",
        "        else {\n",
        "          continue\n",
        "        }\n",
        "\n",
        "        var (eagerImages, eagerLabels) = (batch.data, batch.label)\n",
        "        var npImages = eagerImages.makeNumpyArray()\n",
        "        var resized = tf.image.resize(npImages, [resize_size.0, resize_size.0])\n",
        "        var cropped = tf.image.random_crop(resized, [batchSize, resize_size.1, resize_size.1, 3])\n",
        "        var flipped = tf.image.random_flip_left_right(cropped)\n",
        "        var mixed_up = flipped\n",
        "        var newLabels = Tensor<Float>(Tensor<Int32>(oneHotAtIndices: eagerLabels, depth: 10))\n",
        "        if mixup_alpha > 0.0 {\n",
        "          var npLabels = newLabels.makeNumpyArray()\n",
        "          mixed_up = beta * mixed_up + (1 - beta) * tf.reverse(mixed_up, axis: [0])\n",
        "          npLabels = beta * npLabels + (1 - beta) * tf.reverse(npLabels, axis: [0])\n",
        "          newLabels = Tensor<Float>(numpy: npLabels.numpy())!\n",
        "        }\n",
        "        eagerImages = Tensor<Float>(numpy: mixed_up.numpy())!\n",
        "        let images = Tensor(copying: eagerImages, to: device)\n",
        "        let labels = Tensor(copying: newLabels, to: device)\n",
        "        let 𝛁model = TensorFlow.gradient(at: resnetv2) { resnetv2 -> Tensor<Float> in\n",
        "            let ŷ = resnetv2(images)\n",
        "            let loss = softmaxCrossEntropy(logits: ŷ, probabilities: labels)\n",
        "            trainStats.update(logits: ŷ, labels: labels, loss: loss)\n",
        "            return loss\n",
        "        }\n",
        "\n",
        "        optimizer.update(&resnetv2, along: 𝛁model)\n",
        "        LazyTensorBarrier()\n",
        "\n",
        "        currStep = currStep + 1\n",
        "        if currStep == stepsPerEpoch - 1 {\n",
        "          print(\"Adding model weights\")\n",
        "          swaModelWeights = swaModelWeights + 𝛁model\n",
        "        }\n",
        "    }\n",
        "\n",
        "    Context.local.learningPhase = .inference\n",
        "    for batch in dataset.validation {\n",
        "        var (eagerImages, eagerLabels) = (batch.data, batch.label)\n",
        "        var npImages = eagerImages.makeNumpyArray()\n",
        "        var resized = tf.image.resize(npImages, [resize_size.0, resize_size.0])\n",
        "        eagerImages = Tensor<Float>(numpy: resized.numpy())!\n",
        "        var newLabels = Tensor<Float>(Tensor<Int32>(oneHotAtIndices: eagerLabels, depth: 10))\n",
        "        let images = Tensor(copying: eagerImages, to: device)\n",
        "        let labels = Tensor(copying: newLabels, to: device)\n",
        "        let ŷ = resnetv2(images)\n",
        "        let loss = softmaxCrossEntropy(logits: ŷ, probabilities: labels)\n",
        "        LazyTensorBarrier()\n",
        "        testStats.update(logits: ŷ, labels: labels, loss: loss)\n",
        "    }\n",
        "\n",
        "    print(\n",
        "        \"\"\"\n",
        "        [Epoch \\(epoch)] \\\n",
        "        Training Loss: \\(String(format: \"%.3f\", trainStats.averageLoss)), \\\n",
        "        Training Accuracy: \\(trainStats.correctGuessCount)/\\(trainStats.totalGuessCount) \\\n",
        "        (\\(String(format: \"%.1f\", trainStats.accuracy))%), \\\n",
        "        Test Loss: \\(String(format: \"%.3f\", testStats.averageLoss)), \\\n",
        "        Test Accuracy: \\(testStats.correctGuessCount)/\\(testStats.totalGuessCount) \\\n",
        "        (\\(String(format: \"%.1f\", testStats.accuracy))%) \\\n",
        "        seconds per epoch: \\(String(format: \"%.1f\", Date().timeIntervalSince(start)))\n",
        "        \"\"\")\n",
        "}\n",
        "\n",
        "\n",
        "// optimizer.update(&resnetv2, along: swaModelWeights/Tensor<Float>(10.0*Float(stepsPerEpoch)))\n",
        "\n",
        "// print(BigTransfer.TangentVector.zero.inputStem.conv.filter + 1.0)\n",
        "// print(swaModelWeights)"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.001\n",
            "0.000999984\n",
            "0.0009999359\n",
            "0.0009998555\n",
            "0.0009997431\n",
            "0.0009995986\n",
            "0.000999422\n",
            "0.0009992133\n",
            "0.0009989726\n",
            "0.0009986997\n",
            "0.0009983949\n",
            "0.0009980581\n",
            "0.0009976892\n",
            "0.0009972884\n",
            "0.0009968556\n",
            "0.0009963909\n",
            "0.0009958943\n",
            "0.000995366\n",
            "0.0009948057\n",
            "0.0009942136\n",
            "0.00099359\n",
            "0.0009929345\n",
            "0.0009922474\n",
            "0.0009915285\n",
            "0.0009907783\n",
            "0.0009899965\n",
            "0.0009891831\n",
            "0.0009883384\n",
            "0.0009874622\n",
            "0.0009865549\n",
            "0.0009856162\n",
            "0.0009846465\n",
            "0.0009836454\n",
            "0.0009826135\n",
            "0.0009815504\n",
            "0.0009804566\n",
            "0.0009793317\n",
            "0.0009781762\n",
            "0.0009769899\n",
            "0.0009757731\n",
            "0.00097452564\n",
            "0.00097324775\n",
            "0.0009719395\n",
            "0.0009706008\n",
            "0.00096923206\n",
            "0.00096783316\n",
            "0.00096640416\n",
            "0.00096494536\n",
            "0.0009634565\n",
            "0.00096193806\n",
            "0.0009603899\n",
            "0.0009588122\n",
            "0.00095720513\n",
            "0.0009555686\n",
            "0.00095390296\n",
            "0.00095220807\n",
            "0.0009504842\n",
            "0.00094873144\n",
            "0.0009469498\n",
            "0.0009451396\n",
            "0.00094330084\n",
            "0.0009414336\n",
            "0.000939538\n",
            "0.0009376143\n",
            "0.0009356624\n",
            "0.0009336827\n",
            "0.0009316751\n",
            "0.0009296398\n",
            "0.00092757697\n",
            "0.00092548673\n",
            "0.0009233692\n",
            "0.0009212245\n",
            "0.0009190528\n",
            "0.00091685425\n",
            "0.0009146289\n",
            "0.00091237715\n",
            "0.00091009884\n",
            "0.0009077942\n",
            "0.0009054635\n",
            "0.0009031068\n",
            "0.00090072426\n",
            "0.0008983161\n",
            "0.0008958823\n",
            "0.0008934233\n",
            "0.00089093886\n",
            "0.0008884296\n",
            "0.00088589534\n",
            "0.0008833364\n",
            "0.0008807529\n",
            "0.000878145\n",
            "0.00087551295\n",
            "0.0008728568\n",
            "0.0008701768\n",
            "0.0008674731\n",
            "0.0008647459\n",
            "0.0008619953\n",
            "0.00085922156\n",
            "0.00085642486\n",
            "0.0008536054\n",
            "0.00085076323\n",
            "0.0008478987\n",
            "0.0008450119\n",
            "0.0008421029\n",
            "0.0008391722\n",
            "0.0008362197\n",
            "0.0008332458\n",
            "0.0008302505\n",
            "0.00082723424\n",
            "0.000824197\n",
            "0.000821139\n",
            "0.0008180606\n",
            "0.00081496174\n",
            "0.0008118429\n",
            "0.00080870406\n",
            "0.00080554554\n",
            "0.0008023675\n",
            "0.00079917023\n",
            "0.0007959538\n",
            "0.0007927185\n",
            "0.0007894646\n",
            "0.0007861922\n",
            "0.0007829015\n",
            "0.00077959284\n",
            "0.0007762663\n",
            "0.0007729222\n",
            "0.00076956063\n",
            "0.000766182\n",
            "0.0007627865\n",
            "0.00075937406\n",
            "0.0007559452\n",
            "0.0007525001\n",
            "0.0007490389\n",
            "0.0007455619\n",
            "0.0007420693\n",
            "0.0007385613\n",
            "0.0007350381\n",
            "0.0007315\n",
            "0.0007279472\n",
            "0.0007243799\n",
            "0.00072079845\n",
            "0.0007172029\n",
            "0.0007135936\n",
            "0.00070997083\n",
            "0.0007063347\n",
            "0.0007026856\n",
            "0.00069902354\n",
            "0.000695349\n",
            "0.0006916621\n",
            "0.000687963\n",
            "0.00068425207\n",
            "0.0006805295\n",
            "0.0006767956\n",
            "0.00067305047\n",
            "0.00066929444\n",
            "0.0006655279\n",
            "0.00066175073\n",
            "0.0006579635\n",
            "0.0006541664\n",
            "0.0006503595\n",
            "0.0006465432\n",
            "0.00064271776\n",
            "0.0006388833\n",
            "0.0006350403\n",
            "0.0006311887\n",
            "0.000627329\n",
            "0.0006234613\n",
            "0.00061958603\n",
            "0.0006157033\n",
            "0.0006118133\n",
            "0.0006079164\n",
            "0.0006040128\n",
            "0.0006001028\n",
            "0.0005961867\n",
            "0.0005922646\n",
            "0.00058833684\n",
            "0.0005844037\n",
            "0.0005804654\n",
            "0.00057652214\n",
            "0.0005725744\n",
            "0.00056862214\n",
            "0.00056466577\n",
            "0.00056070555\n",
            "0.0005567417\n",
            "0.0005527744\n",
            "0.0005488042\n",
            "0.000544831\n",
            "0.00054085534\n",
            "0.0005368773\n",
            "0.00053289713\n",
            "0.00052891526\n",
            "0.0005249317\n",
            "0.000520947\n",
            "0.0005169611\n",
            "0.0005129746\n",
            "0.0005089875\n",
            "0.0005050001\n",
            "0.0005010128\n",
            "0.0004970256\n",
            "0.000493039\n",
            "0.00048905326\n",
            "0.00048506845\n",
            "0.00048108495\n",
            "0.00047710302\n",
            "0.00047312287\n",
            "0.0004691448\n",
            "0.00046516905\n",
            "0.000461196\n",
            "0.00045722572\n",
            "0.0004532585\n",
            "0.00044929463\n",
            "0.00044533436\n",
            "0.00044137804\n",
            "0.00043742577\n",
            "0.000433478\n",
            "0.00042953482\n",
            "0.0004255965\n",
            "0.00042166337\n",
            "0.00041773557\n",
            "0.0004138135\n",
            "0.00040989733\n",
            "0.00040598738\n",
            "0.00040208377\n",
            "0.0003981869\n",
            "0.0003942969\n",
            "0.00039041415\n",
            "0.00038653877\n",
            "0.0003826712\n",
            "0.0003788115\n",
            "0.0003749599\n",
            "0.00037111682\n",
            "0.0003672824\n",
            "0.00036345696\n",
            "0.00035964063\n",
            "0.00035583385\n",
            "0.0003520367\n",
            "0.0003482494\n",
            "0.00034447236\n",
            "0.00034070565\n",
            "0.00033694966\n",
            "0.00033320457\n",
            "0.00032947067\n",
            "0.00032574806\n",
            "0.0003220372\n",
            "0.00031833808\n",
            "0.00031465117\n",
            "0.00031097655\n",
            "0.0003073146\n",
            "0.00030366544\n",
            "0.00030002932\n",
            "0.0002964065\n",
            "0.00029279722\n",
            "0.00028920168\n",
            "0.00028562028\n",
            "0.00028205308\n",
            "0.00027850023\n",
            "0.00027496208\n",
            "0.0002714389\n",
            "0.00026793094\n",
            "0.00026443828\n",
            "0.0002609612\n",
            "0.00025750007\n",
            "0.00025405493\n",
            "0.00025062604\n",
            "0.0002472138\n",
            "0.00024381814\n",
            "0.00024043943\n",
            "0.00023707791\n",
            "0.00023373387\n",
            "0.0002304074\n",
            "0.0002270987\n",
            "0.000223808\n",
            "0.00022053556\n",
            "0.00021728165\n",
            "0.00021404645\n",
            "0.00021083\n",
            "0.00020763265\n",
            "0.0002044546\n",
            "0.00020129613\n",
            "0.00019815729\n",
            "0.00019503836\n",
            "0.00019193963\n",
            "0.00018886113\n",
            "0.00018580313\n",
            "0.00018276596\n",
            "0.00017974958\n",
            "0.0001767543\n",
            "0.00017378032\n",
            "0.00017082795\n",
            "0.00016789726\n",
            "0.00016498835\n",
            "0.0001621015\n",
            "0.0001592369\n",
            "0.0001563948\n",
            "0.00015357525\n",
            "0.0001507786\n",
            "0.00014800484\n",
            "0.00014525428\n",
            "0.0001425271\n",
            "0.00013982339\n",
            "0.00013714333\n",
            "0.00013448727\n",
            "0.00013185515\n",
            "0.00012924724\n",
            "0.00012666378\n",
            "0.00012410482\n",
            "0.00012157057\n",
            "0.00011906118\n",
            "0.00011657695\n",
            "0.00011411788\n",
            "0.0001116841\n",
            "0.00010927586\n",
            "0.000106893305\n",
            "0.00010453665\n",
            "0.0001022059\n",
            "9.990134e-05\n",
            "9.762302e-05\n",
            "9.537114e-05\n",
            "9.314592e-05\n",
            "9.094732e-05\n",
            "8.87756e-05\n",
            "8.663096e-05\n",
            "8.4513405e-05\n",
            "8.2423154e-05\n",
            "8.036035e-05\n",
            "7.832506e-05\n",
            "7.631743e-05\n",
            "7.4337644e-05\n",
            "7.238587e-05\n",
            "7.0462076e-05\n",
            "6.8566515e-05\n",
            "6.669924e-05\n",
            "6.4860535e-05\n",
            "6.305027e-05\n",
            "6.1268685e-05\n",
            "5.9515947e-05\n",
            "5.779207e-05\n",
            "5.6097226e-05\n",
            "5.443153e-05\n",
            "5.2795047e-05\n",
            "5.1187886e-05\n",
            "4.9610175e-05\n",
            "4.8062084e-05\n",
            "4.6543555e-05\n",
            "4.50548e-05\n",
            "4.359599e-05\n",
            "4.2167005e-05\n",
            "4.076809e-05\n",
            "3.9399267e-05\n",
            "3.8060716e-05\n",
            "3.6752404e-05\n",
            "3.5474484e-05\n",
            "3.4227043e-05\n",
            "3.301017e-05\n",
            "3.182389e-05\n",
            "3.066832e-05\n",
            "2.9543531e-05\n",
            "2.844963e-05\n",
            "2.7386619e-05\n",
            "2.6354617e-05\n",
            "2.5353684e-05\n",
            "2.4383819e-05\n",
            "2.3445169e-05\n",
            "2.2537764e-05\n",
            "2.1661694e-05\n",
            "2.0816955e-05\n",
            "2.0003641e-05\n",
            "1.9221776e-05\n",
            "1.847148e-05\n",
            "1.7752727e-05\n",
            "1.7065602e-05\n",
            "1.6410104e-05\n",
            "1.5786383e-05\n",
            "1.51943495e-05\n",
            "1.46340935e-05\n",
            "1.4105701e-05\n",
            "1.3609113e-05\n",
            "1.3144421e-05\n",
            "1.2711652e-05\n",
            "1.2310836e-05\n",
            "1.1942002e-05\n",
            "1.1605122e-05\n",
            "1.1300284e-05\n",
            "1.1027487e-05\n",
            "1.0786732e-05\n",
            "1.0578048e-05\n",
            "1.0401464e-05\n",
            "1.0256953e-05\n",
            "1.0144541e-05\n",
            "1.0064231e-05\n",
            "Adding model weights\n",
            "1.00160505e-05\n",
            "[Epoch 0] Training Loss: 0.169, Training Accuracy: 47010/49920 (94.2%), Test Loss: 0.144, Test Accuracy: 9509/10000 (95.1%) seconds per epoch: 278.6\n",
            "0.001\n",
            "0.000999984\n",
            "0.0009999359\n",
            "0.0009998555\n",
            "0.0009997431\n",
            "0.0009995986\n",
            "0.000999422\n",
            "0.0009992133\n",
            "0.0009989726\n",
            "0.0009986997\n",
            "0.0009983949\n",
            "0.0009980581\n",
            "0.0009976892\n",
            "0.0009972884\n",
            "0.0009968556\n",
            "0.0009963909\n",
            "0.0009958943\n",
            "0.000995366\n",
            "0.0009948057\n",
            "0.0009942136\n",
            "0.00099359\n",
            "0.0009929345\n",
            "0.0009922474\n",
            "0.0009915285\n",
            "0.0009907783\n",
            "0.0009899965\n",
            "0.0009891831\n",
            "0.0009883384\n",
            "0.0009874622\n",
            "0.0009865549\n",
            "0.0009856162\n",
            "0.0009846465\n",
            "0.0009836454\n",
            "0.0009826135\n",
            "0.0009815504\n",
            "0.0009804566\n",
            "0.0009793317\n",
            "0.0009781762\n",
            "0.0009769899\n",
            "0.0009757731\n",
            "0.00097452564\n",
            "0.00097324775\n",
            "0.0009719395\n",
            "0.0009706008\n",
            "0.00096923206\n",
            "0.00096783316\n",
            "0.00096640416\n",
            "0.00096494536\n",
            "0.0009634565\n",
            "0.00096193806\n",
            "0.0009603899\n",
            "0.0009588122\n",
            "0.00095720513\n",
            "0.0009555686\n",
            "0.00095390296\n",
            "0.00095220807\n",
            "0.0009504842\n",
            "0.00094873144\n",
            "0.0009469498\n",
            "0.0009451396\n",
            "0.00094330084\n",
            "0.0009414336\n",
            "0.000939538\n",
            "0.0009376143\n",
            "0.0009356624\n",
            "0.0009336827\n",
            "0.0009316751\n",
            "0.0009296398\n",
            "0.00092757697\n",
            "0.00092548673\n",
            "0.0009233692\n",
            "0.0009212245\n",
            "0.0009190528\n",
            "0.00091685425\n",
            "0.0009146289\n",
            "0.00091237715\n",
            "0.00091009884\n",
            "0.0009077942\n",
            "0.0009054635\n",
            "0.0009031068\n",
            "0.00090072426\n",
            "0.0008983161\n",
            "0.0008958823\n",
            "0.0008934233\n",
            "0.00089093886\n",
            "0.0008884296\n",
            "0.00088589534\n",
            "0.0008833364\n",
            "0.0008807529\n",
            "0.000878145\n",
            "0.00087551295\n",
            "0.0008728568\n",
            "0.0008701768\n",
            "0.0008674731\n",
            "0.0008647459\n",
            "0.0008619953\n",
            "0.00085922156\n",
            "0.00085642486\n",
            "0.0008536054\n",
            "0.00085076323\n",
            "0.0008478987\n",
            "0.0008450119\n",
            "0.0008421029\n",
            "0.0008391722\n",
            "0.0008362197\n",
            "0.0008332458\n",
            "0.0008302505\n",
            "0.00082723424\n",
            "0.000824197\n",
            "0.000821139\n",
            "0.0008180606\n",
            "0.00081496174\n",
            "0.0008118429\n",
            "0.00080870406\n",
            "0.00080554554\n",
            "0.0008023675\n",
            "0.00079917023\n",
            "0.0007959538\n",
            "0.0007927185\n",
            "0.0007894646\n",
            "0.0007861922\n",
            "0.0007829015\n",
            "0.00077959284\n",
            "0.0007762663\n",
            "0.0007729222\n",
            "0.00076956063\n",
            "0.000766182\n",
            "0.0007627865\n",
            "0.00075937406\n",
            "0.0007559452\n",
            "0.0007525001\n",
            "0.0007490389\n",
            "0.0007455619\n",
            "0.0007420693\n",
            "0.0007385613\n",
            "0.0007350381\n",
            "0.0007315\n",
            "0.0007279472\n",
            "0.0007243799\n",
            "0.00072079845\n",
            "0.0007172029\n",
            "0.0007135936\n",
            "0.00070997083\n",
            "0.0007063347\n",
            "0.0007026856\n",
            "0.00069902354\n",
            "0.000695349\n",
            "0.0006916621\n",
            "0.000687963\n",
            "0.00068425207\n",
            "0.0006805295\n",
            "0.0006767956\n",
            "0.00067305047\n",
            "0.00066929444\n",
            "0.0006655279\n",
            "0.00066175073\n",
            "0.0006579635\n",
            "0.0006541664\n",
            "0.0006503595\n",
            "0.0006465432\n",
            "0.00064271776\n",
            "0.0006388833\n",
            "0.0006350403\n",
            "0.0006311887\n",
            "0.000627329\n",
            "0.0006234613\n",
            "0.00061958603\n",
            "0.0006157033\n",
            "0.0006118133\n",
            "0.0006079164\n",
            "0.0006040128\n",
            "0.0006001028\n",
            "0.0005961867\n",
            "0.0005922646\n",
            "0.00058833684\n",
            "0.0005844037\n",
            "0.0005804654\n",
            "0.00057652214\n",
            "0.0005725744\n",
            "0.00056862214\n",
            "0.00056466577\n",
            "0.00056070555\n",
            "0.0005567417\n",
            "0.0005527744\n",
            "0.0005488042\n",
            "0.000544831\n",
            "0.00054085534\n",
            "0.0005368773\n",
            "0.00053289713\n",
            "0.00052891526\n",
            "0.0005249317\n",
            "0.000520947\n",
            "0.0005169611\n",
            "0.0005129746\n",
            "0.0005089875\n",
            "0.0005050001\n",
            "0.0005010128\n",
            "0.0004970256\n",
            "0.000493039\n",
            "0.00048905326\n",
            "0.00048506845\n",
            "0.00048108495\n",
            "0.00047710302\n",
            "0.00047312287\n",
            "0.0004691448\n",
            "0.00046516905\n",
            "0.000461196\n",
            "0.00045722572\n",
            "0.0004532585\n",
            "0.00044929463\n",
            "0.00044533436\n",
            "0.00044137804\n",
            "0.00043742577\n",
            "0.000433478\n",
            "0.00042953482\n",
            "0.0004255965\n",
            "0.00042166337\n",
            "0.00041773557\n",
            "0.0004138135\n",
            "0.00040989733\n",
            "0.00040598738\n",
            "0.00040208377\n",
            "0.0003981869\n",
            "0.0003942969\n",
            "0.00039041415\n",
            "0.00038653877\n",
            "0.0003826712\n",
            "0.0003788115\n",
            "0.0003749599\n",
            "0.00037111682\n",
            "0.0003672824\n",
            "0.00036345696\n",
            "0.00035964063\n",
            "0.00035583385\n",
            "0.0003520367\n",
            "0.0003482494\n",
            "0.00034447236\n",
            "0.00034070565\n",
            "0.00033694966\n",
            "0.00033320457\n",
            "0.00032947067\n",
            "0.00032574806\n",
            "0.0003220372\n",
            "0.00031833808\n",
            "0.00031465117\n",
            "0.00031097655\n",
            "0.0003073146\n",
            "0.00030366544\n",
            "0.00030002932\n",
            "0.0002964065\n",
            "0.00029279722\n",
            "0.00028920168\n",
            "0.00028562028\n",
            "0.00028205308\n",
            "0.00027850023\n",
            "0.00027496208\n",
            "0.0002714389\n",
            "0.00026793094\n",
            "0.00026443828\n",
            "0.0002609612\n",
            "0.00025750007\n",
            "0.00025405493\n",
            "0.00025062604\n",
            "0.0002472138\n",
            "0.00024381814\n",
            "0.00024043943\n",
            "0.00023707791\n",
            "0.00023373387\n",
            "0.0002304074\n",
            "0.0002270987\n",
            "0.000223808\n",
            "0.00022053556\n",
            "0.00021728165\n",
            "0.00021404645\n",
            "0.00021083\n",
            "0.00020763265\n",
            "0.0002044546\n",
            "0.00020129613\n",
            "0.00019815729\n",
            "0.00019503836\n",
            "0.00019193963\n",
            "0.00018886113\n",
            "0.00018580313\n",
            "0.00018276596\n",
            "0.00017974958\n",
            "0.0001767543\n",
            "0.00017378032\n",
            "0.00017082795\n",
            "0.00016789726\n",
            "0.00016498835\n",
            "0.0001621015\n",
            "0.0001592369\n",
            "0.0001563948\n",
            "0.00015357525\n",
            "0.0001507786\n",
            "0.00014800484\n",
            "0.00014525428\n",
            "0.0001425271\n",
            "0.00013982339\n",
            "0.00013714333\n",
            "0.00013448727\n",
            "0.00013185515\n",
            "0.00012924724\n",
            "0.00012666378\n",
            "0.00012410482\n",
            "0.00012157057\n",
            "0.00011906118\n",
            "0.00011657695\n",
            "0.00011411788\n",
            "0.0001116841\n",
            "0.00010927586\n",
            "0.000106893305\n",
            "0.00010453665\n",
            "0.0001022059\n",
            "9.990134e-05\n",
            "9.762302e-05\n",
            "9.537114e-05\n",
            "9.314592e-05\n",
            "9.094732e-05\n",
            "8.87756e-05\n",
            "8.663096e-05\n",
            "8.4513405e-05\n",
            "8.2423154e-05\n",
            "8.036035e-05\n",
            "7.832506e-05\n",
            "7.631743e-05\n",
            "7.4337644e-05\n",
            "7.238587e-05\n",
            "7.0462076e-05\n",
            "6.8566515e-05\n",
            "6.669924e-05\n",
            "6.4860535e-05\n",
            "6.305027e-05\n",
            "6.1268685e-05\n",
            "5.9515947e-05\n",
            "5.779207e-05\n",
            "5.6097226e-05\n",
            "5.443153e-05\n",
            "5.2795047e-05\n",
            "5.1187886e-05\n",
            "4.9610175e-05\n",
            "4.8062084e-05\n",
            "4.6543555e-05\n",
            "4.50548e-05\n",
            "4.359599e-05\n",
            "4.2167005e-05\n",
            "4.076809e-05\n",
            "3.9399267e-05\n",
            "3.8060716e-05\n",
            "3.6752404e-05\n",
            "3.5474484e-05\n",
            "3.4227043e-05\n",
            "3.301017e-05\n",
            "3.182389e-05\n",
            "3.066832e-05\n",
            "2.9543531e-05\n",
            "2.844963e-05\n",
            "2.7386619e-05\n",
            "2.6354617e-05\n",
            "2.5353684e-05\n",
            "2.4383819e-05\n",
            "2.3445169e-05\n",
            "2.2537764e-05\n",
            "2.1661694e-05\n",
            "2.0816955e-05\n",
            "2.0003641e-05\n",
            "1.9221776e-05\n",
            "1.847148e-05\n",
            "1.7752727e-05\n",
            "1.7065602e-05\n",
            "1.6410104e-05\n",
            "1.5786383e-05\n",
            "1.51943495e-05\n",
            "1.46340935e-05\n",
            "1.4105701e-05\n",
            "1.3609113e-05\n",
            "1.3144421e-05\n",
            "1.2711652e-05\n",
            "1.2310836e-05\n",
            "1.1942002e-05\n",
            "1.1605122e-05\n",
            "1.1300284e-05\n",
            "1.1027487e-05\n",
            "1.0786732e-05\n",
            "1.0578048e-05\n",
            "1.0401464e-05\n",
            "1.0256953e-05\n",
            "1.0144541e-05\n",
            "1.0064231e-05\n",
            "Adding model weights\n",
            "1.00160505e-05\n",
            "[Epoch 1] Training Loss: 0.125, Training Accuracy: 47779/49920 (95.7%), Test Loss: 0.135, Test Accuracy: 9540/10000 (95.4%) seconds per epoch: 259.5\n",
            "0.001\n",
            "0.000999984\n",
            "0.0009999359\n",
            "0.0009998555\n",
            "0.0009997431\n",
            "0.0009995986\n",
            "0.000999422\n",
            "0.0009992133\n",
            "0.0009989726\n",
            "0.0009986997\n",
            "0.0009983949\n",
            "0.0009980581\n",
            "0.0009976892\n",
            "0.0009972884\n",
            "0.0009968556\n",
            "0.0009963909\n",
            "0.0009958943\n",
            "0.000995366\n",
            "0.0009948057\n",
            "0.0009942136\n",
            "0.00099359\n",
            "0.0009929345\n",
            "0.0009922474\n",
            "0.0009915285\n",
            "0.0009907783\n",
            "0.0009899965\n",
            "0.0009891831\n",
            "0.0009883384\n",
            "0.0009874622\n",
            "0.0009865549\n",
            "0.0009856162\n",
            "0.0009846465\n",
            "0.0009836454\n",
            "0.0009826135\n",
            "0.0009815504\n",
            "0.0009804566\n",
            "0.0009793317\n",
            "0.0009781762\n",
            "0.0009769899\n",
            "0.0009757731\n",
            "0.00097452564\n",
            "0.00097324775\n",
            "0.0009719395\n",
            "0.0009706008\n",
            "0.00096923206\n",
            "0.00096783316\n",
            "0.00096640416\n",
            "0.00096494536\n",
            "0.0009634565\n",
            "0.00096193806\n",
            "0.0009603899\n",
            "0.0009588122\n",
            "0.00095720513\n",
            "0.0009555686\n",
            "0.00095390296\n",
            "0.00095220807\n",
            "0.0009504842\n",
            "0.00094873144\n",
            "0.0009469498\n",
            "0.0009451396\n",
            "0.00094330084\n",
            "0.0009414336\n",
            "0.000939538\n",
            "0.0009376143\n",
            "0.0009356624\n",
            "0.0009336827\n",
            "0.0009316751\n",
            "0.0009296398\n",
            "0.00092757697\n",
            "0.00092548673\n",
            "0.0009233692\n",
            "0.0009212245\n",
            "0.0009190528\n",
            "0.00091685425\n",
            "0.0009146289\n",
            "0.00091237715\n",
            "0.00091009884\n",
            "0.0009077942\n",
            "0.0009054635\n",
            "0.0009031068\n",
            "0.00090072426\n",
            "0.0008983161\n",
            "0.0008958823\n",
            "0.0008934233\n",
            "0.00089093886\n",
            "0.0008884296\n",
            "0.00088589534\n",
            "0.0008833364\n",
            "0.0008807529\n",
            "0.000878145\n",
            "0.00087551295\n",
            "0.0008728568\n",
            "0.0008701768\n",
            "0.0008674731\n",
            "0.0008647459\n",
            "0.0008619953\n",
            "0.00085922156\n",
            "0.00085642486\n",
            "0.0008536054\n",
            "0.00085076323\n",
            "0.0008478987\n",
            "0.0008450119\n",
            "0.0008421029\n",
            "0.0008391722\n",
            "0.0008362197\n",
            "0.0008332458\n",
            "0.0008302505\n",
            "0.00082723424\n",
            "0.000824197\n",
            "0.000821139\n",
            "0.0008180606\n",
            "0.00081496174\n",
            "0.0008118429\n",
            "0.00080870406\n",
            "0.00080554554\n",
            "0.0008023675\n",
            "0.00079917023\n",
            "0.0007959538\n",
            "0.0007927185\n",
            "0.0007894646\n",
            "0.0007861922\n",
            "0.0007829015\n",
            "0.00077959284\n",
            "0.0007762663\n",
            "0.0007729222\n",
            "0.00076956063\n",
            "0.000766182\n",
            "0.0007627865\n",
            "0.00075937406\n",
            "0.0007559452\n",
            "0.0007525001\n",
            "0.0007490389\n",
            "0.0007455619\n",
            "0.0007420693\n",
            "0.0007385613\n",
            "0.0007350381\n",
            "0.0007315\n",
            "0.0007279472\n",
            "0.0007243799\n",
            "0.00072079845\n",
            "0.0007172029\n",
            "0.0007135936\n",
            "0.00070997083\n",
            "0.0007063347\n",
            "0.0007026856\n",
            "0.00069902354\n",
            "0.000695349\n",
            "0.0006916621\n",
            "0.000687963\n",
            "0.00068425207\n",
            "0.0006805295\n",
            "0.0006767956\n",
            "0.00067305047\n",
            "0.00066929444\n",
            "0.0006655279\n",
            "0.00066175073\n",
            "0.0006579635\n",
            "0.0006541664\n",
            "0.0006503595\n",
            "0.0006465432\n",
            "0.00064271776\n",
            "0.0006388833\n",
            "0.0006350403\n",
            "0.0006311887\n",
            "0.000627329\n",
            "0.0006234613\n",
            "0.00061958603\n",
            "0.0006157033\n",
            "0.0006118133\n",
            "0.0006079164\n",
            "0.0006040128\n",
            "0.0006001028\n",
            "0.0005961867\n",
            "0.0005922646\n",
            "0.00058833684\n",
            "0.0005844037\n",
            "0.0005804654\n",
            "0.00057652214\n",
            "0.0005725744\n",
            "0.00056862214\n",
            "0.00056466577\n",
            "0.00056070555\n",
            "0.0005567417\n",
            "0.0005527744\n",
            "0.0005488042\n",
            "0.000544831\n",
            "0.00054085534\n",
            "0.0005368773\n",
            "0.00053289713\n",
            "0.00052891526\n",
            "0.0005249317\n",
            "0.000520947\n",
            "0.0005169611\n",
            "0.0005129746\n",
            "0.0005089875\n",
            "0.0005050001\n",
            "0.0005010128\n",
            "0.0004970256\n",
            "0.000493039\n",
            "0.00048905326\n",
            "0.00048506845\n",
            "0.00048108495\n",
            "0.00047710302\n",
            "0.00047312287\n",
            "0.0004691448\n",
            "0.00046516905\n",
            "0.000461196\n",
            "0.00045722572\n",
            "0.0004532585\n",
            "0.00044929463\n",
            "0.00044533436\n",
            "0.00044137804\n",
            "0.00043742577\n",
            "0.000433478\n",
            "0.00042953482\n",
            "0.0004255965\n",
            "0.00042166337\n",
            "0.00041773557\n",
            "0.0004138135\n",
            "0.00040989733\n",
            "0.00040598738\n",
            "0.00040208377\n",
            "0.0003981869\n",
            "0.0003942969\n",
            "0.00039041415\n",
            "0.00038653877\n",
            "0.0003826712\n",
            "0.0003788115\n",
            "0.0003749599\n",
            "0.00037111682\n",
            "0.0003672824\n",
            "0.00036345696\n",
            "0.00035964063\n",
            "0.00035583385\n",
            "0.0003520367\n",
            "0.0003482494\n",
            "0.00034447236\n",
            "0.00034070565\n",
            "0.00033694966\n",
            "0.00033320457\n",
            "0.00032947067\n",
            "0.00032574806\n",
            "0.0003220372\n",
            "0.00031833808\n",
            "0.00031465117\n",
            "0.00031097655\n",
            "0.0003073146\n",
            "0.00030366544\n",
            "0.00030002932\n",
            "0.0002964065\n",
            "0.00029279722\n",
            "0.00028920168\n",
            "0.00028562028\n",
            "0.00028205308\n",
            "0.00027850023\n",
            "0.00027496208\n",
            "0.0002714389\n",
            "0.00026793094\n",
            "0.00026443828\n",
            "0.0002609612\n",
            "0.00025750007\n",
            "0.00025405493\n",
            "0.00025062604\n",
            "0.0002472138\n",
            "0.00024381814\n",
            "0.00024043943\n",
            "0.00023707791\n",
            "0.00023373387\n",
            "0.0002304074\n",
            "0.0002270987\n",
            "0.000223808\n",
            "0.00022053556\n",
            "0.00021728165\n",
            "0.00021404645\n",
            "0.00021083\n",
            "0.00020763265\n",
            "0.0002044546\n",
            "0.00020129613\n",
            "0.00019815729\n",
            "0.00019503836\n",
            "0.00019193963\n",
            "0.00018886113\n",
            "0.00018580313\n",
            "0.00018276596\n",
            "0.00017974958\n",
            "0.0001767543\n",
            "0.00017378032\n",
            "0.00017082795\n",
            "0.00016789726\n",
            "0.00016498835\n",
            "0.0001621015\n",
            "0.0001592369\n",
            "0.0001563948\n",
            "0.00015357525\n",
            "0.0001507786\n",
            "0.00014800484\n",
            "0.00014525428\n",
            "0.0001425271\n",
            "0.00013982339\n",
            "0.00013714333\n",
            "0.00013448727\n",
            "0.00013185515\n",
            "0.00012924724\n",
            "0.00012666378\n",
            "0.00012410482\n",
            "0.00012157057\n",
            "0.00011906118\n",
            "0.00011657695\n",
            "0.00011411788\n",
            "0.0001116841\n",
            "0.00010927586\n",
            "0.000106893305\n",
            "0.00010453665\n",
            "0.0001022059\n",
            "9.990134e-05\n",
            "9.762302e-05\n",
            "9.537114e-05\n",
            "9.314592e-05\n",
            "9.094732e-05\n",
            "8.87756e-05\n",
            "8.663096e-05\n",
            "8.4513405e-05\n",
            "8.2423154e-05\n",
            "8.036035e-05\n",
            "7.832506e-05\n",
            "7.631743e-05\n",
            "7.4337644e-05\n",
            "7.238587e-05\n",
            "7.0462076e-05\n",
            "6.8566515e-05\n",
            "6.669924e-05\n",
            "6.4860535e-05\n",
            "6.305027e-05\n",
            "6.1268685e-05\n",
            "5.9515947e-05\n",
            "5.779207e-05\n",
            "5.6097226e-05\n",
            "5.443153e-05\n",
            "5.2795047e-05\n",
            "5.1187886e-05\n",
            "4.9610175e-05\n",
            "4.8062084e-05\n",
            "4.6543555e-05\n",
            "4.50548e-05\n",
            "4.359599e-05\n",
            "4.2167005e-05\n",
            "4.076809e-05\n",
            "3.9399267e-05\n",
            "3.8060716e-05\n",
            "3.6752404e-05\n",
            "3.5474484e-05\n",
            "3.4227043e-05\n",
            "3.301017e-05\n",
            "3.182389e-05\n",
            "3.066832e-05\n",
            "2.9543531e-05\n",
            "2.844963e-05\n",
            "2.7386619e-05\n",
            "2.6354617e-05\n",
            "2.5353684e-05\n",
            "2.4383819e-05\n",
            "2.3445169e-05\n",
            "2.2537764e-05\n",
            "2.1661694e-05\n",
            "2.0816955e-05\n",
            "2.0003641e-05\n",
            "1.9221776e-05\n",
            "1.847148e-05\n",
            "1.7752727e-05\n",
            "1.7065602e-05\n",
            "1.6410104e-05\n",
            "1.5786383e-05\n",
            "1.51943495e-05\n",
            "1.46340935e-05\n",
            "1.4105701e-05\n",
            "1.3609113e-05\n",
            "1.3144421e-05\n",
            "1.2711652e-05\n",
            "1.2310836e-05\n",
            "1.1942002e-05\n",
            "1.1605122e-05\n",
            "1.1300284e-05\n",
            "1.1027487e-05\n",
            "1.0786732e-05\n",
            "1.0578048e-05\n",
            "1.0401464e-05\n",
            "1.0256953e-05\n",
            "1.0144541e-05\n",
            "1.0064231e-05\n",
            "Adding model weights\n",
            "1.00160505e-05\n",
            "[Epoch 2] Training Loss: 0.109, Training Accuracy: 48042/49920 (96.2%), Test Loss: 0.127, Test Accuracy: 9571/10000 (95.7%) seconds per epoch: 224.0\n",
            "0.001\n",
            "0.000999984\n",
            "0.0009999359\n",
            "0.0009998555\n",
            "0.0009997431\n",
            "0.0009995986\n",
            "0.000999422\n",
            "0.0009992133\n",
            "0.0009989726\n",
            "0.0009986997\n",
            "0.0009983949\n",
            "0.0009980581\n",
            "0.0009976892\n",
            "0.0009972884\n",
            "0.0009968556\n",
            "0.0009963909\n",
            "0.0009958943\n",
            "0.000995366\n",
            "0.0009948057\n",
            "0.0009942136\n",
            "0.00099359\n",
            "0.0009929345\n",
            "0.0009922474\n",
            "0.0009915285\n",
            "0.0009907783\n",
            "0.0009899965\n",
            "0.0009891831\n",
            "0.0009883384\n",
            "0.0009874622\n",
            "0.0009865549\n",
            "0.0009856162\n",
            "0.0009846465\n",
            "0.0009836454\n",
            "0.0009826135\n",
            "0.0009815504\n",
            "0.0009804566\n",
            "0.0009793317\n",
            "0.0009781762\n",
            "0.0009769899\n",
            "0.0009757731\n",
            "0.00097452564\n",
            "0.00097324775\n",
            "0.0009719395\n",
            "0.0009706008\n",
            "0.00096923206\n",
            "0.00096783316\n",
            "0.00096640416\n",
            "0.00096494536\n",
            "0.0009634565\n",
            "0.00096193806\n",
            "0.0009603899\n",
            "0.0009588122\n",
            "0.00095720513\n",
            "0.0009555686\n",
            "0.00095390296\n",
            "0.00095220807\n",
            "0.0009504842\n",
            "0.00094873144\n",
            "0.0009469498\n",
            "0.0009451396\n",
            "0.00094330084\n",
            "0.0009414336\n",
            "0.000939538\n",
            "0.0009376143\n",
            "0.0009356624\n",
            "0.0009336827\n",
            "0.0009316751\n",
            "0.0009296398\n",
            "0.00092757697\n",
            "0.00092548673\n",
            "0.0009233692\n",
            "0.0009212245\n",
            "0.0009190528\n",
            "0.00091685425\n",
            "0.0009146289\n",
            "0.00091237715\n",
            "0.00091009884\n",
            "0.0009077942\n",
            "0.0009054635\n",
            "0.0009031068\n",
            "0.00090072426\n",
            "0.0008983161\n",
            "0.0008958823\n",
            "0.0008934233\n",
            "0.00089093886\n",
            "0.0008884296\n",
            "0.00088589534\n",
            "0.0008833364\n",
            "0.0008807529\n",
            "0.000878145\n",
            "0.00087551295\n",
            "0.0008728568\n",
            "0.0008701768\n",
            "0.0008674731\n",
            "0.0008647459\n",
            "0.0008619953\n",
            "0.00085922156\n",
            "0.00085642486\n",
            "0.0008536054\n",
            "0.00085076323\n",
            "0.0008478987\n",
            "0.0008450119\n",
            "0.0008421029\n",
            "0.0008391722\n",
            "0.0008362197\n",
            "0.0008332458\n",
            "0.0008302505\n",
            "0.00082723424\n",
            "0.000824197\n",
            "0.000821139\n",
            "0.0008180606\n",
            "0.00081496174\n",
            "0.0008118429\n",
            "0.00080870406\n",
            "0.00080554554\n",
            "0.0008023675\n",
            "0.00079917023\n",
            "0.0007959538\n",
            "0.0007927185\n",
            "0.0007894646\n",
            "0.0007861922\n",
            "0.0007829015\n",
            "0.00077959284\n",
            "0.0007762663\n",
            "0.0007729222\n",
            "0.00076956063\n",
            "0.000766182\n",
            "0.0007627865\n",
            "0.00075937406\n",
            "0.0007559452\n",
            "0.0007525001\n",
            "0.0007490389\n",
            "0.0007455619\n",
            "0.0007420693\n",
            "0.0007385613\n",
            "0.0007350381\n",
            "0.0007315\n",
            "0.0007279472\n",
            "0.0007243799\n",
            "0.00072079845\n",
            "0.0007172029\n",
            "0.0007135936\n",
            "0.00070997083\n",
            "0.0007063347\n",
            "0.0007026856\n",
            "0.00069902354\n",
            "0.000695349\n",
            "0.0006916621\n",
            "0.000687963\n",
            "0.00068425207\n",
            "0.0006805295\n",
            "0.0006767956\n",
            "0.00067305047\n",
            "0.00066929444\n",
            "0.0006655279\n",
            "0.00066175073\n",
            "0.0006579635\n",
            "0.0006541664\n",
            "0.0006503595\n",
            "0.0006465432\n",
            "0.00064271776\n",
            "0.0006388833\n",
            "0.0006350403\n",
            "0.0006311887\n",
            "0.000627329\n",
            "0.0006234613\n",
            "0.00061958603\n",
            "0.0006157033\n",
            "0.0006118133\n",
            "0.0006079164\n",
            "0.0006040128\n",
            "0.0006001028\n",
            "0.0005961867\n",
            "0.0005922646\n",
            "0.00058833684\n",
            "0.0005844037\n",
            "0.0005804654\n",
            "0.00057652214\n",
            "0.0005725744\n",
            "0.00056862214\n",
            "0.00056466577\n",
            "0.00056070555\n",
            "0.0005567417\n",
            "0.0005527744\n",
            "0.0005488042\n",
            "0.000544831\n",
            "0.00054085534\n",
            "0.0005368773\n",
            "0.00053289713\n",
            "0.00052891526\n",
            "0.0005249317\n",
            "0.000520947\n",
            "0.0005169611\n",
            "0.0005129746\n",
            "0.0005089875\n",
            "0.0005050001\n",
            "0.0005010128\n",
            "0.0004970256\n",
            "0.000493039\n",
            "0.00048905326\n",
            "0.00048506845\n",
            "0.00048108495\n",
            "0.00047710302\n",
            "0.00047312287\n",
            "0.0004691448\n",
            "0.00046516905\n",
            "0.000461196\n",
            "0.00045722572\n",
            "0.0004532585\n",
            "0.00044929463\n",
            "0.00044533436\n",
            "0.00044137804\n",
            "0.00043742577\n",
            "0.000433478\n",
            "0.00042953482\n",
            "0.0004255965\n",
            "0.00042166337\n",
            "0.00041773557\n",
            "0.0004138135\n",
            "0.00040989733\n",
            "0.00040598738\n",
            "0.00040208377\n",
            "0.0003981869\n",
            "0.0003942969\n",
            "0.00039041415\n",
            "0.00038653877\n",
            "0.0003826712\n",
            "0.0003788115\n",
            "0.0003749599\n",
            "0.00037111682\n",
            "0.0003672824\n",
            "0.00036345696\n",
            "0.00035964063\n",
            "0.00035583385\n",
            "0.0003520367\n",
            "0.0003482494\n",
            "0.00034447236\n",
            "0.00034070565\n",
            "0.00033694966\n",
            "0.00033320457\n",
            "0.00032947067\n",
            "0.00032574806\n",
            "0.0003220372\n",
            "0.00031833808\n",
            "0.00031465117\n",
            "0.00031097655\n",
            "0.0003073146\n",
            "0.00030366544\n",
            "0.00030002932\n",
            "0.0002964065\n",
            "0.00029279722\n",
            "0.00028920168\n",
            "0.00028562028\n",
            "0.00028205308\n",
            "0.00027850023\n",
            "0.00027496208\n",
            "0.0002714389\n",
            "0.00026793094\n",
            "0.00026443828\n",
            "0.0002609612\n",
            "0.00025750007\n",
            "0.00025405493\n",
            "0.00025062604\n",
            "0.0002472138\n",
            "0.00024381814\n",
            "0.00024043943\n",
            "0.00023707791\n",
            "0.00023373387\n",
            "0.0002304074\n",
            "0.0002270987\n",
            "0.000223808\n",
            "0.00022053556\n",
            "0.00021728165\n",
            "0.00021404645\n",
            "0.00021083\n",
            "0.00020763265\n",
            "0.0002044546\n",
            "0.00020129613\n",
            "0.00019815729\n",
            "0.00019503836\n",
            "0.00019193963\n",
            "0.00018886113\n",
            "0.00018580313\n",
            "0.00018276596\n",
            "0.00017974958\n",
            "0.0001767543\n",
            "0.00017378032\n",
            "0.00017082795\n",
            "0.00016789726\n",
            "0.00016498835\n",
            "0.0001621015\n",
            "0.0001592369\n",
            "0.0001563948\n",
            "0.00015357525\n",
            "0.0001507786\n",
            "0.00014800484\n",
            "0.00014525428\n",
            "0.0001425271\n",
            "0.00013982339\n",
            "0.00013714333\n",
            "0.00013448727\n",
            "0.00013185515\n",
            "0.00012924724\n",
            "0.00012666378\n",
            "0.00012410482\n",
            "0.00012157057\n",
            "0.00011906118\n",
            "0.00011657695\n",
            "0.00011411788\n",
            "0.0001116841\n",
            "0.00010927586\n",
            "0.000106893305\n",
            "0.00010453665\n",
            "0.0001022059\n",
            "9.990134e-05\n",
            "9.762302e-05\n",
            "9.537114e-05\n",
            "9.314592e-05\n",
            "9.094732e-05\n",
            "8.87756e-05\n",
            "8.663096e-05\n",
            "8.4513405e-05\n",
            "8.2423154e-05\n",
            "8.036035e-05\n",
            "7.832506e-05\n",
            "7.631743e-05\n",
            "7.4337644e-05\n",
            "7.238587e-05\n",
            "7.0462076e-05\n",
            "6.8566515e-05\n",
            "6.669924e-05\n",
            "6.4860535e-05\n",
            "6.305027e-05\n",
            "6.1268685e-05\n",
            "5.9515947e-05\n",
            "5.779207e-05\n",
            "5.6097226e-05\n",
            "5.443153e-05\n",
            "5.2795047e-05\n",
            "5.1187886e-05\n",
            "4.9610175e-05\n",
            "4.8062084e-05\n",
            "4.6543555e-05\n",
            "4.50548e-05\n",
            "4.359599e-05\n",
            "4.2167005e-05\n",
            "4.076809e-05\n",
            "3.9399267e-05\n",
            "3.8060716e-05\n",
            "3.6752404e-05\n",
            "3.5474484e-05\n",
            "3.4227043e-05\n",
            "3.301017e-05\n",
            "3.182389e-05\n",
            "3.066832e-05\n",
            "2.9543531e-05\n",
            "2.844963e-05\n",
            "2.7386619e-05\n",
            "2.6354617e-05\n",
            "2.5353684e-05\n",
            "2.4383819e-05\n",
            "2.3445169e-05\n",
            "2.2537764e-05\n",
            "2.1661694e-05\n",
            "2.0816955e-05\n",
            "2.0003641e-05\n",
            "1.9221776e-05\n",
            "1.847148e-05\n",
            "1.7752727e-05\n",
            "1.7065602e-05\n",
            "1.6410104e-05\n",
            "1.5786383e-05\n",
            "1.51943495e-05\n",
            "1.46340935e-05\n",
            "1.4105701e-05\n",
            "1.3609113e-05\n",
            "1.3144421e-05\n",
            "1.2711652e-05\n",
            "1.2310836e-05\n",
            "1.1942002e-05\n",
            "1.1605122e-05\n",
            "1.1300284e-05\n",
            "1.1027487e-05\n",
            "1.0786732e-05\n",
            "1.0578048e-05\n",
            "1.0401464e-05\n",
            "1.0256953e-05\n",
            "1.0144541e-05\n",
            "1.0064231e-05\n",
            "Adding model weights\n",
            "1.00160505e-05\n",
            "[Epoch 3] Training Loss: 0.093, Training Accuracy: 48291/49920 (96.7%), Test Loss: 0.124, Test Accuracy: 9582/10000 (95.8%) seconds per epoch: 223.5\n",
            "0.001\n",
            "0.000999984\n",
            "0.0009999359\n",
            "0.0009998555\n",
            "0.0009997431\n",
            "0.0009995986\n",
            "0.000999422\n",
            "0.0009992133\n",
            "0.0009989726\n",
            "0.0009986997\n",
            "0.0009983949\n",
            "0.0009980581\n",
            "0.0009976892\n",
            "0.0009972884\n",
            "0.0009968556\n",
            "0.0009963909\n",
            "0.0009958943\n",
            "0.000995366\n",
            "0.0009948057\n",
            "0.0009942136\n",
            "0.00099359\n",
            "0.0009929345\n",
            "0.0009922474\n",
            "0.0009915285\n",
            "0.0009907783\n",
            "0.0009899965\n",
            "0.0009891831\n",
            "0.0009883384\n",
            "0.0009874622\n",
            "0.0009865549\n",
            "0.0009856162\n",
            "0.0009846465\n",
            "0.0009836454\n",
            "0.0009826135\n",
            "0.0009815504\n",
            "0.0009804566\n",
            "0.0009793317\n",
            "0.0009781762\n",
            "0.0009769899\n",
            "0.0009757731\n",
            "0.00097452564\n",
            "0.00097324775\n",
            "0.0009719395\n",
            "0.0009706008\n",
            "0.00096923206\n",
            "0.00096783316\n",
            "0.00096640416\n",
            "0.00096494536\n",
            "0.0009634565\n",
            "0.00096193806\n",
            "0.0009603899\n",
            "0.0009588122\n",
            "0.00095720513\n",
            "0.0009555686\n",
            "0.00095390296\n",
            "0.00095220807\n",
            "0.0009504842\n",
            "0.00094873144\n",
            "0.0009469498\n",
            "0.0009451396\n",
            "0.00094330084\n",
            "0.0009414336\n",
            "0.000939538\n",
            "0.0009376143\n",
            "0.0009356624\n",
            "0.0009336827\n",
            "0.0009316751\n",
            "0.0009296398\n",
            "0.00092757697\n",
            "0.00092548673\n",
            "0.0009233692\n",
            "0.0009212245\n",
            "0.0009190528\n",
            "0.00091685425\n",
            "0.0009146289\n",
            "0.00091237715\n",
            "0.00091009884\n",
            "0.0009077942\n",
            "0.0009054635\n",
            "0.0009031068\n",
            "0.00090072426\n",
            "0.0008983161\n",
            "0.0008958823\n",
            "0.0008934233\n",
            "0.00089093886\n",
            "0.0008884296\n",
            "0.00088589534\n",
            "0.0008833364\n",
            "0.0008807529\n",
            "0.000878145\n",
            "0.00087551295\n",
            "0.0008728568\n",
            "0.0008701768\n",
            "0.0008674731\n",
            "0.0008647459\n",
            "0.0008619953\n",
            "0.00085922156\n",
            "0.00085642486\n",
            "0.0008536054\n",
            "0.00085076323\n",
            "0.0008478987\n",
            "0.0008450119\n",
            "0.0008421029\n",
            "0.0008391722\n",
            "0.0008362197\n",
            "0.0008332458\n",
            "0.0008302505\n",
            "0.00082723424\n",
            "0.000824197\n",
            "0.000821139\n",
            "0.0008180606\n",
            "0.00081496174\n",
            "0.0008118429\n",
            "0.00080870406\n",
            "0.00080554554\n",
            "0.0008023675\n",
            "0.00079917023\n",
            "0.0007959538\n",
            "0.0007927185\n",
            "0.0007894646\n",
            "0.0007861922\n",
            "0.0007829015\n",
            "0.00077959284\n",
            "0.0007762663\n",
            "0.0007729222\n",
            "0.00076956063\n",
            "0.000766182\n",
            "0.0007627865\n",
            "0.00075937406\n",
            "0.0007559452\n",
            "0.0007525001\n",
            "0.0007490389\n",
            "0.0007455619\n",
            "0.0007420693\n",
            "0.0007385613\n",
            "0.0007350381\n",
            "0.0007315\n",
            "0.0007279472\n",
            "0.0007243799\n",
            "0.00072079845\n",
            "0.0007172029\n",
            "0.0007135936\n",
            "0.00070997083\n",
            "0.0007063347\n",
            "0.0007026856\n",
            "0.00069902354\n",
            "0.000695349\n",
            "0.0006916621\n",
            "0.000687963\n",
            "0.00068425207\n",
            "0.0006805295\n",
            "0.0006767956\n",
            "0.00067305047\n",
            "0.00066929444\n",
            "0.0006655279\n",
            "0.00066175073\n",
            "0.0006579635\n",
            "0.0006541664\n",
            "0.0006503595\n",
            "0.0006465432\n",
            "0.00064271776\n",
            "0.0006388833\n",
            "0.0006350403\n",
            "0.0006311887\n",
            "0.000627329\n",
            "0.0006234613\n",
            "0.00061958603\n",
            "0.0006157033\n",
            "0.0006118133\n",
            "0.0006079164\n",
            "0.0006040128\n",
            "0.0006001028\n",
            "0.0005961867\n",
            "0.0005922646\n",
            "0.00058833684\n",
            "0.0005844037\n",
            "0.0005804654\n",
            "0.00057652214\n",
            "0.0005725744\n",
            "0.00056862214\n",
            "0.00056466577\n",
            "0.00056070555\n",
            "0.0005567417\n",
            "0.0005527744\n",
            "0.0005488042\n",
            "0.000544831\n",
            "0.00054085534\n",
            "0.0005368773\n",
            "0.00053289713\n",
            "0.00052891526\n",
            "0.0005249317\n",
            "0.000520947\n",
            "0.0005169611\n",
            "0.0005129746\n",
            "0.0005089875\n",
            "0.0005050001\n",
            "0.0005010128\n",
            "0.0004970256\n",
            "0.000493039\n",
            "0.00048905326\n",
            "0.00048506845\n",
            "0.00048108495\n",
            "0.00047710302\n",
            "0.00047312287\n",
            "0.0004691448\n",
            "0.00046516905\n",
            "0.000461196\n",
            "0.00045722572\n",
            "0.0004532585\n",
            "0.00044929463\n",
            "0.00044533436\n",
            "0.00044137804\n",
            "0.00043742577\n",
            "0.000433478\n",
            "0.00042953482\n",
            "0.0004255965\n",
            "0.00042166337\n",
            "0.00041773557\n",
            "0.0004138135\n",
            "0.00040989733\n",
            "0.00040598738\n",
            "0.00040208377\n",
            "0.0003981869\n",
            "0.0003942969\n",
            "0.00039041415\n",
            "0.00038653877\n",
            "0.0003826712\n",
            "0.0003788115\n",
            "0.0003749599\n",
            "0.00037111682\n",
            "0.0003672824\n",
            "0.00036345696\n",
            "0.00035964063\n",
            "0.00035583385\n",
            "0.0003520367\n",
            "0.0003482494\n",
            "0.00034447236\n",
            "0.00034070565\n",
            "0.00033694966\n",
            "0.00033320457\n",
            "0.00032947067\n",
            "0.00032574806\n",
            "0.0003220372\n",
            "0.00031833808\n",
            "0.00031465117\n",
            "0.00031097655\n",
            "0.0003073146\n",
            "0.00030366544\n",
            "0.00030002932\n",
            "0.0002964065\n",
            "0.00029279722\n",
            "0.00028920168\n",
            "0.00028562028\n",
            "0.00028205308\n",
            "0.00027850023\n",
            "0.00027496208\n",
            "0.0002714389\n",
            "0.00026793094\n",
            "0.00026443828\n",
            "0.0002609612\n",
            "0.00025750007\n",
            "0.00025405493\n",
            "0.00025062604\n",
            "0.0002472138\n",
            "0.00024381814\n",
            "0.00024043943\n",
            "0.00023707791\n",
            "0.00023373387\n",
            "0.0002304074\n",
            "0.0002270987\n",
            "0.000223808\n",
            "0.00022053556\n",
            "0.00021728165\n",
            "0.00021404645\n",
            "0.00021083\n",
            "0.00020763265\n",
            "0.0002044546\n",
            "0.00020129613\n",
            "0.00019815729\n",
            "0.00019503836\n",
            "0.00019193963\n",
            "0.00018886113\n",
            "0.00018580313\n",
            "0.00018276596\n",
            "0.00017974958\n",
            "0.0001767543\n",
            "0.00017378032\n",
            "0.00017082795\n",
            "0.00016789726\n",
            "0.00016498835\n",
            "0.0001621015\n",
            "0.0001592369\n",
            "0.0001563948\n",
            "0.00015357525\n",
            "0.0001507786\n",
            "0.00014800484\n",
            "0.00014525428\n",
            "0.0001425271\n",
            "0.00013982339\n",
            "0.00013714333\n",
            "0.00013448727\n",
            "0.00013185515\n",
            "0.00012924724\n",
            "0.00012666378\n",
            "0.00012410482\n",
            "0.00012157057\n",
            "0.00011906118\n",
            "0.00011657695\n",
            "0.00011411788\n",
            "0.0001116841\n",
            "0.00010927586\n",
            "0.000106893305\n",
            "0.00010453665\n",
            "0.0001022059\n",
            "9.990134e-05\n",
            "9.762302e-05\n",
            "9.537114e-05\n",
            "9.314592e-05\n",
            "9.094732e-05\n",
            "8.87756e-05\n",
            "8.663096e-05\n",
            "8.4513405e-05\n",
            "8.2423154e-05\n",
            "8.036035e-05\n",
            "7.832506e-05\n",
            "7.631743e-05\n",
            "7.4337644e-05\n",
            "7.238587e-05\n",
            "7.0462076e-05\n",
            "6.8566515e-05\n",
            "6.669924e-05\n",
            "6.4860535e-05\n",
            "6.305027e-05\n",
            "6.1268685e-05\n",
            "5.9515947e-05\n",
            "5.779207e-05\n",
            "5.6097226e-05\n",
            "5.443153e-05\n",
            "5.2795047e-05\n",
            "5.1187886e-05\n",
            "4.9610175e-05\n",
            "4.8062084e-05\n",
            "4.6543555e-05\n",
            "4.50548e-05\n",
            "4.359599e-05\n",
            "4.2167005e-05\n",
            "4.076809e-05\n",
            "3.9399267e-05\n",
            "3.8060716e-05\n",
            "3.6752404e-05\n",
            "3.5474484e-05\n",
            "3.4227043e-05\n",
            "3.301017e-05\n",
            "3.182389e-05\n",
            "3.066832e-05\n",
            "2.9543531e-05\n",
            "2.844963e-05\n",
            "2.7386619e-05\n",
            "2.6354617e-05\n",
            "2.5353684e-05\n",
            "2.4383819e-05\n",
            "2.3445169e-05\n",
            "2.2537764e-05\n",
            "2.1661694e-05\n",
            "2.0816955e-05\n",
            "2.0003641e-05\n",
            "1.9221776e-05\n",
            "1.847148e-05\n",
            "1.7752727e-05\n",
            "1.7065602e-05\n",
            "1.6410104e-05\n",
            "1.5786383e-05\n",
            "1.51943495e-05\n",
            "1.46340935e-05\n",
            "1.4105701e-05\n",
            "1.3609113e-05\n",
            "1.3144421e-05\n",
            "1.2711652e-05\n",
            "1.2310836e-05\n",
            "1.1942002e-05\n",
            "1.1605122e-05\n",
            "1.1300284e-05\n",
            "1.1027487e-05\n",
            "1.0786732e-05\n",
            "1.0578048e-05\n",
            "1.0401464e-05\n",
            "1.0256953e-05\n",
            "1.0144541e-05\n",
            "1.0064231e-05\n",
            "Adding model weights\n",
            "1.00160505e-05\n",
            "[Epoch 4] Training Loss: 0.079, Training Accuracy: 48538/49920 (97.2%), Test Loss: 0.117, Test Accuracy: 9609/10000 (96.1%) seconds per epoch: 222.7\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lfN3AsN7deyb",
        "outputId": "f5d294ac-79aa-4221-f879-35b53ff7d712",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "var testStats = Statistics(on: device)\n",
        "Context.local.learningPhase = .inference\n",
        "for batch in dataset.validation {\n",
        "    var (eagerImages, eagerLabels) = (batch.data, batch.label)\n",
        "    var npImages = eagerImages.makeNumpyArray()\n",
        "    var resized = tf.image.resize(npImages, [resize_size.0, resize_size.0])\n",
        "    eagerImages = Tensor<Float>(numpy: resized.numpy())!\n",
        "    var newLabels = Tensor<Float>(Tensor<Int32>(oneHotAtIndices: eagerLabels, depth: 10))\n",
        "    let images = Tensor(copying: eagerImages, to: device)\n",
        "    let labels = Tensor(copying: newLabels, to: device)\n",
        "    let ŷ = resnetv2(images)\n",
        "    let loss = softmaxCrossEntropy(logits: ŷ, probabilities: labels)\n",
        "    LazyTensorBarrier()\n",
        "    testStats.update(logits: ŷ, labels: labels, loss: loss)\n",
        "}\n",
        "print(\n",
        "  \"\"\"\n",
        "  Test Accuracy: \\(testStats.correctGuessCount)/\\(testStats.totalGuessCount)\n",
        "  \"\"\")"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test Accuracy: 9609/10000\r\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KPiwQA-fd471",
        "outputId": "145802ad-b9f1-4268-d779-3231f8417a66",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "var asdf = BigTransfer.TangentVector.zero\n",
        "asdf.inputStem.conv.filter = swaModelWeights.inputStem.conv.filter/Float(numCycles)\n",
        "asdf.groupNorm.offset = swaModelWeights.groupNorm.offset/Float(numCycles)\n",
        "asdf.groupNorm.scale = swaModelWeights.groupNorm.scale/Float(numCycles)\n",
        "asdf.classifier.weight = swaModelWeights.classifier.weight/Float(numCycles)\n",
        "asdf.classifier.bias = swaModelWeights.classifier.bias/Float(numCycles)\n",
        "\n",
        "// for (index, weight)  in swaModelWeights.residualBlocks.enumerated() {\n",
        "//   asdf.residualBlocks[index].shortcut.projection.conv.filter = weight.shortcut.projection.conv.filter\n",
        "//   asdf.residualBlocks[index].shortcut.norm.offset = weight.shortcut.norm.offset\n",
        "//   asdf.residualBlocks[index].shortcut.norm.scale = weight.shortcut.norm.scale\n",
        "\n",
        "// }\n",
        "print(BigTransfer.TangentVector.zero.residualBlocks[0])\n",
        "print(swaModelWeights.residualBlocks[0].shortcut.norm.scale)\n",
        "//print(swaModelWeights.inputStem.conv.filter + swaModelWeights.inputStem.conv.filter)\n",
        "//print(swaModelWeights.residualBlocks.count)"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TangentVector(shortcut: __lldb_expr_77.ShortcutBiT.TangentVector(projection: __lldb_expr_77.StandardizedConv2D.TangentVector(conv: TensorFlow.Conv2D<Swift.Float>.TangentVector(filter: 0.0, bias: 0.0)), norm: TensorFlow.GroupNorm<Swift.Float>.TangentVector(offset: 0.0, scale: 0.0)), convs: [])\r\n",
            "[  -0.010515584,   0.0060409624,    -0.00770534,   0.0012274466,     -0.0177833,\r\n",
            "    0.006847801,   -0.010782016,   0.0116677275,    0.028987981,   -0.019971855,\r\n",
            "   0.0030247504,    0.002705772,   -0.006524962,   -0.001354458,   -0.009118077,\r\n",
            "  0.00014815025, -0.00032911674,   0.0054871496,   0.0039285836,   0.0116301365,\r\n",
            "    0.005478776,    0.003805984,  -0.0011149442,  -4.791573e-05,   0.0051373257,\r\n",
            "  -0.0054048714,     0.00106544,   0.0012090919,  -0.0032560853,    0.003604373,\r\n",
            "   0.0037225801,  -0.0015190264,  -0.0025417386,  -0.0062690736,            0.0,\r\n",
            "   0.0055292347,  0.00016697636,   0.0027372146,  -0.0023406518,    0.008996561,\r\n",
            "  -0.0153351985,    0.007796812,   -0.005184309,       0.002982,   -0.004954813,\r\n",
            "   0.0022939392,  0.00066940393,    0.001215063, -0.00010113767,  -0.0040062293,\r\n",
            "  -0.0076326104,  -0.0046433564,  -0.0018205608,    0.010031092,  0.00011184928,\r\n",
            "   -0.004220788,   0.0045620427,   0.0038310974, -0.00088164327,  -0.0035337375,\r\n",
            " -0.00011212262,   0.0022281948,    -0.04122532,     0.01919267]\r\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iHY3FEzx5v0k"
      },
      "source": [
        "print(BigTransfer.TangentVector.zero)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0W2hwKo76BbN"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}