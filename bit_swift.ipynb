{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "bit_swift.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "swift",
      "display_name": "Swift"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/acoadmarmon/big-transfer-swift/blob/main/bit_swift.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kZRlD4utdPuX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "926d9595-9b51-428b-dacd-13b4c547277e"
      },
      "source": [
        "%install '.package(url: \"https://github.com/tensorflow/swift-models\", .branch(\"tensorflow-0.11\"))' Datasets ImageClassificationModels\n",
        "print(\"\\u{001B}[2J\")"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\r\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wTl6d5efpeb_"
      },
      "source": [
        "import Datasets\n",
        "import ImageClassificationModels\n",
        "import TensorFlow\n",
        "import Python\n",
        "import Foundation\n",
        "import Differentiable\n"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JDjdyWTlaDBc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "484bec3d-1bbe-4014-ae85-c9aeca5ec55b"
      },
      "source": [
        "%include \"EnableIPythonDisplay.swift\"\n",
        "IPythonDisplay.shell.enable_matplotlib(\"inline\")\n",
        "\n",
        "let plt = Python.import(\"matplotlib.pyplot\")\n",
        "let np  = Python.import(\"numpy\")\n",
        "let subprocess = Python.import(\"subprocess\")\n",
        "let glob = Python.import(\"glob\")\n",
        "let pil = Python.import(\"PIL\")\n",
        "let tf = Python.import(\"tensorflow\")\n",
        "//let tfp = Python.import(\"tensorflow_probability\")\n",
        "let h5py = Python.import(\"h5py\")\n",
        "let path = Python.import(\"os.path\")"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2020-11-18 03:05:34.079802: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\r\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h3ilNx78jjEA"
      },
      "source": [
        "//subprocess.call(\"curl https://sdk.cloud.google.com | bash; exec -l $SHELL; gsutil ls gs://uspto-pair/applications/0800401*\", shell: true)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E2SFbiPWpinG"
      },
      "source": [
        "let epochCount = 12\n",
        "let batchSize = 32"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jAEB0ZyHrvGS"
      },
      "source": [
        "// Set hyperrule parameters from bit_hyperrule.py\n",
        "enum ValueError: Error {\n",
        "    case invalidInput(String)\n",
        "}\n",
        "\n",
        "func get_resolution(original_resolution: (Int, Int)) -> (Int, Int) {\n",
        "  let area = original_resolution.0 * original_resolution.1\n",
        "  return area < 96*96 ? (160, 128) : (512, 480)\n",
        "}\n",
        "\n",
        "\n",
        "let known_dataset_sizes:[String: (Int, Int)] = [\n",
        "  \"cifar10\": (32, 32),\n",
        "  \"cifar100\": (32, 32),\n",
        "  \"oxford_iiit_pet\": (224, 224),\n",
        "  \"oxford_flowers102\": (224, 224),\n",
        "  \"imagenet2012\": (224, 224),\n",
        "]\n",
        "\n",
        "func get_resolution_from_dataset(dataset: String) throws -> (Int, Int) {\n",
        "  if let resolution = known_dataset_sizes[dataset] {\n",
        "    return get_resolution(original_resolution: resolution)\n",
        "  }\n",
        "  print(\"Unsupported dataset \" + dataset + \". Add your own here :)\")\n",
        "  throw ValueError.invalidInput(dataset)\n",
        "\n",
        "}\n",
        "\n",
        "func get_mixup(dataset_size: Int) -> Double {\n",
        "  return dataset_size < 20_000 ? 0.0 : 0.1\n",
        "}\n",
        "\n",
        "\n",
        "func get_schedule(dataset_size: Int) -> Array<Int> {\n",
        "  if dataset_size < 20_000{\n",
        "    return [100, 200, 300, 400, 500]\n",
        "  }\n",
        "  else if dataset_size < 500_000 {\n",
        "    return [500, 3000, 6000, 9000, 10_000]\n",
        "  }\n",
        "  else {\n",
        "    return [500, 6000, 12_000, 18_000, 20_000]\n",
        "  }\n",
        "}\n",
        "\n",
        "\n",
        "func get_lr(step: Int, dataset_size: Int, base_lr: Float = 0.003) -> Float? {\n",
        "  /* Returns learning-rate for `step` or nil at the end. */\n",
        "  let supports = get_schedule(dataset_size: dataset_size)\n",
        "  // Linear warmup\n",
        "  if step < supports[0] {\n",
        "    return base_lr * Float(step) / Float(supports[0])\n",
        "  }\n",
        "  // End of training\n",
        "  else if step >= supports.last! {\n",
        "    return nil\n",
        "  }\n",
        "  // Staircase decays by factor of 10\n",
        "  else {\n",
        "    var base_lr = base_lr\n",
        "    for s in supports[1...] {\n",
        "      if s < step {\n",
        "        base_lr = base_lr / 10.0\n",
        "      }\n",
        "    }\n",
        "    return base_lr\n",
        "  }\n",
        "}"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Er7kXeINnwuW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "39163e6b-8ca6-4269-ffba-199a16e8993a"
      },
      "source": [
        "get_lr(step: 7000, dataset_size: 50000, base_lr: 0.01)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "‚ñø Optional<Float>\n",
              "  - some : 9.999999e-05\n"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RFmZWY2q-xzd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "35d2685b-d4e8-477e-f34f-b4767a087a40"
      },
      "source": [
        "// Build model with weights\n",
        "import FileManager\n",
        "import PythonKit\n",
        "let tf = Python.import(\"tensorflow\")\n",
        "_ = Python.builtins\n",
        "\n",
        "var known_models = [String: String]()\n",
        "// var resnetv2 = ResNetV2(classCount: 1000, depth: .resNet50)\n",
        "\n",
        "let model_name = \"BiT-M-R50x1\"\n",
        "\n",
        "struct Weights {\n",
        "    let name: String\n",
        "    let layer: Tensor<Float>\n",
        "}\n",
        "\n",
        "func get_pretrained_weights_dict(model_name: String) -> Array<Weights> {\n",
        "  let valid_types = [\"BiT-S\", \"BiT-M\"]\n",
        "  let valid_sizes = [(50, 1), (50, 3), (101, 1), (101, 3), (152, 4)]\n",
        "  let bit_url = \"gs://bit_models/\"\n",
        "\n",
        "  for types in valid_types {\n",
        "    for sizes in valid_sizes {\n",
        "      let model_string = types + \"-R\" + String(sizes.0) + \"x\" + String(sizes.1)\n",
        "      known_models[model_string] = bit_url + model_string + \".npz\"\n",
        "    }\n",
        "  }\n",
        "\n",
        "  var f = Python.None\n",
        "  \n",
        "  if let model_path = known_models[model_name] {\n",
        "    subprocess.call(\"gsutil cp \" + model_path + \" .\", shell: true)\n",
        "  }\n",
        "\n",
        "  let weights = np.load(\"./\" + model_name + \".npz\")\n",
        "\n",
        "  var weights_array = Array<Weights>()\n",
        "  for param in weights {\n",
        "      weights_array.append(Weights(name: String(param)!, layer: Tensor<Float>(numpy: weights[param])!))\n",
        "  }\n",
        "  return weights_array\n",
        "}\n",
        "var weights_array = get_pretrained_weights_dict(model_name: model_name)\n",
        "// Get convolution weights"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Copying gs://bit_models/BiT-M-R50x1.npz...\n",
            "/ [1 files][260.4 MiB/260.4 MiB]                                                \n",
            "Operation completed over 1 objects/260.4 MiB.                                    \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hwWf0PEYMA_N"
      },
      "source": [
        "import TensorFlow\n",
        "func getPaddingDimensionsFromKernelSize(kernelSize: Int) -> (Int, Int) {\n",
        "  let padTotal = kernelSize - 1\n",
        "  let padBeginning = Int(padTotal / 2)\n",
        "  let padEnd = padTotal - padBeginning\n",
        "  return (kernelSize + padBeginning, kernelSize + padEnd)\n",
        "}\n",
        "\n",
        "func paddingFromKernelSize(kernelSize: Int) -> [(before: Int, after: Int)] {\n",
        "  let padTotal = kernelSize - 1\n",
        "  let padBeginning = Int(padTotal / 2)\n",
        "  let padEnd = padTotal - padBeginning\n",
        "  let padding = [\n",
        "        (before: 0, after: 0),\n",
        "        (before: padBeginning, after: padEnd),\n",
        "        (before: padBeginning, after: padEnd),\n",
        "        (before: 0, after: 0)]\n",
        "  return padding\n",
        "}\n",
        "\n",
        "public struct StandardizedConv2D: Layer {\n",
        "  public var conv: Conv2D<Float>\n",
        "\n",
        "  public init(\n",
        "    filterShape: (Int, Int, Int, Int),\n",
        "    strides: (Int, Int) = (1, 1),\n",
        "    padding: Padding = .valid,\n",
        "    useBias: Bool = true\n",
        "  )\n",
        "  {\n",
        "  self.conv = Conv2D(\n",
        "      filterShape: filterShape, \n",
        "      strides: strides, \n",
        "      padding: padding,\n",
        "      useBias: useBias)\n",
        "  }\n",
        "\n",
        "  @differentiable\n",
        "  public func callAsFunction(_ input: Tensor<Float>) -> Tensor<Float> {\n",
        "      let axes: Array<Int> = [0, 1, 2]\n",
        "      var standardizedConv = conv\n",
        "      standardizedConv.filter = (standardizedConv.filter - standardizedConv.filter.mean(squeezingAxes: axes)) / sqrt((standardizedConv.filter.variance(squeezingAxes: axes) + 1e-16))\n",
        "      return standardizedConv(input)\n",
        "  }\n",
        "\n",
        "}\n",
        "\n",
        "\n",
        "public struct ConvGNV2: Layer {\n",
        "    public var conv: StandardizedConv2D\n",
        "    public var norm: GroupNorm<Float>\n",
        "    @noDerivative public var isSecond: Bool\n",
        "\n",
        "    public init(\n",
        "        inFilters: Int,\n",
        "        outFilters: Int,\n",
        "        kernelSize: Int = 1,\n",
        "        stride: Int = 1,\n",
        "        padding: Padding = .valid,\n",
        "        isSecond: Bool = false\n",
        "    ) {\n",
        "        self.conv = StandardizedConv2D(\n",
        "            filterShape: (kernelSize, kernelSize, inFilters, outFilters), \n",
        "            strides: (stride, stride), \n",
        "            padding: padding,\n",
        "            useBias: false)\n",
        "        self.norm = GroupNorm<Float>(\n",
        "              offset: Tensor(zeros: [inFilters]),\n",
        "              scale: Tensor(zeros: [inFilters]),\n",
        "              groupCount: 2,\n",
        "              axis: -1,\n",
        "              epsilon: 0.001)\n",
        "        self.isSecond = isSecond\n",
        "    }\n",
        "\n",
        "    @differentiable\n",
        "    public func callAsFunction(_ input: Tensor<Float>) -> Tensor<Float> {\n",
        "        var normResult = norm(input)\n",
        "        if self.isSecond {\n",
        "            normResult = normResult.padded(forSizes: paddingFromKernelSize(kernelSize: 3))\n",
        "        }\n",
        "        let reluResult = relu(normResult)\n",
        "        let convResult = conv(reluResult)\n",
        "        return convResult\n",
        "    }\n",
        "}\n",
        "public struct ShortcutBiT: Layer {\n",
        "    public var projection: StandardizedConv2D\n",
        "    public var norm: GroupNorm<Float>\n",
        "    @noDerivative public let needsProjection: Bool\n",
        "    \n",
        "    public init(inFilters: Int, outFilters: Int, stride: Int) {\n",
        "      needsProjection = (stride > 1 || inFilters != outFilters)\n",
        "      norm = GroupNorm<Float>(\n",
        "          offset: Tensor(zeros: [needsProjection ? inFilters  : 1]),\n",
        "          scale: Tensor(zeros: [needsProjection ? inFilters  : 1]),\n",
        "          groupCount: needsProjection ? 2  : 1,\n",
        "          axis: -1,\n",
        "          epsilon: 0.001)\n",
        "        \n",
        "        projection =  StandardizedConv2D(\n",
        "            filterShape: (1, 1, needsProjection ? inFilters  : 1, needsProjection ? outFilters : 1), \n",
        "            strides: (stride, stride), \n",
        "            padding: .valid,\n",
        "            useBias: false)\n",
        "    }\n",
        "    \n",
        "    @differentiable\n",
        "    public func callAsFunction(_ input: Tensor<Float>) -> Tensor<Float> {\n",
        "        var res = input\n",
        "        if needsProjection { \n",
        "          res = norm(res)\n",
        "          res = relu(res)\n",
        "          res = projection(res)\n",
        "        }\n",
        "        return res\n",
        "    }\n",
        "}\n",
        "\n",
        "\n",
        "\n",
        "public struct ResidualBlockBiT: Layer {\n",
        "    public var shortcut: ShortcutBiT\n",
        "    public var convs: [ConvGNV2]\n",
        "\n",
        "    public init(inFilters: Int, outFilters: Int, stride: Int, expansion: Int){\n",
        "        if expansion == 1 {\n",
        "            convs = [\n",
        "                ConvGNV2(inFilters: inFilters,  outFilters: outFilters, kernelSize: 3, stride: stride),\n",
        "                ConvGNV2(inFilters: outFilters, outFilters: outFilters, kernelSize: 3, isSecond: true)\n",
        "            ]\n",
        "        } else {\n",
        "            convs = [\n",
        "                ConvGNV2(inFilters: inFilters,    outFilters: outFilters/4),\n",
        "                ConvGNV2(inFilters: outFilters/4, outFilters: outFilters/4, kernelSize: 3, stride: stride, isSecond: true),\n",
        "                ConvGNV2(inFilters: outFilters/4, outFilters: outFilters)\n",
        "            ]\n",
        "        }\n",
        "        shortcut = ShortcutBiT(inFilters: inFilters, outFilters: outFilters, stride: stride)\n",
        "    }\n",
        "\n",
        "    @differentiable\n",
        "    public func callAsFunction(_ input: Tensor<Float>) -> Tensor<Float> {\n",
        "        let convResult = convs.differentiableReduce(input) { $1($0) }\n",
        "        return convResult + shortcut(input)\n",
        "    }\n",
        "}\n",
        "\n",
        "public struct BigTransfer: Layer {\n",
        "  public var inputStem: StandardizedConv2D\n",
        "  public var maxPool: MaxPool2D<Float>\n",
        "  public var residualBlocks: [ResidualBlockBiT] = []\n",
        "  public var groupNorm : GroupNorm<Float>\n",
        "  public var flatten = Flatten<Float>()\n",
        "  public var classifier: Dense<Float>\n",
        "  public var avgPool = GlobalAvgPool2D<Float>()\n",
        "  @noDerivative public var finalOutFilter : Int = 0\n",
        "\n",
        "  public init(\n",
        "        classCount: Int, \n",
        "        depth: Depth, \n",
        "        inputChannels: Int = 3\n",
        "    ) {\n",
        "\n",
        "        self.inputStem = StandardizedConv2D(filterShape: (7, 7, 3, 64), strides: (2, 2), padding: .valid, useBias: false)\n",
        "        self.maxPool = MaxPool2D(poolSize: (3, 3), strides: (2, 2), padding: .valid)\n",
        "        let sizes = [64 / depth.expansion, 64, 128, 256, 512]\n",
        "        for (iBlock, nBlocks) in depth.layerBlockSizes.enumerated() {\n",
        "            let (nIn, nOut) = (sizes[iBlock] * depth.expansion, sizes[iBlock+1] * depth.expansion)\n",
        "            for j in 0..<nBlocks {\n",
        "\n",
        "                self.residualBlocks.append(ResidualBlockBiT(\n",
        "                    inFilters: j==0 ? nIn : nOut,  \n",
        "                    outFilters: nOut, \n",
        "                    stride: (iBlock != 0) && (j == 0) ? 2 : 1, \n",
        "                    expansion: depth.expansion\n",
        "                ))\n",
        "                self.finalOutFilter = nOut\n",
        "            }\n",
        "        }\n",
        "        self.groupNorm = GroupNorm<Float>(\n",
        "              offset: Tensor(zeros: [self.finalOutFilter]),\n",
        "              scale: Tensor(zeros: [self.finalOutFilter]),\n",
        "              groupCount: 2,\n",
        "              axis: -1,\n",
        "              epsilon: 0.001)\n",
        "        self.classifier = Dense(inputSize: 512 * depth.expansion, outputSize: classCount)\n",
        "    }\n",
        "\n",
        "  @differentiable\n",
        "  public func callAsFunction(_ input: Tensor<Float>) -> Tensor<Float> {\n",
        "      var paddedInput = input.padded(forSizes: paddingFromKernelSize(kernelSize: 7))\n",
        "      paddedInput = inputStem(paddedInput).padded(forSizes: paddingFromKernelSize(kernelSize: 3))\n",
        "      let inputLayer = maxPool(paddedInput)\n",
        "      let blocksReduced = residualBlocks.differentiableReduce(inputLayer) { $1($0) }\n",
        "      let normalized = relu(groupNorm(blocksReduced))\n",
        "      return normalized.sequenced(through: avgPool, flatten, classifier)\n",
        "  }\n",
        "}\n",
        "\n",
        "extension BigTransfer {\n",
        "    public enum Depth {\n",
        "        case resNet18\n",
        "        case resNet34\n",
        "        case resNet50\n",
        "        case resNet101\n",
        "        case resNet152\n",
        "\n",
        "        var expansion: Int {\n",
        "            switch self {\n",
        "            case .resNet18, .resNet34: return 1\n",
        "            default: return 4\n",
        "            }\n",
        "        }\n",
        "\n",
        "        var layerBlockSizes: [Int] {\n",
        "            switch self {\n",
        "            case .resNet18:  return [2, 2, 2,  2]\n",
        "            case .resNet34:  return [3, 4, 6,  3]\n",
        "            case .resNet50:  return [3, 4, 6,  3]\n",
        "            case .resNet101: return [3, 4, 23, 3]\n",
        "            case .resNet152: return [3, 8, 36, 3]\n",
        "            }\n",
        "        }\n",
        "    }\n",
        "}"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TpdLHcEOpGeU"
      },
      "source": [
        "func get_model_units(model_name: String) -> BigTransfer.Depth {\n",
        "  if model_name.contains(\"R50\") {\n",
        "    return .resNet50\n",
        "  }\n",
        "  else if model_name.contains(\"R101\") {\n",
        "    return .resNet101\n",
        "  }\n",
        "  else {\n",
        "    return .resNet152\n",
        "  }\n",
        "}\n",
        "\n",
        "let bit_type = get_model_units(model_name: model_name)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o-MYC9LUr4HM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "41feae4c-28a6-42f8-a995-2fedae02a16e"
      },
      "source": [
        "var resnetv2 = BigTransfer(classCount: 10, depth: .resNet50)\n",
        "\n",
        "let convs = weights_array.filter {key in return key.name.contains(\"/block\") && key.name.contains(\"standardized_conv2d/kernel\") && !(key.name.contains(\"proj\"))}\n",
        "\n",
        "var k = 0\n",
        "for (idx, i) in resnetv2.residualBlocks.enumerated() {\n",
        "  for (jdx, _) in i.convs.enumerated() {\n",
        "    assert(resnetv2.residualBlocks[idx].convs[jdx].conv.conv.filter.shape == convs[k].layer.shape)\n",
        "    resnetv2.residualBlocks[idx].convs[jdx].conv.conv.filter = convs[k].layer\n",
        "    k = k + 1\n",
        "  }\n",
        "}\n",
        "print(convs.count, k)\n",
        "// https://github.com/zaidalyafeai/Swift4TF/blob/master/Swift4TF_TransferLearning.ipynb "
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "48 48\r\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PtrRmtXMHLU6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f3007646-1b18-4beb-8a05-cf4ad9a22f6c"
      },
      "source": [
        "// Set weights for all projective convolutions\n",
        "let projective_convs = weights_array.filter {key in return key.name.contains(\"/block\") && key.name.contains(\"standardized_conv2d/kernel\") && (key.name.contains(\"proj\"))}\n",
        "let norm_scale = weights_array.filter {key in return key.name.contains(\"unit01/a/group_norm/gamma\")}\n",
        "let norm_offset = weights_array.filter {key in return key.name.contains(\"unit01/a/group_norm/beta\")}\n",
        "\n",
        "var k = 0\n",
        "for (idx, i) in resnetv2.residualBlocks.enumerated() {\n",
        "    if (i.shortcut.projection.conv.filter.shape != [1, 1, 1, 1])\n",
        "    {\n",
        "      assert(resnetv2.residualBlocks[idx].shortcut.projection.conv.filter.shape == projective_convs[k].layer.shape)\n",
        "      resnetv2.residualBlocks[idx].shortcut.projection.conv.filter = projective_convs[k].layer\n",
        "\n",
        "      assert(resnetv2.residualBlocks[idx].shortcut.norm.scale.shape == norm_scale[k].layer.shape)\n",
        "      resnetv2.residualBlocks[idx].shortcut.norm.scale = norm_scale[k].layer\n",
        "\n",
        "      assert(resnetv2.residualBlocks[idx].shortcut.norm.offset.shape == norm_offset[k].layer.shape)\n",
        "      resnetv2.residualBlocks[idx].shortcut.norm.offset = norm_offset[k].layer\n",
        "      k = k + 1\n",
        "    }\n",
        "}\n",
        "print(projective_convs.count, k)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "4 4\r\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hu7mrFlh7j0P"
      },
      "source": [
        "// Set weights for all standard norms\n",
        "let norm_scale = weights_array.filter {key in return key.name.contains(\"gamma\")}\n",
        "\n",
        "var k = 0\n",
        "for (idx, i) in resnetv2.residualBlocks.enumerated() {\n",
        "  for (jdx, _) in i.convs.enumerated() {\n",
        "    assert(norm_scale[k].layer.shape == resnetv2.residualBlocks[idx].convs[jdx].norm.scale.shape)\n",
        "    resnetv2.residualBlocks[idx].convs[jdx].norm.scale = norm_scale[k].layer\n",
        "    k = k + 1\n",
        "  }\n",
        "}\n",
        "\n",
        "let norm_offset = weights_array.filter {key in return key.name.contains(\"beta\")}\n",
        "\n",
        "var l = 0\n",
        "for (idx, i) in resnetv2.residualBlocks.enumerated() {\n",
        "  for (jdx, _) in i.convs.enumerated() {\n",
        "    assert(norm_offset[l].layer.shape == resnetv2.residualBlocks[idx].convs[jdx].norm.offset.shape)\n",
        "    resnetv2.residualBlocks[idx].convs[jdx].norm.offset = norm_offset[l].layer\n",
        "    l = l + 1\n",
        "  }\n",
        "}\n",
        "\n",
        "assert(resnetv2.groupNorm.scale.shape == norm_scale[k].layer.shape)\n",
        "resnetv2.groupNorm.scale = norm_scale[k].layer\n",
        "assert(resnetv2.groupNorm.offset.shape == norm_offset[l].layer.shape)\n",
        "resnetv2.groupNorm.offset = norm_offset[l].layer"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XSGpiMF-Fcxj"
      },
      "source": [
        "\n"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a62-npIMD24Y"
      },
      "source": [
        "// Set weight for input block\n",
        "let root_convs = weights_array.filter {key in return key.name.contains(\"root_block\")}\n",
        "assert(resnetv2.inputStem.conv.filter.shape == root_convs[0].layer.shape)\n",
        "resnetv2.inputStem.conv.filter = root_convs[0].layer"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HZWyUKAxEHqt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b216a909-7770-4ddb-daf5-6128d53923f3"
      },
      "source": [
        "let norm_scale = weights_array.filter {key in return key.name.contains(\"unit01/a/group_norm/gamma\")}\n",
        "print(norm_scale.count, projective_convs.count)\n",
        "for i in projective_convs {\n",
        "  print(i.name)\n",
        "}\n",
        "\n"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "4 4\r\n",
            "resnet/block1/unit01/a/proj/standardized_conv2d/kernel\r\n",
            "resnet/block2/unit01/a/proj/standardized_conv2d/kernel\r\n",
            "resnet/block3/unit01/a/proj/standardized_conv2d/kernel\r\n",
            "resnet/block4/unit01/a/proj/standardized_conv2d/kernel\r\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TYKUEjZTQx18",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c09f433c-fdba-490f-a3ea-55121868a9b3"
      },
      "source": [
        "let norm_scale = weights_array.filter {key in return key.name.contains(\"dense\")}\n",
        "norm_scale"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0 elements\n"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "51749rK4sSFS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b63494c6-ddeb-44d5-c47c-37f4292ba6c5"
      },
      "source": [
        "let _ = _ExecutionContext.global\n",
        "\n",
        "let device = Device.defaultXLA\n",
        "device"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "‚ñø Device(kind: .GPU, ordinal: 0, backend: .XLA)\n",
              "  - kind : TensorFlow.Device.Kind.GPU\n",
              "  - ordinal : 0\n",
              "  - backend : TensorFlow.Device.Backend.XLA\n"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "17K06oETPq_P",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "26c0e745-85f3-441a-fa9a-514b8551cebf"
      },
      "source": [
        "resnetv2.move(to: device)\n",
        "var optimizer = SGD(for: resnetv2, learningRate: 0.001, momentum: 0.9)\n",
        "optimizer = SGD(copying: optimizer, to: device)\n",
        "\n",
        "let cifar10_training_size = 50000\n",
        "\n",
        "Context.local.learningPhase = .training\n",
        "let batchSize = 128\n",
        "let lrSupports = get_schedule(dataset_size: cifar10_training_size)\n",
        "let scheduleLength = lrSupports.last!\n",
        "let stepsPerEpoch = cifar10_training_size / batchSize\n",
        "\n",
        "\n",
        "var epochCount = scheduleLength / stepsPerEpoch\n",
        "let dataset = CIFAR10(batchSize: batchSize, on: Device.default)\n",
        "\n",
        "print(stepsPerEpoch)\n",
        "print(epochCount)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "390\r\n",
            "25\r\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KHxauxK0OFCN"
      },
      "source": [
        "struct Statistics {\n",
        "    var correctGuessCount = Tensor<Int32>(0, on: Device.default)\n",
        "    var totalGuessCount = Tensor<Int32>(0, on: Device.default)\n",
        "    var totalLoss = Tensor<Float>(0, on: Device.default)\n",
        "    var batches: Int = 0\n",
        "    var accuracy: Float { \n",
        "        Float(correctGuessCount.scalarized()) / Float(totalGuessCount.scalarized()) * 100 \n",
        "    } \n",
        "    var averageLoss: Float { totalLoss.scalarized() / Float(batches) }\n",
        "\n",
        "    init(on device: Device = Device.default) {\n",
        "        correctGuessCount = Tensor<Int32>(0, on: device)\n",
        "        totalGuessCount = Tensor<Int32>(0, on: device)\n",
        "        totalLoss = Tensor<Float>(0, on: device)\n",
        "    }\n",
        "\n",
        "    mutating func update(logits: Tensor<Float>, labels: Tensor<Float>, loss: Tensor<Float>) {\n",
        "        let correct = logits.argmax(squeezingAxis: 1) .== labels.argmax(squeezingAxis: 1)\n",
        "        correctGuessCount += Tensor<Int32>(correct).sum()\n",
        "        totalGuessCount += Int32(labels.shape[0])\n",
        "        totalLoss += loss\n",
        "        batches += 1\n",
        "    }\n",
        "}\n",
        "\n",
        "\n"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jMKT1a_pa8ut"
      },
      "source": [
        ""
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wzYP191ERHI8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "58dd3ef0-89a3-4e8c-e8d1-febcac7e43fc"
      },
      "source": [
        "print(\"Beginning training...\")\n",
        "var curr_step: Int = 1\n",
        "let resize_size = get_resolution(original_resolution: (32, 32))\n",
        "var mixup_alpha = get_mixup(dataset_size: cifar10_training_size)\n",
        "let beta = np.random.beta(mixup_alpha, mixup_alpha)\n",
        "\n",
        "for (epoch, batches) in dataset.training.prefix(epochCount).enumerated() {\n",
        "    let start = Date()\n",
        "    var trainStats = Statistics(on: device)\n",
        "    var testStats = Statistics(on: device)\n",
        "    \n",
        "    Context.local.learningPhase = .training\n",
        "    for batch in batches {\n",
        "        if let new_lr = get_lr(step: curr_step, dataset_size: cifar10_training_size, base_lr: 0.003) {\n",
        "          optimizer.learningRate = new_lr\n",
        "          curr_step = curr_step + 1\n",
        "        }\n",
        "        else {\n",
        "          continue\n",
        "        }\n",
        "\n",
        "        var (eagerImages, eagerLabels) = (batch.data, batch.label)\n",
        "        var npImages = eagerImages.makeNumpyArray()\n",
        "        var resized = tf.image.resize(npImages, [resize_size.0, resize_size.0])\n",
        "        var cropped = tf.image.random_crop(resized, [batchSize, resize_size.1, resize_size.1, 3])\n",
        "        var flipped = tf.image.random_flip_left_right(cropped)\n",
        "        var mixed_up = flipped\n",
        "        var newLabels = Tensor<Float>(Tensor<Int32>(oneHotAtIndices: eagerLabels, depth: 10))\n",
        "        if mixup_alpha > 0.0 {\n",
        "          var npLabels = newLabels.makeNumpyArray()\n",
        "          mixed_up = beta * mixed_up + (1 - beta) * tf.reverse(mixed_up, axis: [0])\n",
        "          npLabels = beta * npLabels + (1 - beta) * tf.reverse(npLabels, axis: [0])\n",
        "          newLabels = Tensor<Float>(numpy: npLabels.numpy())!\n",
        "        }\n",
        "        eagerImages = Tensor<Float>(numpy: mixed_up.numpy())!\n",
        "        let images = Tensor(copying: eagerImages, to: device)\n",
        "        let labels = Tensor(copying: newLabels, to: device)\n",
        "\n",
        "        let ùõÅmodel = TensorFlow.gradient(at: resnetv2) { resnetv2 -> Tensor<Float> in\n",
        "            let ≈∑ = resnetv2(images)\n",
        "            let loss = softmaxCrossEntropy(logits: ≈∑, probabilities: labels)\n",
        "            trainStats.update(logits: ≈∑, labels: labels, loss: loss)\n",
        "            return loss\n",
        "        }\n",
        "\n",
        "        \n",
        "        optimizer.update(&resnetv2, along: ùõÅmodel)\n",
        "        LazyTensorBarrier()\n",
        "    }\n",
        "\n",
        "    Context.local.learningPhase = .inference\n",
        "    for batch in dataset.validation {\n",
        "        var (eagerImages, eagerLabels) = (batch.data, batch.label)\n",
        "        var npImages = eagerImages.makeNumpyArray()\n",
        "        var resized = tf.image.resize(npImages, [resize_size.0, resize_size.0])\n",
        "        eagerImages = Tensor<Float>(numpy: resized.numpy())!\n",
        "        var newLabels = Tensor<Float>(Tensor<Int32>(oneHotAtIndices: eagerLabels, depth: 10))\n",
        "        let images = Tensor(copying: eagerImages, to: device)\n",
        "        let labels = Tensor(copying: newLabels, to: device)\n",
        "        let ≈∑ = resnetv2(images)\n",
        "        let loss = softmaxCrossEntropy(logits: ≈∑, probabilities: labels)\n",
        "        LazyTensorBarrier()\n",
        "        testStats.update(logits: ≈∑, labels: labels, loss: loss)\n",
        "    }\n",
        "\n",
        "    print(\n",
        "        \"\"\"\n",
        "        [Epoch \\(epoch)] \\\n",
        "        Training Loss: \\(String(format: \"%.3f\", trainStats.averageLoss)), \\\n",
        "        Training Accuracy: \\(trainStats.correctGuessCount)/\\(trainStats.totalGuessCount) \\\n",
        "        (\\(String(format: \"%.1f\", trainStats.accuracy))%), \\\n",
        "        Test Loss: \\(String(format: \"%.3f\", testStats.averageLoss)), \\\n",
        "        Test Accuracy: \\(testStats.correctGuessCount)/\\(testStats.totalGuessCount) \\\n",
        "        (\\(String(format: \"%.1f\", testStats.accuracy))%) \\\n",
        "        seconds per epoch: \\(String(format: \"%.1f\", Date().timeIntervalSince(start)))\n",
        "        \"\"\")\n",
        "}\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Beginning training...\n",
            "2020-11-18 03:06:16.838107: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcuda.so.1\n",
            "2020-11-18 03:06:16.838328: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-11-18 03:06:16.838990: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1716] Found device 0 with properties: \n",
            "pciBusID: 0000:00:04.0 name: Tesla T4 computeCapability: 7.5\n",
            "coreClock: 1.59GHz coreCount: 40 deviceMemorySize: 14.73GiB deviceMemoryBandwidth: 298.08GiB/s\n",
            "2020-11-18 03:06:16.839061: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\n",
            "2020-11-18 03:06:16.839113: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcublas.so.10\n",
            "2020-11-18 03:06:16.839149: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcufft.so.10\n",
            "2020-11-18 03:06:16.839180: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcurand.so.10\n",
            "2020-11-18 03:06:16.839209: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusolver.so.10\n",
            "2020-11-18 03:06:16.839238: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusparse.so.10\n",
            "2020-11-18 03:06:16.839267: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudnn.so.7\n",
            "2020-11-18 03:06:16.839373: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-11-18 03:06:16.840071: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-11-18 03:06:16.840626: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1858] Adding visible gpu devices: 0\n",
            "2020-11-18 03:06:16.847957: I tensorflow/core/platform/profile_utils/cpu_utils.cc:104] CPU Frequency: 2200000000 Hz\n",
            "2020-11-18 03:06:16.848282: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x2f77f640 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
            "2020-11-18 03:06:16.848317: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
            "2020-11-18 03:06:16.850267: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-11-18 03:06:16.850974: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x2f7fb0f0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
            "2020-11-18 03:06:16.851002: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Tesla T4, Compute Capability 7.5\n",
            "2020-11-18 03:06:16.851239: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-11-18 03:06:16.851868: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1716] Found device 0 with properties: \n",
            "pciBusID: 0000:00:04.0 name: Tesla T4 computeCapability: 7.5\n",
            "coreClock: 1.59GHz coreCount: 40 deviceMemorySize: 14.73GiB deviceMemoryBandwidth: 298.08GiB/s\n",
            "2020-11-18 03:06:16.851928: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\n",
            "2020-11-18 03:06:16.851970: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcublas.so.10\n",
            "2020-11-18 03:06:16.852017: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcufft.so.10\n",
            "2020-11-18 03:06:16.852047: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcurand.so.10\n",
            "2020-11-18 03:06:16.852075: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusolver.so.10\n",
            "2020-11-18 03:06:16.852102: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusparse.so.10\n",
            "2020-11-18 03:06:16.852130: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudnn.so.7\n",
            "2020-11-18 03:06:16.852213: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-11-18 03:06:16.852888: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-11-18 03:06:16.853460: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1858] Adding visible gpu devices: 0\n",
            "2020-11-18 03:06:16.853519: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\n",
            "2020-11-18 03:06:17.283003: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1257] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
            "2020-11-18 03:06:17.283064: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1263]      0 \n",
            "2020-11-18 03:06:17.283077: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1276] 0:   N \n",
            "2020-11-18 03:06:17.283354: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-11-18 03:06:17.284120: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-11-18 03:06:17.284718: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
            "2020-11-18 03:06:17.284765: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1402] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 13716 MB memory) -> physical GPU (device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5)\n",
            "[Epoch 0] Training Loss: 0.859, Training Accuracy: 37328/49920 (74.8%), Test Loss: 0.358, Test Accuracy: 8794/10000 (87.9%) seconds per epoch: 380.9\n",
            "[Epoch 1] Training Loss: 0.264, Training Accuracy: 45648/49920 (91.4%), Test Loss: 0.195, Test Accuracy: 9359/10000 (93.6%) seconds per epoch: 310.2\n",
            "[Epoch 2] Training Loss: 0.172, Training Accuracy: 47234/49920 (94.6%), Test Loss: 0.169, Test Accuracy: 9414/10000 (94.1%) seconds per epoch: 278.0\n",
            "[Epoch 3] Training Loss: 0.127, Training Accuracy: 47958/49920 (96.1%), Test Loss: 0.154, Test Accuracy: 9484/10000 (94.8%) seconds per epoch: 278.1\n",
            "[Epoch 4] Training Loss: 0.106, Training Accuracy: 48363/49920 (96.9%), Test Loss: 0.149, Test Accuracy: 9498/10000 (95.0%) seconds per epoch: 278.2\n",
            "[Epoch 5] Training Loss: 0.083, Training Accuracy: 48759/49920 (97.7%), Test Loss: 0.132, Test Accuracy: 9572/10000 (95.7%) seconds per epoch: 278.2\n",
            "[Epoch 6] Training Loss: 0.074, Training Accuracy: 48937/49920 (98.0%), Test Loss: 0.142, Test Accuracy: 9546/10000 (95.5%) seconds per epoch: 277.8\n",
            "[Epoch 7] Training Loss: 0.068, Training Accuracy: 48993/49920 (98.1%), Test Loss: 0.116, Test Accuracy: 9600/10000 (96.0%) seconds per epoch: 277.2\n",
            "[Epoch 8] Training Loss: 0.041, Training Accuracy: 49540/49920 (99.2%), Test Loss: 0.104, Test Accuracy: 9657/10000 (96.6%) seconds per epoch: 277.2\n",
            "[Epoch 9] Training Loss: 0.033, Training Accuracy: 49649/49920 (99.5%), Test Loss: 0.101, Test Accuracy: 9658/10000 (96.6%) seconds per epoch: 276.9\n",
            "[Epoch 10] Training Loss: 0.030, Training Accuracy: 49695/49920 (99.5%), Test Loss: 0.100, Test Accuracy: 9674/10000 (96.7%) seconds per epoch: 277.6\n",
            "[Epoch 11] Training Loss: 0.029, Training Accuracy: 49723/49920 (99.6%), Test Loss: 0.102, Test Accuracy: 9676/10000 (96.8%) seconds per epoch: 277.1\n",
            "[Epoch 12] Training Loss: 0.029, Training Accuracy: 49707/49920 (99.6%), Test Loss: 0.101, Test Accuracy: 9687/10000 (96.9%) seconds per epoch: 278.5\n",
            "[Epoch 13] Training Loss: 0.027, Training Accuracy: 49759/49920 (99.7%), Test Loss: 0.103, Test Accuracy: 9682/10000 (96.8%) seconds per epoch: 276.7\n",
            "[Epoch 14] Training Loss: 0.024, Training Accuracy: 49791/49920 (99.7%), Test Loss: 0.103, Test Accuracy: 9691/10000 (96.9%) seconds per epoch: 276.3\n",
            "[Epoch 15] Training Loss: 0.025, Training Accuracy: 49803/49920 (99.8%), Test Loss: 0.102, Test Accuracy: 9686/10000 (96.9%) seconds per epoch: 277.0\n",
            "[Epoch 16] Training Loss: 0.023, Training Accuracy: 49815/49920 (99.8%), Test Loss: 0.102, Test Accuracy: 9685/10000 (96.8%) seconds per epoch: 277.9\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mOxWs5HlYm9X",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5652fb8b-6651-4e1f-ad8b-9b441f43e56e"
      },
      "source": [
        "// Try doing SWA on stable LR\n",
        "\n",
        "\"\"\"\n",
        "On ImageNet we experimented with ResNet-50, ResNet-152[He et al., 2016] and DenseNet-161[Huang et al.,2017].  \n",
        "For these architectures we used pretrained mod-els  fromPyTorch.torchvision.   For  each  of  themodels we ran \n",
        "SWA for10epochs with a cyclical learn-ing rate schedule with the same parameters for all models\n",
        "(the details can be found in the Appendix), \n",
        "and report themean and standard deviation of test error averaged over3runs. The results are shown in Table 2.\n",
        "\"\"\"\n",
        "// Run 10 epochs of cyclical LR at the end here with SWA\n",
        "// https://arxiv.org/pdf/1803.05407.pdf\n",
        "// https://pytorch.org/blog/stochastic-weight-averaging-in-pytorch/\n",
        "// https://github.com/izmailovpavel/contrib_swa_examples/blob/master/train.py"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"On ImageNet we experimented with ResNet-50, ResNet-152[He et al., 2016] and DenseNet-161[Huang et al.,2017].  \\nFor these architectures we used pretrained mod-els  fromPyTorch.torchvision.   For  each  of  themodels we ran \\nSWA for10epochs with a cyclical learn-ing rate schedule with the same parameters for all models\\n(the details can be found in the Appendix), \\nand report themean and standard deviation of test error averaged over3runs. The results are shown in Table 2.\"\n"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mIpxzm0swMvv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "24e4bb7e-822e-49ea-c844-68d98b82161d"
      },
      "source": [
        "func get_cosine_annealing_lr(step: Int, steps_per_epoch: Int, base_lr: Float = 0.003, max_lr: Float = 0.1) -> Float? {\n",
        "    return base_lr + 0.5*(max_lr - base_lr)*(1 + cos(Float.pi * Float(step)/Float(steps_per_epoch)))\n",
        "}\n",
        "get_cosine_annealing_lr(step: 1, steps_per_epoch: 100)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "‚ñø Optional<Float>\n",
              "  - some : 0.09997606\n"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F18FmtFIsxWP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "90780be0-2ec1-4807-b990-3fc1be0de3e8"
      },
      "source": [
        "var swaModelWeights: BigTransfer.TangentVector = BigTransfer.TangentVector.zero\n",
        "swaModelWeights = BigTransfer.TangentVector(copying: swaModelWeights, to: device)\n",
        "\n",
        "var numCycles = 10\n",
        "for (epoch, batches) in dataset.training.prefix(numCycles).enumerated() {\n",
        "    let start = Date()\n",
        "    var trainStats = Statistics(on: device)\n",
        "    var testStats = Statistics(on: device)\n",
        "    \n",
        "    var currStep: Int = 0\n",
        "    Context.local.learningPhase = .training\n",
        "    for batch in batches {\n",
        "        if let new_lr = get_cosine_annealing_lr(step: currStep, steps_per_epoch: stepsPerEpoch, base_lr: 0.0001, max_lr: 0.001) {\n",
        "          optimizer.learningRate = new_lr\n",
        "          curr_step = curr_step + 1\n",
        "        }\n",
        "        else {\n",
        "          continue\n",
        "        }\n",
        "\n",
        "        var (eagerImages, eagerLabels) = (batch.data, batch.label)\n",
        "        var npImages = eagerImages.makeNumpyArray()\n",
        "        var resized = tf.image.resize(npImages, [resize_size.0, resize_size.0])\n",
        "        var cropped = tf.image.random_crop(resized, [batchSize, resize_size.1, resize_size.1, 3])\n",
        "        var flipped = tf.image.random_flip_left_right(cropped)\n",
        "        var mixed_up = flipped\n",
        "        var newLabels = Tensor<Float>(Tensor<Int32>(oneHotAtIndices: eagerLabels, depth: 10))\n",
        "        if mixup_alpha > 0.0 {\n",
        "          var npLabels = newLabels.makeNumpyArray()\n",
        "          mixed_up = beta * mixed_up + (1 - beta) * tf.reverse(mixed_up, axis: [0])\n",
        "          npLabels = beta * npLabels + (1 - beta) * tf.reverse(npLabels, axis: [0])\n",
        "          newLabels = Tensor<Float>(numpy: npLabels.numpy())!\n",
        "        }\n",
        "        eagerImages = Tensor<Float>(numpy: mixed_up.numpy())!\n",
        "        let images = Tensor(copying: eagerImages, to: device)\n",
        "        let labels = Tensor(copying: newLabels, to: device)\n",
        "        let ùõÅmodel = TensorFlow.gradient(at: resnetv2) { resnetv2 -> Tensor<Float> in\n",
        "            let ≈∑ = resnetv2(images)\n",
        "            let loss = softmaxCrossEntropy(logits: ≈∑, probabilities: labels)\n",
        "            trainStats.update(logits: ≈∑, labels: labels, loss: loss)\n",
        "            return loss\n",
        "        }\n",
        "\n",
        "        optimizer.update(&resnetv2, along: ùõÅmodel)\n",
        "        LazyTensorBarrier()\n",
        "\n",
        "        currStep = currStep + 1\n",
        "        if currStep == stepsPerEpoch - 1 {\n",
        "          print(\"Adding model weights\")\n",
        "          swaModelWeights = swaModelWeights + ùõÅmodel\n",
        "        }\n",
        "    }\n",
        "\n",
        "    Context.local.learningPhase = .inference\n",
        "    for batch in dataset.validation {\n",
        "        var (eagerImages, eagerLabels) = (batch.data, batch.label)\n",
        "        var npImages = eagerImages.makeNumpyArray()\n",
        "        var resized = tf.image.resize(npImages, [resize_size.0, resize_size.0])\n",
        "        eagerImages = Tensor<Float>(numpy: resized.numpy())!\n",
        "        var newLabels = Tensor<Float>(Tensor<Int32>(oneHotAtIndices: eagerLabels, depth: 10))\n",
        "        let images = Tensor(copying: eagerImages, to: device)\n",
        "        let labels = Tensor(copying: newLabels, to: device)\n",
        "        let ≈∑ = resnetv2(images)\n",
        "        let loss = softmaxCrossEntropy(logits: ≈∑, probabilities: labels)\n",
        "        LazyTensorBarrier()\n",
        "        testStats.update(logits: ≈∑, labels: labels, loss: loss)\n",
        "    }\n",
        "\n",
        "    print(\n",
        "        \"\"\"\n",
        "        [Epoch \\(epoch)] \\\n",
        "        Training Loss: \\(String(format: \"%.3f\", trainStats.averageLoss)), \\\n",
        "        Training Accuracy: \\(trainStats.correctGuessCount)/\\(trainStats.totalGuessCount) \\\n",
        "        (\\(String(format: \"%.1f\", trainStats.accuracy))%), \\\n",
        "        Test Loss: \\(String(format: \"%.3f\", testStats.averageLoss)), \\\n",
        "        Test Accuracy: \\(testStats.correctGuessCount)/\\(testStats.totalGuessCount) \\\n",
        "        (\\(String(format: \"%.1f\", testStats.accuracy))%) \\\n",
        "        seconds per epoch: \\(String(format: \"%.1f\", Date().timeIntervalSince(start)))\n",
        "        \"\"\")\n",
        "}"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Adding model weights\n",
            "[Epoch 0] Training Loss: 3.373, Training Accuracy: 9053/49920 (18.1%), Test Loss: 2.163, Test Accuracy: 1983/10000 (19.8%) seconds per epoch: 303.4\n",
            "Adding model weights\n",
            "[Epoch 1] Training Loss: 2.106, Training Accuracy: 11060/49920 (22.2%), Test Loss: 2.077, Test Accuracy: 2305/10000 (23.0%) seconds per epoch: 307.1\n",
            "Adding model weights\n",
            "[Epoch 2] Training Loss: 2.067, Training Accuracy: 11664/49920 (23.4%), Test Loss: 2.035, Test Accuracy: 2461/10000 (24.6%) seconds per epoch: 278.8\n",
            "Adding model weights\n",
            "[Epoch 3] Training Loss: 2.031, Training Accuracy: 12311/49920 (24.7%), Test Loss: 2.001, Test Accuracy: 2561/10000 (25.6%) seconds per epoch: 283.6\n",
            "Adding model weights\n",
            "[Epoch 4] Training Loss: 2.011, Training Accuracy: 12890/49920 (25.8%), Test Loss: 1.991, Test Accuracy: 2651/10000 (26.5%) seconds per epoch: 285.5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KPiwQA-fd471"
      },
      "source": [
        "var averagedModelWeights = swaModelWeights\n",
        "averagedModelWeights.inputStem.conv.filter = swaModelWeights.inputStem.conv.filter/Float(numCycles)\n",
        "averagedModelWeights.groupNorm.offset = swaModelWeights.groupNorm.offset/Float(numCycles)\n",
        "averagedModelWeights.groupNorm.scale = swaModelWeights.groupNorm.scale/Float(numCycles)\n",
        "averagedModelWeights.classifier.weight = swaModelWeights.classifier.weight/Float(numCycles)\n",
        "averagedModelWeights.classifier.bias = swaModelWeights.classifier.bias/Float(numCycles)\n",
        "\n",
        "for (index, weight)  in swaModelWeights.residualBlocks.enumerated() {\n",
        "  averagedModelWeights.residualBlocks[index].shortcut.projection.conv.filter = weight.shortcut.projection.conv.filter/Float(numCycles)\n",
        "  averagedModelWeights.residualBlocks[index].shortcut.norm.offset = weight.shortcut.norm.offset/Float(numCycles)\n",
        "  averagedModelWeights.residualBlocks[index].shortcut.norm.scale = weight.shortcut.norm.scale/Float(numCycles)\n",
        "     for (conv_index, conv_weight) in weight.convs.enumerated() {\n",
        "       averagedModelWeights.residualBlocks[index].convs[conv_index].conv.conv.filter = conv_weight.conv.conv.filter/Float(numCycles)\n",
        "       averagedModelWeights.residualBlocks[index].convs[conv_index].norm.offset = conv_weight.norm.offset/Float(numCycles)\n",
        "       averagedModelWeights.residualBlocks[index].convs[conv_index].norm.scale = conv_weight.norm.scale/Float(numCycles)\n",
        "     }\n",
        "\n",
        "}"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iHY3FEzx5v0k"
      },
      "source": [
        "optimizer.update(&resnetv2, along: averagedModelWeights)"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0W2hwKo76BbN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ceda8299-5d90-46ff-81ff-c14d86574573"
      },
      "source": [
        "var testStats = Statistics(on: device)\n",
        "Context.local.learningPhase = .inference\n",
        "for batch in dataset.validation {\n",
        "    var (eagerImages, eagerLabels) = (batch.data, batch.label)\n",
        "    var npImages = eagerImages.makeNumpyArray()\n",
        "    var resized = tf.image.resize(npImages, [resize_size.0, resize_size.0])\n",
        "    eagerImages = Tensor<Float>(numpy: resized.numpy())!\n",
        "    var newLabels = Tensor<Float>(Tensor<Int32>(oneHotAtIndices: eagerLabels, depth: 10))\n",
        "    let images = Tensor(copying: eagerImages, to: device)\n",
        "    let labels = Tensor(copying: newLabels, to: device)\n",
        "    let ≈∑ = resnetv2(images)\n",
        "    let loss = softmaxCrossEntropy(logits: ≈∑, probabilities: labels)\n",
        "    LazyTensorBarrier()\n",
        "    testStats.update(logits: ≈∑, labels: labels, loss: loss)\n",
        "}\n",
        "print(\n",
        "  \"\"\"\n",
        "  Test Accuracy: \\(testStats.correctGuessCount)/\\(testStats.totalGuessCount)\n",
        "  \"\"\")"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test Accuracy: 2642/10000\r\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_cp2_2dPvLpp"
      },
      "source": [
        ""
      ],
      "execution_count": 29,
      "outputs": []
    }
  ]
}