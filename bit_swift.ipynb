{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "bit_swift.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "swift",
      "display_name": "Swift"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/acoadmarmon/big-transfer-swift/blob/main/bit_swift.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kZRlD4utdPuX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fa5d6c57-5e99-4c59-89bc-381748ad9d74"
      },
      "source": [
        "%install '.package(url: \"https://github.com/tensorflow/swift-models\", .branch(\"master\"))' Datasets ImageClassificationModels\n",
        "print(\"\\u{001B}[2J\")"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\r\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wTl6d5efpeb_"
      },
      "source": [
        "import Datasets\n",
        "import ImageClassificationModels\n",
        "import TensorFlow\n",
        "import Python\n",
        "import Foundation\n",
        "import Differentiable\n"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JDjdyWTlaDBc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "66a3b245-b0bb-4a68-ea6f-48576ed52de8"
      },
      "source": [
        "%include \"EnableIPythonDisplay.swift\"\n",
        "IPythonDisplay.shell.enable_matplotlib(\"inline\")\n",
        "\n",
        "let plt = Python.import(\"matplotlib.pyplot\")\n",
        "let np  = Python.import(\"numpy\")\n",
        "let subprocess = Python.import(\"subprocess\")\n",
        "let glob = Python.import(\"glob\")\n",
        "let pil = Python.import(\"PIL\")\n",
        "let tf = Python.import(\"tensorflow\")\n",
        "//let tfp = Python.import(\"tensorflow_probability\")\n",
        "let h5py = Python.import(\"h5py\")\n",
        "let path = Python.import(\"os.path\")"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2020-11-24 23:31:27.610901: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\r\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h3ilNx78jjEA"
      },
      "source": [
        "//subprocess.call(\"curl https://sdk.cloud.google.com | bash; exec -l $SHELL; gsutil ls gs://uspto-pair/applications/0800401*\", shell: true)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E2SFbiPWpinG"
      },
      "source": [
        "let epochCount = 12\n",
        "let batchSize = 32"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jAEB0ZyHrvGS"
      },
      "source": [
        "// Set hyperrule parameters from bit_hyperrule.py\n",
        "enum ValueError: Error {\n",
        "    case invalidInput(String)\n",
        "}\n",
        "\n",
        "func get_resolution(original_resolution: (Int, Int)) -> (Int, Int) {\n",
        "  let area = original_resolution.0 * original_resolution.1\n",
        "  return area < 96*96 ? (160, 128) : (512, 480)\n",
        "}\n",
        "\n",
        "\n",
        "let known_dataset_sizes:[String: (Int, Int)] = [\n",
        "  \"cifar10\": (32, 32),\n",
        "  \"cifar100\": (32, 32),\n",
        "  \"oxford_iiit_pet\": (224, 224),\n",
        "  \"oxford_flowers102\": (224, 224),\n",
        "  \"imagenet2012\": (224, 224),\n",
        "]\n",
        "\n",
        "func get_resolution_from_dataset(dataset: String) throws -> (Int, Int) {\n",
        "  if let resolution = known_dataset_sizes[dataset] {\n",
        "    return get_resolution(original_resolution: resolution)\n",
        "  }\n",
        "  print(\"Unsupported dataset \" + dataset + \". Add your own here :)\")\n",
        "  throw ValueError.invalidInput(dataset)\n",
        "\n",
        "}\n",
        "\n",
        "func get_mixup(dataset_size: Int) -> Double {\n",
        "  return dataset_size < 20_000 ? 0.0 : 0.1\n",
        "}\n",
        "\n",
        "\n",
        "func get_schedule(dataset_size: Int) -> Array<Int> {\n",
        "  if dataset_size < 20_000{\n",
        "    return [100, 200, 300, 400, 500]\n",
        "  }\n",
        "  else if dataset_size < 500_000 {\n",
        "    return [500, 3000, 6000, 9000, 10_000]\n",
        "  }\n",
        "  else {\n",
        "    return [500, 6000, 12_000, 18_000, 20_000]\n",
        "  }\n",
        "}\n",
        "\n",
        "\n",
        "func get_lr(step: Int, dataset_size: Int, base_lr: Float = 0.003) -> Float? {\n",
        "  /* Returns learning-rate for `step` or nil at the end. */\n",
        "  let supports = get_schedule(dataset_size: dataset_size)\n",
        "  // Linear warmup\n",
        "  if step < supports[0] {\n",
        "    return base_lr * Float(step) / Float(supports[0])\n",
        "  }\n",
        "  // End of training\n",
        "  else if step >= supports.last! {\n",
        "    return nil\n",
        "  }\n",
        "  // Staircase decays by factor of 10\n",
        "  else {\n",
        "    var base_lr = base_lr\n",
        "    for s in supports[1...] {\n",
        "      if s < step {\n",
        "        base_lr = base_lr / 10.0\n",
        "      }\n",
        "    }\n",
        "    return base_lr\n",
        "  }\n",
        "}"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Er7kXeINnwuW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "900f02a0-6c45-41bc-8cbd-ac03d54efd32"
      },
      "source": [
        "get_lr(step: 7000, dataset_size: 50000, base_lr: 0.01)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Use `print()` to show values.\n"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RFmZWY2q-xzd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "20d6c042-7778-40c1-c354-8ff1f17bc08b"
      },
      "source": [
        "// Build model with weights\n",
        "import FileManager\n",
        "import PythonKit\n",
        "let tf = Python.import(\"tensorflow\")\n",
        "_ = Python.builtins\n",
        "\n",
        "var known_models = [String: String]()\n",
        "// var resnetv2 = ResNetV2(classCount: 1000, depth: .resNet50)\n",
        "\n",
        "let model_name = \"BiT-M-R50x1\"\n",
        "\n",
        "struct Weights {\n",
        "    let name: String\n",
        "    let layer: Tensor<Float>\n",
        "}\n",
        "\n",
        "func get_pretrained_weights_dict(model_name: String) -> Array<Weights> {\n",
        "  let valid_types = [\"BiT-S\", \"BiT-M\"]\n",
        "  let valid_sizes = [(50, 1), (50, 3), (101, 1), (101, 3), (152, 4)]\n",
        "  let bit_url = \"gs://bit_models/\"\n",
        "\n",
        "  for types in valid_types {\n",
        "    for sizes in valid_sizes {\n",
        "      let model_string = types + \"-R\" + String(sizes.0) + \"x\" + String(sizes.1)\n",
        "      known_models[model_string] = bit_url + model_string + \".npz\"\n",
        "    }\n",
        "  }\n",
        "\n",
        "  var f = Python.None\n",
        "  \n",
        "  if let model_path = known_models[model_name] {\n",
        "    subprocess.call(\"gsutil cp \" + model_path + \" .\", shell: true)\n",
        "  }\n",
        "\n",
        "  let weights = np.load(\"./\" + model_name + \".npz\")\n",
        "\n",
        "  var weights_array = Array<Weights>()\n",
        "  for param in weights {\n",
        "      weights_array.append(Weights(name: String(param)!, layer: Tensor<Float>(numpy: weights[param])!))\n",
        "  }\n",
        "  return weights_array\n",
        "}\n",
        "var weights_array = get_pretrained_weights_dict(model_name: model_name)\n",
        "// Get convolution weights"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Copying gs://bit_models/BiT-M-R50x1.npz...\n",
            "- [1 files][260.4 MiB/260.4 MiB]                                                \n",
            "Operation completed over 1 objects/260.4 MiB.                                    \n",
            "2020-11-24 23:31:36.525519: W tensorflow/core/framework/cpu_allocator_impl.cc:81] Allocation of 178937856 exceeds 10% of free system memory.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hwWf0PEYMA_N"
      },
      "source": [
        "import TensorFlow\n",
        "func getPaddingDimensionsFromKernelSize(kernelSize: Int) -> (Int, Int) {\n",
        "  let padTotal = kernelSize - 1\n",
        "  let padBeginning = Int(padTotal / 2)\n",
        "  let padEnd = padTotal - padBeginning\n",
        "  return (kernelSize + padBeginning, kernelSize + padEnd)\n",
        "}\n",
        "\n",
        "func paddingFromKernelSize(kernelSize: Int) -> [(before: Int, after: Int)] {\n",
        "  let padTotal = kernelSize - 1\n",
        "  let padBeginning = Int(padTotal / 2)\n",
        "  let padEnd = padTotal - padBeginning\n",
        "  let padding = [\n",
        "        (before: 0, after: 0),\n",
        "        (before: padBeginning, after: padEnd),\n",
        "        (before: padBeginning, after: padEnd),\n",
        "        (before: 0, after: 0)]\n",
        "  return padding\n",
        "}\n",
        "\n",
        "public struct StandardizedConv2D: Layer {\n",
        "  public var conv: Conv2D<Float>\n",
        "\n",
        "  public init(\n",
        "    filterShape: (Int, Int, Int, Int),\n",
        "    strides: (Int, Int) = (1, 1),\n",
        "    padding: Padding = .valid,\n",
        "    useBias: Bool = true\n",
        "  )\n",
        "  {\n",
        "  self.conv = Conv2D(\n",
        "      filterShape: filterShape, \n",
        "      strides: strides, \n",
        "      padding: padding,\n",
        "      useBias: useBias)\n",
        "  }\n",
        "\n",
        "  @differentiable\n",
        "  public func callAsFunction(_ input: Tensor<Float>) -> Tensor<Float> {\n",
        "      let axes: Array<Int> = [0, 1, 2]\n",
        "      var standardizedConv = conv\n",
        "      standardizedConv.filter = (standardizedConv.filter - standardizedConv.filter.mean(squeezingAxes: axes)) / sqrt((standardizedConv.filter.variance(squeezingAxes: axes) + 1e-16))\n",
        "      return standardizedConv(input)\n",
        "  }\n",
        "\n",
        "}\n",
        "\n",
        "\n",
        "public struct ConvGNV2: Layer {\n",
        "    public var conv: StandardizedConv2D\n",
        "    public var norm: GroupNorm<Float>\n",
        "    @noDerivative public var isSecond: Bool\n",
        "\n",
        "    public init(\n",
        "        inFilters: Int,\n",
        "        outFilters: Int,\n",
        "        kernelSize: Int = 1,\n",
        "        stride: Int = 1,\n",
        "        padding: Padding = .valid,\n",
        "        isSecond: Bool = false\n",
        "    ) {\n",
        "        self.conv = StandardizedConv2D(\n",
        "            filterShape: (kernelSize, kernelSize, inFilters, outFilters), \n",
        "            strides: (stride, stride), \n",
        "            padding: padding,\n",
        "            useBias: false)\n",
        "        self.norm = GroupNorm<Float>(\n",
        "              offset: Tensor(zeros: [inFilters]),\n",
        "              scale: Tensor(zeros: [inFilters]),\n",
        "              groupCount: 2,\n",
        "              axis: -1,\n",
        "              epsilon: 0.001)\n",
        "        self.isSecond = isSecond\n",
        "    }\n",
        "\n",
        "    @differentiable\n",
        "    public func callAsFunction(_ input: Tensor<Float>) -> Tensor<Float> {\n",
        "        var normResult = norm(input)\n",
        "        if self.isSecond {\n",
        "            normResult = normResult.padded(forSizes: paddingFromKernelSize(kernelSize: 3))\n",
        "        }\n",
        "        let reluResult = relu(normResult)\n",
        "        let convResult = conv(reluResult)\n",
        "        return convResult\n",
        "    }\n",
        "}\n",
        "public struct ShortcutBiT: Layer {\n",
        "    public var projection: StandardizedConv2D\n",
        "    public var norm: GroupNorm<Float>\n",
        "    @noDerivative public let needsProjection: Bool\n",
        "    \n",
        "    public init(inFilters: Int, outFilters: Int, stride: Int) {\n",
        "      needsProjection = (stride > 1 || inFilters != outFilters)\n",
        "      norm = GroupNorm<Float>(\n",
        "          offset: Tensor(zeros: [needsProjection ? inFilters  : 1]),\n",
        "          scale: Tensor(zeros: [needsProjection ? inFilters  : 1]),\n",
        "          groupCount: needsProjection ? 2  : 1,\n",
        "          axis: -1,\n",
        "          epsilon: 0.001)\n",
        "        \n",
        "        projection =  StandardizedConv2D(\n",
        "            filterShape: (1, 1, needsProjection ? inFilters  : 1, needsProjection ? outFilters : 1), \n",
        "            strides: (stride, stride), \n",
        "            padding: .valid,\n",
        "            useBias: false)\n",
        "    }\n",
        "    \n",
        "    @differentiable\n",
        "    public func callAsFunction(_ input: Tensor<Float>) -> Tensor<Float> {\n",
        "        var res = input\n",
        "        if needsProjection { \n",
        "          res = norm(res)\n",
        "          res = relu(res)\n",
        "          res = projection(res)\n",
        "        }\n",
        "        return res\n",
        "    }\n",
        "}\n",
        "\n",
        "\n",
        "\n",
        "public struct ResidualBlockBiT: Layer {\n",
        "    public var shortcut: ShortcutBiT\n",
        "    public var convs: [ConvGNV2]\n",
        "\n",
        "    public init(inFilters: Int, outFilters: Int, stride: Int, expansion: Int){\n",
        "        if expansion == 1 {\n",
        "            convs = [\n",
        "                ConvGNV2(inFilters: inFilters,  outFilters: outFilters, kernelSize: 3, stride: stride),\n",
        "                ConvGNV2(inFilters: outFilters, outFilters: outFilters, kernelSize: 3, isSecond: true)\n",
        "            ]\n",
        "        } else {\n",
        "            convs = [\n",
        "                ConvGNV2(inFilters: inFilters,    outFilters: outFilters/4),\n",
        "                ConvGNV2(inFilters: outFilters/4, outFilters: outFilters/4, kernelSize: 3, stride: stride, isSecond: true),\n",
        "                ConvGNV2(inFilters: outFilters/4, outFilters: outFilters)\n",
        "            ]\n",
        "        }\n",
        "        shortcut = ShortcutBiT(inFilters: inFilters, outFilters: outFilters, stride: stride)\n",
        "    }\n",
        "\n",
        "    @differentiable\n",
        "    public func callAsFunction(_ input: Tensor<Float>) -> Tensor<Float> {\n",
        "        let convResult = convs.differentiableReduce(input) { $1($0) }\n",
        "        return convResult + shortcut(input)\n",
        "    }\n",
        "}\n",
        "\n",
        "public struct BigTransfer: Layer {\n",
        "  public var inputStem: StandardizedConv2D\n",
        "  public var maxPool: MaxPool2D<Float>\n",
        "  public var residualBlocks: [ResidualBlockBiT] = []\n",
        "  public var groupNorm : GroupNorm<Float>\n",
        "  public var flatten = Flatten<Float>()\n",
        "  public var classifier: Dense<Float>\n",
        "  public var avgPool = GlobalAvgPool2D<Float>()\n",
        "  @noDerivative public var finalOutFilter : Int = 0\n",
        "\n",
        "  public init(\n",
        "        classCount: Int, \n",
        "        depth: Depth, \n",
        "        inputChannels: Int = 3\n",
        "    ) {\n",
        "\n",
        "        self.inputStem = StandardizedConv2D(filterShape: (7, 7, 3, 64), strides: (2, 2), padding: .valid, useBias: false)\n",
        "        self.maxPool = MaxPool2D(poolSize: (3, 3), strides: (2, 2), padding: .valid)\n",
        "        let sizes = [64 / depth.expansion, 64, 128, 256, 512]\n",
        "        for (iBlock, nBlocks) in depth.layerBlockSizes.enumerated() {\n",
        "            let (nIn, nOut) = (sizes[iBlock] * depth.expansion, sizes[iBlock+1] * depth.expansion)\n",
        "            for j in 0..<nBlocks {\n",
        "\n",
        "                self.residualBlocks.append(ResidualBlockBiT(\n",
        "                    inFilters: j==0 ? nIn : nOut,  \n",
        "                    outFilters: nOut, \n",
        "                    stride: (iBlock != 0) && (j == 0) ? 2 : 1, \n",
        "                    expansion: depth.expansion\n",
        "                ))\n",
        "                self.finalOutFilter = nOut\n",
        "            }\n",
        "        }\n",
        "        self.groupNorm = GroupNorm<Float>(\n",
        "              offset: Tensor(zeros: [self.finalOutFilter]),\n",
        "              scale: Tensor(zeros: [self.finalOutFilter]),\n",
        "              groupCount: 2,\n",
        "              axis: -1,\n",
        "              epsilon: 0.001)\n",
        "        self.classifier = Dense(inputSize: 512 * depth.expansion, outputSize: classCount)\n",
        "    }\n",
        "\n",
        "  @differentiable\n",
        "  public func callAsFunction(_ input: Tensor<Float>) -> Tensor<Float> {\n",
        "      var paddedInput = input.padded(forSizes: paddingFromKernelSize(kernelSize: 7))\n",
        "      paddedInput = inputStem(paddedInput).padded(forSizes: paddingFromKernelSize(kernelSize: 3))\n",
        "      let inputLayer = maxPool(paddedInput)\n",
        "      let blocksReduced = residualBlocks.differentiableReduce(inputLayer) { $1($0) }\n",
        "      let normalized = relu(groupNorm(blocksReduced))\n",
        "      return normalized.sequenced(through: avgPool, flatten, classifier)\n",
        "  }\n",
        "}\n",
        "\n",
        "extension BigTransfer {\n",
        "    public enum Depth {\n",
        "        case resNet18\n",
        "        case resNet34\n",
        "        case resNet50\n",
        "        case resNet101\n",
        "        case resNet152\n",
        "\n",
        "        var expansion: Int {\n",
        "            switch self {\n",
        "            case .resNet18, .resNet34: return 1\n",
        "            default: return 4\n",
        "            }\n",
        "        }\n",
        "\n",
        "        var layerBlockSizes: [Int] {\n",
        "            switch self {\n",
        "            case .resNet18:  return [2, 2, 2,  2]\n",
        "            case .resNet34:  return [3, 4, 6,  3]\n",
        "            case .resNet50:  return [3, 4, 6,  3]\n",
        "            case .resNet101: return [3, 4, 23, 3]\n",
        "            case .resNet152: return [3, 8, 36, 3]\n",
        "            }\n",
        "        }\n",
        "    }\n",
        "}"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TpdLHcEOpGeU"
      },
      "source": [
        "func get_model_units(model_name: String) -> BigTransfer.Depth {\n",
        "  if model_name.contains(\"R50\") {\n",
        "    return .resNet50\n",
        "  }\n",
        "  else if model_name.contains(\"R101\") {\n",
        "    return .resNet101\n",
        "  }\n",
        "  else {\n",
        "    return .resNet152\n",
        "  }\n",
        "}\n",
        "\n",
        "let bit_type = get_model_units(model_name: model_name)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o-MYC9LUr4HM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f5c64364-4278-48f2-f0aa-4f87d235a7e5"
      },
      "source": [
        "var resnetv2 = BigTransfer(classCount: 100, depth: .resNet50)\n",
        "\n",
        "let convs = weights_array.filter {key in return key.name.contains(\"/block\") && key.name.contains(\"standardized_conv2d/kernel\") && !(key.name.contains(\"proj\"))}\n",
        "\n",
        "var k = 0\n",
        "for (idx, i) in resnetv2.residualBlocks.enumerated() {\n",
        "  for (jdx, _) in i.convs.enumerated() {\n",
        "    assert(resnetv2.residualBlocks[idx].convs[jdx].conv.conv.filter.shape == convs[k].layer.shape)\n",
        "    resnetv2.residualBlocks[idx].convs[jdx].conv.conv.filter = convs[k].layer\n",
        "    k = k + 1\n",
        "  }\n",
        "}\n",
        "print(convs.count, k)\n",
        "// https://github.com/zaidalyafeai/Swift4TF/blob/master/Swift4TF_TransferLearning.ipynb "
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2020-11-24 23:31:50.079689: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN)to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX AVX2 FMA\r\n",
            "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\r\n",
            "2020-11-24 23:31:50.095250: I tensorflow/core/platform/profile_utils/cpu_utils.cc:104] CPU Frequency: 2200000000 Hz\r\n",
            "2020-11-24 23:31:50.095595: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x2c921a0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\r\n",
            "2020-11-24 23:31:50.095628: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\r\n",
            "2020-11-24 23:31:50.122360: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcuda.so.1\r\n",
            "2020-11-24 23:31:50.128590: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n",
            "2020-11-24 23:31:50.129056: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1716] Found device 0 with properties: \r\n",
            "pciBusID: 0000:00:04.0 name: Tesla T4 computeCapability: 7.5\r\n",
            "coreClock: 1.59GHz coreCount: 40 deviceMemorySize: 14.73GiB deviceMemoryBandwidth: 298.08GiB/s\r\n",
            "2020-11-24 23:31:50.129159: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\n",
            "2020-11-24 23:31:50.437219: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcublas.so.10\n",
            "2020-11-24 23:31:50.717082: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcufft.so.10\n",
            "2020-11-24 23:31:50.832214: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcurand.so.10\n",
            "2020-11-24 23:31:51.192462: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusolver.so.10\n",
            "2020-11-24 23:31:51.221751: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusparse.so.10\n",
            "2020-11-24 23:31:52.064741: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudnn.so.7\n",
            "2020-11-24 23:31:52.064995: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-11-24 23:31:52.065754: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-11-24 23:31:52.066306: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1858] Adding visible gpu devices: 0\n",
            "2020-11-24 23:31:53.118864: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1257] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
            "2020-11-24 23:31:53.118929: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1263]      0 \n",
            "2020-11-24 23:31:53.118944: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1276] 0:   N \n",
            "2020-11-24 23:31:53.119177: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-11-24 23:31:53.119699: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-11-24 23:31:53.120157: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-11-24 23:31:53.120531: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1402] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 6951 MB memory) -> physical GPU (device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5)\n",
            "2020-11-24 23:31:53.122770: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x323b2790 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
            "2020-11-24 23:31:53.122802: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Tesla T4, Compute Capability 7.5\n",
            "48 48\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PtrRmtXMHLU6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6771db89-f6a7-4dc2-cda8-66bc13f9f660"
      },
      "source": [
        "// Set weights for all projective convolutions\n",
        "let projective_convs = weights_array.filter {key in return key.name.contains(\"/block\") && key.name.contains(\"standardized_conv2d/kernel\") && (key.name.contains(\"proj\"))}\n",
        "let norm_scale = weights_array.filter {key in return key.name.contains(\"unit01/a/group_norm/gamma\")}\n",
        "let norm_offset = weights_array.filter {key in return key.name.contains(\"unit01/a/group_norm/beta\")}\n",
        "\n",
        "var k = 0\n",
        "for (idx, i) in resnetv2.residualBlocks.enumerated() {\n",
        "    if (i.shortcut.projection.conv.filter.shape != [1, 1, 1, 1])\n",
        "    {\n",
        "      assert(resnetv2.residualBlocks[idx].shortcut.projection.conv.filter.shape == projective_convs[k].layer.shape)\n",
        "      resnetv2.residualBlocks[idx].shortcut.projection.conv.filter = projective_convs[k].layer\n",
        "\n",
        "      assert(resnetv2.residualBlocks[idx].shortcut.norm.scale.shape == norm_scale[k].layer.shape)\n",
        "      resnetv2.residualBlocks[idx].shortcut.norm.scale = norm_scale[k].layer\n",
        "\n",
        "      assert(resnetv2.residualBlocks[idx].shortcut.norm.offset.shape == norm_offset[k].layer.shape)\n",
        "      resnetv2.residualBlocks[idx].shortcut.norm.offset = norm_offset[k].layer\n",
        "      k = k + 1\n",
        "    }\n",
        "}\n",
        "print(projective_convs.count, k)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "4 4\r\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hu7mrFlh7j0P"
      },
      "source": [
        "// Set weights for all standard norms\n",
        "let norm_scale = weights_array.filter {key in return key.name.contains(\"gamma\")}\n",
        "\n",
        "var k = 0\n",
        "for (idx, i) in resnetv2.residualBlocks.enumerated() {\n",
        "  for (jdx, _) in i.convs.enumerated() {\n",
        "    assert(norm_scale[k].layer.shape == resnetv2.residualBlocks[idx].convs[jdx].norm.scale.shape)\n",
        "    resnetv2.residualBlocks[idx].convs[jdx].norm.scale = norm_scale[k].layer\n",
        "    k = k + 1\n",
        "  }\n",
        "}\n",
        "\n",
        "let norm_offset = weights_array.filter {key in return key.name.contains(\"beta\")}\n",
        "\n",
        "var l = 0\n",
        "for (idx, i) in resnetv2.residualBlocks.enumerated() {\n",
        "  for (jdx, _) in i.convs.enumerated() {\n",
        "    assert(norm_offset[l].layer.shape == resnetv2.residualBlocks[idx].convs[jdx].norm.offset.shape)\n",
        "    resnetv2.residualBlocks[idx].convs[jdx].norm.offset = norm_offset[l].layer\n",
        "    l = l + 1\n",
        "  }\n",
        "}\n",
        "\n",
        "assert(resnetv2.groupNorm.scale.shape == norm_scale[k].layer.shape)\n",
        "resnetv2.groupNorm.scale = norm_scale[k].layer\n",
        "assert(resnetv2.groupNorm.offset.shape == norm_offset[l].layer.shape)\n",
        "resnetv2.groupNorm.offset = norm_offset[l].layer"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XSGpiMF-Fcxj"
      },
      "source": [
        "\n"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a62-npIMD24Y"
      },
      "source": [
        "// Set weight for input block\n",
        "let root_convs = weights_array.filter {key in return key.name.contains(\"root_block\")}\n",
        "assert(resnetv2.inputStem.conv.filter.shape == root_convs[0].layer.shape)\n",
        "resnetv2.inputStem.conv.filter = root_convs[0].layer"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HZWyUKAxEHqt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0a46b32c-255f-4a2a-8954-6541f2480eb4"
      },
      "source": [
        "let norm_scale = weights_array.filter {key in return key.name.contains(\"unit01/a/group_norm/gamma\")}\n",
        "print(norm_scale.count, projective_convs.count)\n",
        "for i in projective_convs {\n",
        "  print(i.name)\n",
        "}\n",
        "\n"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "4 4\r\n",
            "resnet/block1/unit01/a/proj/standardized_conv2d/kernel\r\n",
            "resnet/block2/unit01/a/proj/standardized_conv2d/kernel\r\n",
            "resnet/block3/unit01/a/proj/standardized_conv2d/kernel\r\n",
            "resnet/block4/unit01/a/proj/standardized_conv2d/kernel\r\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TYKUEjZTQx18",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9996973c-4036-4658-abd0-ed6661e0626d"
      },
      "source": [
        "let norm_scale = weights_array.filter {key in return key.name.contains(\"dense\")}\n",
        "norm_scale"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Use `print()` to show values.\n"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "51749rK4sSFS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "350f2136-c9c2-46ae-f84d-4e421d5f7a9b"
      },
      "source": [
        "let _ = _ExecutionContext.global\n",
        "\n",
        "let device = Device.defaultXLA\n",
        "device"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2020-11-24 23:31:55.293194: I tensorflow/compiler/xla/xla_client/xrt_local_service.cc:54] Peer localservice 1 {localhost:35115}\r\n",
            "2020-11-24 23:31:55.293539: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1257] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n",
            "2020-11-24 23:31:55.293569: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1263]      \r\n",
            "2020-11-24 23:31:55.304877: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:301] Initialize GrpcChannelCache for job localservice -> {0 -> localhost:35115}\r\n",
            "2020-11-24 23:31:55.306089: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:405] Started server with target: grpc://localhost:35115\r\n",
            "2020-11-24 23:31:55.306652: I tensorflow/compiler/xla/xla_client/computation_client.cc:202] NAME: CPU:0\r\n",
            "2020-11-24 23:31:55.306711: I tensorflow/compiler/xla/xla_client/computation_client.cc:202] NAME: GPU:0\r\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Use `print()` to show values.\n"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VmjRiIx9FST3"
      },
      "source": [
        "// Copyright 2019 The TensorFlow Authors. All Rights Reserved.\n",
        "//\n",
        "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "// you may not use this file except in compliance with the License.\n",
        "// You may obtain a copy of the License at\n",
        "//\n",
        "//     http://www.apache.org/licenses/LICENSE-2.0\n",
        "//\n",
        "// Unless required by applicable law or agreed to in writing, software\n",
        "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "// See the License for the specific language governing permissions and\n",
        "// limitations under the License.\n",
        "\n",
        "// Original source:\n",
        "// \"The CIFAR-10 dataset\"\n",
        "// Alex Krizhevsky, Vinod Nair, and Geoffrey Hinton.\n",
        "// https://www.cs.toronto.edu/~kriz/cifar.html\n",
        "\n",
        "import Foundation\n",
        "import ModelSupport\n",
        "import TensorFlow\n",
        "import Batcher\n",
        "\n",
        "public struct CIFAR100<Entropy: RandomNumberGenerator> {\n",
        "  /// Type of the collection of non-collated batches.\n",
        "  public typealias Batches = Slices<Sampling<[(data: [UInt8], label: Int32)], ArraySlice<Int>>>\n",
        "  /// The type of the training data, represented as a sequence of epochs, which\n",
        "  /// are collection of batches.\n",
        "  public typealias Training = LazyMapSequence<\n",
        "    TrainingEpochs<[(data: [UInt8], label: Int32)], Entropy>,\n",
        "    LazyMapSequence<Batches, LabeledImage>\n",
        "  >\n",
        "  /// The type of the validation data, represented as a collection of batches.\n",
        "  public typealias Validation = LazyMapSequence<Slices<[(data: [UInt8], label: Int32)]>, LabeledImage>\n",
        "  /// The training epochs.\n",
        "  public let training: Training\n",
        "  /// The validation batches.\n",
        "  public let validation: Validation\n",
        "\n",
        "  /// Creates an instance with `batchSize`.\n",
        "  ///\n",
        "  /// - Parameter entropy: a source of randomness used to shuffle sample \n",
        "  ///   ordering.  It  will be stored in `self`, so if it is only pseudorandom \n",
        "  ///   and has value semantics, the sequence of epochs is deterministic and not \n",
        "  ///   dependent on other operations.\n",
        "  public init(batchSize: Int, entropy: Entropy) {\n",
        "    self.init(\n",
        "      batchSize: batchSize,\n",
        "      entropy: entropy,\n",
        "      device: Device.default,\n",
        "      remoteBinaryArchiveLocation: URL(\n",
        "        string: \"https://www.cs.toronto.edu/~kriz/cifar-100-binary.tar.gz\")!, \n",
        "      normalizing: true)\n",
        "  }\n",
        "  \n",
        "  /// Creates an instance with `batchSize` on `device` using `remoteBinaryArchiveLocation`.\n",
        "  ///\n",
        "  /// - Parameters:\n",
        "  ///   - entropy: a source of randomness used to shuffle sample ordering.  It  \n",
        "  ///     will be stored in `self`, so if it is only pseudorandom and has value \n",
        "  ///     semantics, the sequence of epochs is deterministic and not dependent \n",
        "  ///     on other operations.\n",
        "  ///   - normalizing: normalizes the batches with the mean and standard deviation\n",
        "  ///     of the dataset iff `true`. Default value is `true`.\n",
        "  public init(\n",
        "    batchSize: Int,\n",
        "    entropy: Entropy,\n",
        "    device: Device,\n",
        "    remoteBinaryArchiveLocation: URL, \n",
        "    localStorageDirectory: URL = DatasetUtilities.defaultDirectory\n",
        "      .appendingPathComponent(\"CIFAR100\", isDirectory: true), \n",
        "    normalizing: Bool\n",
        "  ){\n",
        "    downloadCIFAR100IfNotPresent(from: remoteBinaryArchiveLocation, to: localStorageDirectory)\n",
        "    \n",
        "    // Training data\n",
        "    let trainingSamples = loadCIFARTrainingFiles(in: localStorageDirectory)\n",
        "    training = TrainingEpochs(samples: trainingSamples, batchSize: batchSize, entropy: entropy)\n",
        "      .lazy.map { (batches: Batches) -> LazyMapSequence<Batches, LabeledImage> in\n",
        "        return batches.lazy.map{ makeBatch(samples: $0, normalizing: normalizing, device: device) }\n",
        "      }\n",
        "      \n",
        "    // Validation data\n",
        "    let validationSamples = loadCIFARTestFile(in: localStorageDirectory)\n",
        "    validation = validationSamples.inBatches(of: batchSize).lazy.map {\n",
        "      makeBatch(samples: $0, normalizing: normalizing, device: device)\n",
        "    }\n",
        "  }\n",
        "}\n",
        "\n",
        "extension CIFAR100: ImageClassificationData where Entropy == SystemRandomNumberGenerator {\n",
        "  /// Creates an instance with `batchSize`.\n",
        "  public init(batchSize: Int, on: Device) {\n",
        "    self.init(batchSize: batchSize, entropy: SystemRandomNumberGenerator())\n",
        "  }\n",
        "}\n",
        "\n",
        "func downloadCIFAR100IfNotPresent(from location: URL, to directory: URL) {\n",
        "  let downloadPath = directory.path\n",
        "  let directoryExists = FileManager.default.fileExists(atPath: downloadPath)\n",
        "  let contentsOfDir = try? FileManager.default.contentsOfDirectory(atPath: downloadPath)\n",
        "  let directoryEmpty = (contentsOfDir == nil) || (contentsOfDir!.isEmpty)\n",
        "\n",
        "  guard !directoryExists || directoryEmpty else { return }\n",
        "\n",
        "  let _ = DatasetUtilities.downloadResource(\n",
        "    filename: \"cifar-100-binary\", fileExtension: \"tar.gz\",\n",
        "    remoteRoot: location.deletingLastPathComponent(), localStorageDirectory: directory)\n",
        "}\n",
        "\n",
        "func loadCIFARFile(named name: String, in directory: URL) -> [(data: [UInt8], label: Int32)] {\n",
        "  let path = directory.appendingPathComponent(\"cifar-100-binary/\\(name)\").path\n",
        "\n",
        "\n",
        "  var imageCount = 50000\n",
        "  guard let fileContents = try? Data(contentsOf: URL(fileURLWithPath: path)) else {\n",
        "    printError(\"Could not read dataset file: \\(name)\")\n",
        "    exit(-1)\n",
        "  }\n",
        "  if name.contains(\"test\") {\n",
        "      guard fileContents.count == 307_400_00 else {\n",
        "      printError(\n",
        "        \"Dataset file \\(name) should have 307_400_00 bytes, instead had \\(fileContents.count)\")\n",
        "      exit(-1)\n",
        "    }\n",
        "    imageCount = 10000\n",
        "  }\n",
        "  else {\n",
        "      guard fileContents.count == 153_700_000 else {\n",
        "        printError(\n",
        "          \"Dataset file \\(name) should have 15370000 bytes, instead had \\(fileContents.count)\")\n",
        "        exit(-1)\n",
        "      }\n",
        "  }\n",
        "\n",
        "\n",
        "  var labeledImages: [(data: [UInt8], label: Int32)] = []\n",
        "\n",
        "  let imageByteSize = 3074\n",
        "  for imageIndex in 0..<imageCount {\n",
        "    let baseAddress = imageIndex * imageByteSize\n",
        "    let label = Int32(fileContents[baseAddress + 1])\n",
        "    let data = [UInt8](fileContents[(baseAddress + 2)..<(baseAddress + 3074)])\n",
        "    labeledImages.append((data: data, label: label))\n",
        "  }\n",
        "\n",
        "  return labeledImages\n",
        "}\n",
        "\n",
        "func loadCIFARTrainingFiles(in localStorageDirectory: URL) -> [(data: [UInt8], label: Int32)] {\n",
        "  print(localStorageDirectory.path)\n",
        "  return loadCIFARFile(named: \"train.bin\", in: localStorageDirectory)\n",
        "}\n",
        "\n",
        "func loadCIFARTestFile(in localStorageDirectory: URL) -> [(data: [UInt8], label: Int32)] {\n",
        "  return loadCIFARFile(named: \"test.bin\", in: localStorageDirectory)\n",
        "}\n",
        "\n",
        "fileprivate func makeBatch<BatchSamples: Collection>(\n",
        "  samples: BatchSamples, normalizing: Bool, device: Device\n",
        ") -> LabeledImage where BatchSamples.Element == (data: [UInt8], label: Int32) {\n",
        "  let bytes = samples.lazy.map(\\.data).reduce(into: [], +=)\n",
        "  let images = Tensor<UInt8>(shape: [samples.count, 3, 32, 32], scalars: bytes, on: device)\n",
        "  \n",
        "  var imageTensor = Tensor<Float>(images.transposed(permutation: [0, 2, 3, 1]))\n",
        "  imageTensor /= 255.0\n",
        "  if normalizing {\n",
        "    let mean = Tensor<Float>([0.5071, 0.4867, 0.4408], on: device)\n",
        "    let std = Tensor<Float>([0.2675, 0.2565, 0.2761], on: device)\n",
        "    imageTensor = (imageTensor - mean) / std\n",
        "  }\n",
        "  \n",
        "  let labels = Tensor<Int32>(samples.map(\\.label))\n",
        "  return LabeledImage(data: imageTensor, label: labels)\n",
        "}\n"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IGiHiC-PK6vM",
        "outputId": "f6c5e386-b4ca-4c98-e8a5-4fd4f4707edd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "let dataset = CIFAR100(batchSize: 32, on: Device.default)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/root/.cache/swift-models/datasets/CIFAR100\r\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "17K06oETPq_P",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "838e47db-f79d-47f5-8a28-b652e3add232"
      },
      "source": [
        "resnetv2.move(to: device)\n",
        "var optimizer = SGD(for: resnetv2, learningRate: 0.001, momentum: 0.9)\n",
        "optimizer = SGD(copying: optimizer, to: device)\n",
        "\n",
        "let cifar10_training_size = 50000\n",
        "\n",
        "Context.local.learningPhase = .training\n",
        "let batchSize = 128\n",
        "let lrSupports = get_schedule(dataset_size: cifar10_training_size)\n",
        "let scheduleLength = lrSupports.last!\n",
        "let stepsPerEpoch = cifar10_training_size / batchSize\n",
        "\n",
        "\n",
        "var epochCount = scheduleLength / stepsPerEpoch\n",
        "let dataset = CIFAR100(batchSize: batchSize, on: Device.default)\n",
        "\n",
        "print(stepsPerEpoch)\n",
        "print(epochCount)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/root/.cache/swift-models/datasets/CIFAR100\n",
            "390\n",
            "25\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KHxauxK0OFCN"
      },
      "source": [
        "struct Statistics {\n",
        "    var correctGuessCount = Tensor<Int32>(0, on: Device.default)\n",
        "    var totalGuessCount = Tensor<Int32>(0, on: Device.default)\n",
        "    var totalLoss = Tensor<Float>(0, on: Device.default)\n",
        "    var batches: Int = 0\n",
        "    var accuracy: Float { \n",
        "        Float(correctGuessCount.scalarized()) / Float(totalGuessCount.scalarized()) * 100 \n",
        "    } \n",
        "    var averageLoss: Float { totalLoss.scalarized() / Float(batches) }\n",
        "\n",
        "    init(on device: Device = Device.default) {\n",
        "        correctGuessCount = Tensor<Int32>(0, on: device)\n",
        "        totalGuessCount = Tensor<Int32>(0, on: device)\n",
        "        totalLoss = Tensor<Float>(0, on: device)\n",
        "    }\n",
        "\n",
        "    mutating func update(logits: Tensor<Float>, labels: Tensor<Float>, loss: Tensor<Float>) {\n",
        "        let correct = logits.argmax(squeezingAxis: 1) .== labels.argmax(squeezingAxis: 1)\n",
        "        correctGuessCount += Tensor<Int32>(correct).sum()\n",
        "        totalGuessCount += Int32(labels.shape[0])\n",
        "        totalLoss += loss\n",
        "        batches += 1\n",
        "    }\n",
        "}\n",
        "\n",
        "\n"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jMKT1a_pa8ut"
      },
      "source": [
        ""
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wzYP191ERHI8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b6561000-2d30-4611-b562-c377573ae4d7"
      },
      "source": [
        "print(\"Beginning training...\")\n",
        "var curr_step: Int = 1\n",
        "let resize_size = get_resolution(original_resolution: (32, 32))\n",
        "var mixup_alpha = get_mixup(dataset_size: cifar10_training_size)\n",
        "let beta = np.random.beta(mixup_alpha, mixup_alpha)\n",
        "\n",
        "for (epoch, batches) in dataset.training.prefix(epochCount).enumerated() {\n",
        "    let start = Date()\n",
        "    var trainStats = Statistics(on: device)\n",
        "    var testStats = Statistics(on: device)\n",
        "    \n",
        "    Context.local.learningPhase = .training\n",
        "    for batch in batches {\n",
        "        if let new_lr = get_lr(step: curr_step, dataset_size: cifar10_training_size, base_lr: 0.003) {\n",
        "          optimizer.learningRate = new_lr\n",
        "          curr_step = curr_step + 1\n",
        "        }\n",
        "        else {\n",
        "          continue\n",
        "        }\n",
        "\n",
        "        var (eagerImages, eagerLabels) = (batch.data, batch.label)\n",
        "        var npImages = eagerImages.makeNumpyArray()\n",
        "        var resized = tf.image.resize(npImages, [resize_size.0, resize_size.0])\n",
        "        var cropped = tf.image.random_crop(resized, [batchSize, resize_size.1, resize_size.1, 3])\n",
        "        var flipped = tf.image.random_flip_left_right(cropped)\n",
        "        var mixed_up = flipped\n",
        "        var newLabels = Tensor<Float>(Tensor<Int32>(oneHotAtIndices: eagerLabels, depth: 100))\n",
        "        if mixup_alpha > 0.0 {\n",
        "          var npLabels = newLabels.makeNumpyArray()\n",
        "          mixed_up = beta * mixed_up + (1 - beta) * tf.reverse(mixed_up, axis: [0])\n",
        "          npLabels = beta * npLabels + (1 - beta) * tf.reverse(npLabels, axis: [0])\n",
        "          newLabels = Tensor<Float>(numpy: npLabels.numpy())!\n",
        "        }\n",
        "        eagerImages = Tensor<Float>(numpy: mixed_up.numpy())!\n",
        "        let images = Tensor(copying: eagerImages, to: device)\n",
        "        let labels = Tensor(copying: newLabels, to: device)\n",
        "\n",
        "        let 𝛁model = TensorFlow.gradient(at: resnetv2) { resnetv2 -> Tensor<Float> in\n",
        "            let ŷ = resnetv2(images)\n",
        "            let loss = softmaxCrossEntropy(logits: ŷ, probabilities: labels)\n",
        "            trainStats.update(logits: ŷ, labels: labels, loss: loss)\n",
        "            return loss\n",
        "        }\n",
        "\n",
        "        \n",
        "        optimizer.update(&resnetv2, along: 𝛁model)\n",
        "        LazyTensorBarrier()\n",
        "    }\n",
        "\n",
        "    Context.local.learningPhase = .inference\n",
        "    for batch in dataset.validation {\n",
        "        var (eagerImages, eagerLabels) = (batch.data, batch.label)\n",
        "        var npImages = eagerImages.makeNumpyArray()\n",
        "        var resized = tf.image.resize(npImages, [resize_size.0, resize_size.0])\n",
        "        eagerImages = Tensor<Float>(numpy: resized.numpy())!\n",
        "        var newLabels = Tensor<Float>(Tensor<Int32>(oneHotAtIndices: eagerLabels, depth: 100))\n",
        "        let images = Tensor(copying: eagerImages, to: device)\n",
        "        let labels = Tensor(copying: newLabels, to: device)\n",
        "        let ŷ = resnetv2(images)\n",
        "        let loss = softmaxCrossEntropy(logits: ŷ, probabilities: labels)\n",
        "        LazyTensorBarrier()\n",
        "        testStats.update(logits: ŷ, labels: labels, loss: loss)\n",
        "    }\n",
        "\n",
        "    print(\n",
        "        \"\"\"\n",
        "        [Epoch \\(epoch)] \\\n",
        "        Training Loss: \\(String(format: \"%.3f\", trainStats.averageLoss)), \\\n",
        "        Training Accuracy: \\(trainStats.correctGuessCount)/\\(trainStats.totalGuessCount) \\\n",
        "        (\\(String(format: \"%.1f\", trainStats.accuracy))%), \\\n",
        "        Test Loss: \\(String(format: \"%.3f\", testStats.averageLoss)), \\\n",
        "        Test Accuracy: \\(testStats.correctGuessCount)/\\(testStats.totalGuessCount) \\\n",
        "        (\\(String(format: \"%.1f\", testStats.accuracy))%) \\\n",
        "        seconds per epoch: \\(String(format: \"%.1f\", Date().timeIntervalSince(start)))\n",
        "        \"\"\")\n",
        "}\n"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Beginning training...\n",
            "2020-11-24 23:32:01.619161: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcuda.so.1\n",
            "2020-11-24 23:32:01.619395: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-11-24 23:32:01.619838: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1716] Found device 0 with properties: \n",
            "pciBusID: 0000:00:04.0 name: Tesla T4 computeCapability: 7.5\n",
            "coreClock: 1.59GHz coreCount: 40 deviceMemorySize: 14.73GiB deviceMemoryBandwidth: 298.08GiB/s\n",
            "2020-11-24 23:32:01.619895: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\n",
            "2020-11-24 23:32:01.619970: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcublas.so.10\n",
            "2020-11-24 23:32:01.620004: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcufft.so.10\n",
            "2020-11-24 23:32:01.620038: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcurand.so.10\n",
            "2020-11-24 23:32:01.620068: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusolver.so.10\n",
            "2020-11-24 23:32:01.620098: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusparse.so.10\n",
            "2020-11-24 23:32:01.620130: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudnn.so.7\n",
            "2020-11-24 23:32:01.620234: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-11-24 23:32:01.620678: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-11-24 23:32:01.621077: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1858] Adding visible gpu devices: 0\n",
            "2020-11-24 23:32:01.628794: I tensorflow/core/platform/profile_utils/cpu_utils.cc:104] CPU Frequency: 2200000000 Hz\n",
            "2020-11-24 23:32:01.629139: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x33150880 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
            "2020-11-24 23:32:01.629169: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
            "2020-11-24 23:32:01.630998: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-11-24 23:32:01.631486: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x4ff50ac0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
            "2020-11-24 23:32:01.631513: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Tesla T4, Compute Capability 7.5\n",
            "2020-11-24 23:32:01.631699: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-11-24 23:32:01.632138: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1716] Found device 0 with properties: \n",
            "pciBusID: 0000:00:04.0 name: Tesla T4 computeCapability: 7.5\n",
            "coreClock: 1.59GHz coreCount: 40 deviceMemorySize: 14.73GiB deviceMemoryBandwidth: 298.08GiB/s\n",
            "2020-11-24 23:32:01.632188: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\n",
            "2020-11-24 23:32:01.632228: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcublas.so.10\n",
            "2020-11-24 23:32:01.632257: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcufft.so.10\n",
            "2020-11-24 23:32:01.632285: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcurand.so.10\n",
            "2020-11-24 23:32:01.632311: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusolver.so.10\n",
            "2020-11-24 23:32:01.632336: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusparse.so.10\n",
            "2020-11-24 23:32:01.632362: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudnn.so.7\n",
            "2020-11-24 23:32:01.632436: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-11-24 23:32:01.632875: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-11-24 23:32:01.633248: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1858] Adding visible gpu devices: 0\n",
            "2020-11-24 23:32:01.633304: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\n",
            "2020-11-24 23:32:01.633390: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1257] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
            "2020-11-24 23:32:01.633405: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1263]      0 \n",
            "2020-11-24 23:32:01.633417: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1276] 0:   N \n",
            "2020-11-24 23:32:01.633566: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-11-24 23:32:01.634004: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-11-24 23:32:01.634370: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
            "2020-11-24 23:32:01.634410: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1402] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 6828 MB memory) -> physical GPU (device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5)\n",
            "2020-11-24 23:32:03.145487: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcublas.so.10\n",
            "2020-11-24 23:32:03.370802: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudnn.so.7\n",
            "2020-11-24 23:36:43.603838: W tensorflow/core/framework/cpu_allocator_impl.cc:81] Allocation of 39321600 exceeds 10% of free system memory.\n",
            "2020-11-24 23:36:43.681949: W tensorflow/core/framework/cpu_allocator_impl.cc:81] Allocation of 39321600 exceeds 10% of free system memory.\n",
            "2020-11-24 23:36:53.017684: W tensorflow/core/framework/cpu_allocator_impl.cc:81] Allocation of 39321600 exceeds 10% of free system memory.\n",
            "2020-11-24 23:36:53.135473: W tensorflow/core/framework/cpu_allocator_impl.cc:81] Allocation of 39321600 exceeds 10% of free system memory.\n",
            "2020-11-24 23:36:59.989484: W tensorflow/core/framework/cpu_allocator_impl.cc:81] Allocation of 39321600 exceeds 10% of free system memory.\n",
            "2020-11-24 23:37:00.106061: W tensorflow/core/framework/cpu_allocator_impl.cc:81] Allocation of 39321600 exceeds 10% of free system memory.\n",
            "2020-11-24 23:37:00.350201: W tensorflow/core/framework/cpu_allocator_impl.cc:81] Allocation of 39321600 exceeds 10% of free system memory.\n",
            "2020-11-24 23:37:00.459974: W tensorflow/core/framework/cpu_allocator_impl.cc:81] Allocation of 39321600 exceeds 10% of free system memory.\n",
            "2020-11-24 23:37:00.559962: W tensorflow/core/framework/cpu_allocator_impl.cc:81] Allocation of 39321600 exceeds 10% of free system memory.\n",
            "[Epoch 0] Training Loss: 3.041, Training Accuracy: 17240/49920 (34.5%), Test Loss: 1.421, Test Accuracy: 6069/10000 (60.7%) seconds per epoch: 325.6\n",
            "[Epoch 1] Training Loss: 1.342, Training Accuracy: 34087/49920 (68.3%), Test Loss: 0.991, Test Accuracy: 7235/10000 (72.3%) seconds per epoch: 264.7\n",
            "[Epoch 2] Training Loss: 0.987, Training Accuracy: 39030/49920 (78.2%), Test Loss: 0.803, Test Accuracy: 7733/10000 (77.3%) seconds per epoch: 234.3\n",
            "[Epoch 3] Training Loss: 0.817, Training Accuracy: 41482/49920 (83.1%), Test Loss: 0.765, Test Accuracy: 7848/10000 (78.5%) seconds per epoch: 234.5\n",
            "[Epoch 4] Training Loss: 0.718, Training Accuracy: 43100/49920 (86.3%), Test Loss: 0.730, Test Accuracy: 7983/10000 (79.8%) seconds per epoch: 234.6\n",
            "[Epoch 5] Training Loss: 0.623, Training Accuracy: 44632/49920 (89.4%), Test Loss: 0.706, Test Accuracy: 8077/10000 (80.8%) seconds per epoch: 234.0\n",
            "[Epoch 6] Training Loss: 0.560, Training Accuracy: 45599/49920 (91.3%), Test Loss: 0.688, Test Accuracy: 8132/10000 (81.3%) seconds per epoch: 234.3\n",
            "[Epoch 7] Training Loss: 0.499, Training Accuracy: 46702/49920 (93.6%), Test Loss: 0.618, Test Accuracy: 8332/10000 (83.3%) seconds per epoch: 234.2\n",
            "[Epoch 8] Training Loss: 0.406, Training Accuracy: 48369/49920 (96.9%), Test Loss: 0.592, Test Accuracy: 8429/10000 (84.3%) seconds per epoch: 234.0\n",
            "[Epoch 9] Training Loss: 0.387, Training Accuracy: 48722/49920 (97.6%), Test Loss: 0.585, Test Accuracy: 8444/10000 (84.4%) seconds per epoch: 235.7\n",
            "[Epoch 10] Training Loss: 0.378, Training Accuracy: 48860/49920 (97.9%), Test Loss: 0.582, Test Accuracy: 8465/10000 (84.7%) seconds per epoch: 235.9\n",
            "[Epoch 11] Training Loss: 0.371, Training Accuracy: 49000/49920 (98.2%), Test Loss: 0.576, Test Accuracy: 8480/10000 (84.8%) seconds per epoch: 236.1\n",
            "[Epoch 12] Training Loss: 0.366, Training Accuracy: 49063/49920 (98.3%), Test Loss: 0.580, Test Accuracy: 8481/10000 (84.8%) seconds per epoch: 236.3\n",
            "[Epoch 13] Training Loss: 0.361, Training Accuracy: 49138/49920 (98.4%), Test Loss: 0.576, Test Accuracy: 8488/10000 (84.9%) seconds per epoch: 236.0\n",
            "[Epoch 14] Training Loss: 0.356, Training Accuracy: 49219/49920 (98.6%), Test Loss: 0.571, Test Accuracy: 8505/10000 (85.0%) seconds per epoch: 236.6\n",
            "[Epoch 15] Training Loss: 0.353, Training Accuracy: 49274/49920 (98.7%), Test Loss: 0.575, Test Accuracy: 8515/10000 (85.1%) seconds per epoch: 236.6\n",
            "[Epoch 16] Training Loss: 0.349, Training Accuracy: 49363/49920 (98.9%), Test Loss: 0.574, Test Accuracy: 8518/10000 (85.2%) seconds per epoch: 236.3\n",
            "[Epoch 17] Training Loss: 0.350, Training Accuracy: 49307/49920 (98.8%), Test Loss: 0.573, Test Accuracy: 8517/10000 (85.2%) seconds per epoch: 236.7\n",
            "[Epoch 18] Training Loss: 0.348, Training Accuracy: 49340/49920 (98.8%), Test Loss: 0.574, Test Accuracy: 8517/10000 (85.2%) seconds per epoch: 236.8\n",
            "[Epoch 19] Training Loss: 0.349, Training Accuracy: 49307/49920 (98.8%), Test Loss: 0.574, Test Accuracy: 8518/10000 (85.2%) seconds per epoch: 237.1\n",
            "[Epoch 20] Training Loss: 0.349, Training Accuracy: 49347/49920 (98.9%), Test Loss: 0.574, Test Accuracy: 8517/10000 (85.2%) seconds per epoch: 236.4\n",
            "[Epoch 21] Training Loss: 0.347, Training Accuracy: 49378/49920 (98.9%), Test Loss: 0.574, Test Accuracy: 8513/10000 (85.1%) seconds per epoch: 236.5\n",
            "[Epoch 22] Training Loss: 0.346, Training Accuracy: 49387/49920 (98.9%), Test Loss: 0.574, Test Accuracy: 8518/10000 (85.2%) seconds per epoch: 237.2\n",
            "[Epoch 23] Training Loss: 0.347, Training Accuracy: 49385/49920 (98.9%), Test Loss: 0.574, Test Accuracy: 8513/10000 (85.1%) seconds per epoch: 237.5\n",
            "[Epoch 24] Training Loss: 0.347, Training Accuracy: 49364/49920 (98.9%), Test Loss: 0.574, Test Accuracy: 8513/10000 (85.1%) seconds per epoch: 237.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mOxWs5HlYm9X",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6073c39f-2d3b-48b3-eb5f-8a8ed64d80b4"
      },
      "source": [
        "// Try doing SWA on stable LR\n",
        "\n",
        "\"\"\"\n",
        "On ImageNet we experimented with ResNet-50, ResNet-152[He et al., 2016] and DenseNet-161[Huang et al.,2017].  \n",
        "For these architectures we used pretrained mod-els  fromPyTorch.torchvision.   For  each  of  themodels we ran \n",
        "SWA for10epochs with a cyclical learn-ing rate schedule with the same parameters for all models\n",
        "(the details can be found in the Appendix), \n",
        "and report themean and standard deviation of test error averaged over3runs. The results are shown in Table 2.\n",
        "\"\"\"\n",
        "// Run 10 epochs of cyclical LR at the end here with SWA\n",
        "// https://arxiv.org/pdf/1803.05407.pdf\n",
        "// https://pytorch.org/blog/stochastic-weight-averaging-in-pytorch/\n",
        "// https://github.com/izmailovpavel/contrib_swa_examples/blob/master/train.py"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Use `print()` to show values.\n"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mIpxzm0swMvv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f2863f59-d715-43b9-e12e-aa1d94473b87"
      },
      "source": [
        "func get_cosine_annealing_lr(step: Int, steps_per_epoch: Int, base_lr: Float = 0.003, max_lr: Float = 0.1) -> Float? {\n",
        "    return base_lr + 0.5*(max_lr - base_lr)*(1 + cos(Float.pi * Float(step)/Float(steps_per_epoch)))\n",
        "}\n",
        "get_cosine_annealing_lr(step: 1, steps_per_epoch: 100)"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Use `print()` to show values.\n"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F18FmtFIsxWP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7fb39904-6cbf-49ce-99b8-ab376562fa1e"
      },
      "source": [
        "var swaModelWeights: BigTransfer.TangentVector = BigTransfer.TangentVector.zero\n",
        "swaModelWeights = BigTransfer.TangentVector(copying: swaModelWeights, to: device)\n",
        "\n",
        "var numCycles = 10\n",
        "var mixup_alpha = 0.0\n",
        "for (epoch, batches) in dataset.training.prefix(numCycles).enumerated() {\n",
        "    let start = Date()\n",
        "    var trainStats = Statistics(on: device)\n",
        "    var testStats = Statistics(on: device)\n",
        "    \n",
        "    var currStep: Int = 0\n",
        "    Context.local.learningPhase = .training\n",
        "    for batch in batches {\n",
        "        if let new_lr = get_cosine_annealing_lr(step: currStep, steps_per_epoch: stepsPerEpoch, base_lr: 0.0001, max_lr: 0.001) {\n",
        "          optimizer.learningRate = new_lr\n",
        "          curr_step = curr_step + 1\n",
        "        }\n",
        "        else {\n",
        "          continue\n",
        "        }\n",
        "\n",
        "        var (eagerImages, eagerLabels) = (batch.data, batch.label)\n",
        "        var npImages = eagerImages.makeNumpyArray()\n",
        "        var resized = tf.image.resize(npImages, [resize_size.0, resize_size.0])\n",
        "        var cropped = tf.image.random_crop(resized, [batchSize, resize_size.1, resize_size.1, 3])\n",
        "        var flipped = tf.image.random_flip_left_right(cropped)\n",
        "        var mixed_up = flipped\n",
        "        var newLabels = Tensor<Float>(Tensor<Int32>(oneHotAtIndices: eagerLabels, depth: 100))\n",
        "        if mixup_alpha > 0.0 {\n",
        "          var npLabels = newLabels.makeNumpyArray()\n",
        "          mixed_up = beta * mixed_up + (1 - beta) * tf.reverse(mixed_up, axis: [0])\n",
        "          npLabels = beta * npLabels + (1 - beta) * tf.reverse(npLabels, axis: [0])\n",
        "          newLabels = Tensor<Float>(numpy: npLabels.numpy())!\n",
        "        }\n",
        "        eagerImages = Tensor<Float>(numpy: mixed_up.numpy())!\n",
        "        let images = Tensor(copying: eagerImages, to: device)\n",
        "        let labels = Tensor(copying: newLabels, to: device)\n",
        "        let 𝛁model = TensorFlow.gradient(at: resnetv2) { resnetv2 -> Tensor<Float> in\n",
        "            let ŷ = resnetv2(images)\n",
        "            let loss = softmaxCrossEntropy(logits: ŷ, probabilities: labels)\n",
        "            trainStats.update(logits: ŷ, labels: labels, loss: loss)\n",
        "            return loss\n",
        "        }\n",
        "\n",
        "        optimizer.update(&resnetv2, along: 𝛁model)\n",
        "        LazyTensorBarrier()\n",
        "\n",
        "        currStep = currStep + 1\n",
        "        if currStep == stepsPerEpoch - 1 {\n",
        "          print(\"Adding model weights\")\n",
        "          swaModelWeights = swaModelWeights + 𝛁model\n",
        "        }\n",
        "    }\n",
        "\n",
        "    Context.local.learningPhase = .inference\n",
        "    for batch in dataset.validation {\n",
        "        var (eagerImages, eagerLabels) = (batch.data, batch.label)\n",
        "        var npImages = eagerImages.makeNumpyArray()\n",
        "        var resized = tf.image.resize(npImages, [resize_size.0, resize_size.0])\n",
        "        eagerImages = Tensor<Float>(numpy: resized.numpy())!\n",
        "        var newLabels = Tensor<Float>(Tensor<Int32>(oneHotAtIndices: eagerLabels, depth: 100))\n",
        "        let images = Tensor(copying: eagerImages, to: device)\n",
        "        let labels = Tensor(copying: newLabels, to: device)\n",
        "        let ŷ = resnetv2(images)\n",
        "        let loss = softmaxCrossEntropy(logits: ŷ, probabilities: labels)\n",
        "        LazyTensorBarrier()\n",
        "        testStats.update(logits: ŷ, labels: labels, loss: loss)\n",
        "    }\n",
        "\n",
        "    print(\n",
        "        \"\"\"\n",
        "        [Epoch \\(epoch)] \\\n",
        "        Training Loss: \\(String(format: \"%.3f\", trainStats.averageLoss)), \\\n",
        "        Training Accuracy: \\(trainStats.correctGuessCount)/\\(trainStats.totalGuessCount) \\\n",
        "        (\\(String(format: \"%.1f\", trainStats.accuracy))%), \\\n",
        "        Test Loss: \\(String(format: \"%.3f\", testStats.averageLoss)), \\\n",
        "        Test Accuracy: \\(testStats.correctGuessCount)/\\(testStats.totalGuessCount) \\\n",
        "        (\\(String(format: \"%.1f\", testStats.accuracy))%) \\\n",
        "        seconds per epoch: \\(String(format: \"%.1f\", Date().timeIntervalSince(start)))\n",
        "        \"\"\")\n",
        "}"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Adding model weights\n",
            "[Epoch 0] Training Loss: 0.022, Training Accuracy: 49637/49920 (99.4%), Test Loss: 0.599, Test Accuracy: 8513/10000 (85.1%) seconds per epoch: 240.1\n",
            "Adding model weights\n",
            "[Epoch 1] Training Loss: 0.021, Training Accuracy: 49638/49920 (99.4%), Test Loss: 0.618, Test Accuracy: 8476/10000 (84.8%) seconds per epoch: 243.5\n",
            "Adding model weights\n",
            "[Epoch 2] Training Loss: 0.021, Training Accuracy: 49606/49920 (99.4%), Test Loss: 0.624, Test Accuracy: 8487/10000 (84.9%) seconds per epoch: 242.3\n",
            "Adding model weights\n",
            "[Epoch 3] Training Loss: 0.018, Training Accuracy: 49684/49920 (99.5%), Test Loss: 0.628, Test Accuracy: 8509/10000 (85.1%) seconds per epoch: 243.2\n",
            "Adding model weights\n",
            "[Epoch 4] Training Loss: 0.017, Training Accuracy: 49699/49920 (99.6%), Test Loss: 0.640, Test Accuracy: 8495/10000 (84.9%) seconds per epoch: 241.6\n",
            "Adding model weights\n",
            "[Epoch 5] Training Loss: 0.015, Training Accuracy: 49732/49920 (99.6%), Test Loss: 0.651, Test Accuracy: 8485/10000 (84.8%) seconds per epoch: 241.8\n",
            "Adding model weights\n",
            "[Epoch 6] Training Loss: 0.016, Training Accuracy: 49719/49920 (99.6%), Test Loss: 0.651, Test Accuracy: 8497/10000 (85.0%) seconds per epoch: 241.6\n",
            "Adding model weights\n",
            "[Epoch 7] Training Loss: 0.014, Training Accuracy: 49731/49920 (99.6%), Test Loss: 0.651, Test Accuracy: 8484/10000 (84.8%) seconds per epoch: 240.5\n",
            "Adding model weights\n",
            "[Epoch 8] Training Loss: 0.014, Training Accuracy: 49727/49920 (99.6%), Test Loss: 0.658, Test Accuracy: 8493/10000 (84.9%) seconds per epoch: 241.6\n",
            "Adding model weights\n",
            "[Epoch 9] Training Loss: 0.014, Training Accuracy: 49725/49920 (99.6%), Test Loss: 0.655, Test Accuracy: 8496/10000 (85.0%) seconds per epoch: 240.8\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KPiwQA-fd471"
      },
      "source": [
        "var averagedModelWeights = swaModelWeights\n",
        "averagedModelWeights.inputStem.conv.filter = swaModelWeights.inputStem.conv.filter/Float(numCycles)\n",
        "averagedModelWeights.groupNorm.offset = swaModelWeights.groupNorm.offset/Float(numCycles)\n",
        "averagedModelWeights.groupNorm.scale = swaModelWeights.groupNorm.scale/Float(numCycles)\n",
        "averagedModelWeights.classifier.weight = swaModelWeights.classifier.weight/Float(numCycles)\n",
        "averagedModelWeights.classifier.bias = swaModelWeights.classifier.bias/Float(numCycles)\n",
        "\n",
        "for (index, weight)  in swaModelWeights.residualBlocks.enumerated() {\n",
        "  averagedModelWeights.residualBlocks[index].shortcut.projection.conv.filter = weight.shortcut.projection.conv.filter/Float(numCycles)\n",
        "  averagedModelWeights.residualBlocks[index].shortcut.norm.offset = weight.shortcut.norm.offset/Float(numCycles)\n",
        "  averagedModelWeights.residualBlocks[index].shortcut.norm.scale = weight.shortcut.norm.scale/Float(numCycles)\n",
        "     for (conv_index, conv_weight) in weight.convs.enumerated() {\n",
        "       averagedModelWeights.residualBlocks[index].convs[conv_index].conv.conv.filter = conv_weight.conv.conv.filter/Float(numCycles)\n",
        "       averagedModelWeights.residualBlocks[index].convs[conv_index].norm.offset = conv_weight.norm.offset/Float(numCycles)\n",
        "       averagedModelWeights.residualBlocks[index].convs[conv_index].norm.scale = conv_weight.norm.scale/Float(numCycles)\n",
        "     }\n",
        "\n",
        "}"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iHY3FEzx5v0k"
      },
      "source": [
        "optimizer.update(&resnetv2, along: averagedModelWeights)"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0W2hwKo76BbN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6f247561-7bbc-4546-b6e5-135e7c4f3091"
      },
      "source": [
        "var testStats = Statistics(on: device)\n",
        "Context.local.learningPhase = .inference\n",
        "for batch in dataset.validation {\n",
        "    var (eagerImages, eagerLabels) = (batch.data, batch.label)\n",
        "    var npImages = eagerImages.makeNumpyArray()\n",
        "    var resized = tf.image.resize(npImages, [resize_size.0, resize_size.0])\n",
        "    eagerImages = Tensor<Float>(numpy: resized.numpy())!\n",
        "    var newLabels = Tensor<Float>(Tensor<Int32>(oneHotAtIndices: eagerLabels, depth: 100))\n",
        "    let images = Tensor(copying: eagerImages, to: device)\n",
        "    let labels = Tensor(copying: newLabels, to: device)\n",
        "    let ŷ = resnetv2(images)\n",
        "    let loss = softmaxCrossEntropy(logits: ŷ, probabilities: labels)\n",
        "    LazyTensorBarrier()\n",
        "    testStats.update(logits: ŷ, labels: labels, loss: loss)\n",
        "}\n",
        "print(\n",
        "  \"\"\"\n",
        "  Test Accuracy: \\(testStats.correctGuessCount)/\\(testStats.totalGuessCount)\n",
        "  \"\"\")"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test Accuracy: 8496/10000\r\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_cp2_2dPvLpp"
      },
      "source": [
        ""
      ],
      "execution_count": 38,
      "outputs": []
    }
  ]
}